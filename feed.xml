<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://19revey.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://19revey.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-03-07T06:41:14+00:00</updated><id>https://19revey.github.io/feed.xml</id><title type="html">Yifei Duan</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Transformer from scratch using PyTorch</title><link href="https://19revey.github.io/blog/2023/transformer/" rel="alternate" type="text/html" title="Transformer from scratch using PyTorch"/><published>2023-07-11T12:57:00+00:00</published><updated>2023-07-11T12:57:00+00:00</updated><id>https://19revey.github.io/blog/2023/transformer</id><content type="html" xml:base="https://19revey.github.io/blog/2023/transformer/"><![CDATA[<h1 align="left" style="color:purple;font-size: 2em;">Overview</h1> <ul> <li><a href="#section1">1. Introduction</a></li> <li><a href="#section2">2. Import libraries</a></li> <li><a href="#section3">3. Basic components</a> <ul> <li><a href="#section4">Create Word Embeddings</a></li> <li><a href="#section5">Positional Encoding</a></li> <li><a href="#section6">Self Attention</a></li> </ul> </li> <li><a href="#section7">4. Encoder</a></li> <li><a href="#section8">5. Decoder</a></li> <li><a href="#section9">6. Testing our code</a></li> <li><a href="#section10">7. Some useful resources</a></li> </ul> <p><a class="anchor" id="section1"></a></p> <h2 style="color:purple;font-size: 2em;">1. Introduction</h2> <p><img src="https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png" width="330" height="470"/></p> <p><a class="anchor" id="section2"></a></p> <h2 style="color:purple;font-size: 2em;">2. Import Libraries</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">__version__</span><span class="p">)</span>

<span class="n">device</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
    <span class="k">else</span> <span class="sh">"</span><span class="s">mps</span><span class="sh">"</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
    <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="p">)</span>
</code></pre></div></div> <p><a class="anchor" id="section3"></a></p> <h2 style="color:purple;font-size: 2em;">3. Basic components</h2> <p><a class="anchor" id="section4"></a></p> <h2 style="color:purple;font-size: 1.5em;">Word Embeddings</h2> <p>Each word will be mapped to corresponding $d_{model}=512$ embedding vector. Suppose we have batch size of N=32 and sequence length of T=10 (10 words). The the output will be NxTxC (32X10X512).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Embedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            vocab_size: size of vocabulary
            embed_dim: dimension of embeddings, i.e. d_model
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Embedding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            x: input vector, i.e. (batch, seq_len, vocab_size)
        Returns:
            out: embedding vector (batch, seq_len, embed_dim)
        </span><span class="sh">"""</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div> <p><a class="anchor" id="section5"></a></p> <h2 style="color:purple;font-size: 1.5em;"> Positional Encoding</h2> \[PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\] \[PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})\] <p>Here \(pos\) is the position of the word in the sentence, and \(i\) refers to position along embedding vector dimension.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PositionalEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">max_seq_len</span><span class="p">,</span><span class="n">embed_model_dim</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            seq_len: length of input sequence
            embed_model_dim: demension of embedding, d_model
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">PositionalEmbedding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_model_dim</span>

        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">,</span><span class="mi">2</span><span class="p">):</span>
                <span class="n">pe</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10000</span> <span class="o">**</span> <span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">)</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)))</span>
                <span class="n">pe</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10000</span> <span class="o">**</span> <span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)))</span>

        <span class="c1"># adding batch dimension for broadcasting
</span>        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> 

        <span class="c1"># register buffer in Pytorch -&gt;
</span>        <span class="c1"># If you have parameters in your model, which should be saved and restored in the state_dict,
</span>        <span class="c1"># but not trained by the optimizer, you should register them as buffers.
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">pe</span><span class="sh">'</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            x: input vector
        Returns:
            x: output
        </span><span class="sh">"""</span>
        <span class="c1"># make embeddings relatively larger
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="c1">#add constant to embedding
</span>        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># x = x + torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">].</span><span class="nf">detach</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># src = torch.zeros(2,12)
# model = PositionalEmbedding(2,12)
# print(model(src).shape)
</span></code></pre></div></div> <p><a class="anchor" id="section6"></a></p> <h2 style="color:purple; font-size: 1.5em;"> Self Attention</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            embed_dim: dimension of embeding vector output
            n_heads: number of self attention heads
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>    <span class="c1">#512 dim
</span>        <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>   <span class="c1">#8
</span>        <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">)</span>   <span class="c1">#512/8 = 64  . each key,query, value will be of 64d
</span>       
        <span class="c1">#key,query and value matrixes    #64 x 64   
</span>        <span class="n">self</span><span class="p">.</span><span class="n">query_matrix</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># single key matrix for all 8 keys #512x512
</span>        <span class="n">self</span><span class="p">.</span><span class="n">key_matrix</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span>  <span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value_matrix</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span> 

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">key</span><span class="p">,</span><span class="n">query</span><span class="p">,</span><span class="n">value</span><span class="p">,</span><span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>    <span class="c1">#batch_size x sequence_length x embedding_dim    # 32 x 10 x 512
</span>        
        <span class="sh">"""</span><span class="s">
        Args:
           key : key vector
           query : query vector
           value : value vector
           mask: mask for decoder
        
        Returns:
           output vector from multihead attention
        </span><span class="sh">"""</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">seq_length</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># query dimension can change in decoder during inference. 
</span>        <span class="c1"># so we cant take general seq_length
</span>        <span class="n">seq_length_query</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># 32x10x512
</span>        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="p">)</span>  <span class="c1">#batch_size x sequence_length x n_heads x single_head_dim = (32x10x8x64)
</span>        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length_query</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="p">)</span> <span class="c1">#(32x10x8x64)
</span>        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="p">)</span> <span class="c1">#(32x10x8x64)
</span>       
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key_matrix</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>       <span class="c1"># (32x10x8x64)
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query_matrix</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>   
        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value_matrix</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, n_heads, seq_len, single_head_dim)    # (32 x 8 x 10 x 64)
</span>        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, n_heads, seq_len, single_head_dim)
</span>        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, n_heads, seq_len, single_head_dim)
</span>       
        <span class="c1"># computes attention
</span>        <span class="c1"># adjust key for matrix multiplication
</span>        <span class="n">k_adjusted</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1">#(batch_size, n_heads, single_head_dim, seq_ken)  #(32 x 8 x 64 x 10)
</span>        <span class="n">product</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k_adjusted</span><span class="p">)</span>  <span class="c1">#(32 x 8 x 10 x 64) x (32 x 8 x 64 x 10) = #(32x8x10x10)
</span>        
        <span class="c1"># fill those positions of product matrix as (-1e20) where mask positions are 0
</span>        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
             <span class="n">product</span> <span class="o">=</span> <span class="n">product</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">"</span><span class="s">-1e20</span><span class="sh">"</span><span class="p">))</span>

        <span class="c1">#divising by square root of key dimension
</span>        <span class="n">product</span> <span class="o">=</span> <span class="n">product</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="p">)</span> <span class="c1"># / sqrt(64)
</span>
        <span class="c1">#applying softmax
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">product</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
 
        <span class="c1">#mutiply with value matrix
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>  <span class="c1">##(32x8x 10x 10) x (32 x 8 x 10 x 64) = (32 x 8 x 10 x 64) 
</span>        
        <span class="c1">#concatenated output
</span>        <span class="n">concat</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length_query</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">)</span>  <span class="c1"># (32x8x10x64) -&gt; (32x10x8x64)  -&gt; (32,10,512)
</span>        
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">out</span><span class="p">(</span><span class="n">concat</span><span class="p">)</span> <span class="c1">#(32,10,512) -&gt; (32,10,512)
</span>       
        <span class="k">return</span> <span class="n">output</span>

</code></pre></div></div>]]></content><author><name></name></author><category term="machine_learning"/><category term="jupyter-notebook"/><summary type="html"><![CDATA[based on the paper "Attention is all you need"]]></summary></entry><entry><title type="html">Improve resume using LLM</title><link href="https://19revey.github.io/blog/2023/resume/" rel="alternate" type="text/html" title="Improve resume using LLM"/><published>2023-01-02T17:39:00+00:00</published><updated>2023-01-02T17:39:00+00:00</updated><id>https://19revey.github.io/blog/2023/resume</id><content type="html" xml:base="https://19revey.github.io/blog/2023/resume/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><category term="machine_learning"/><category term="coding"/><summary type="html"><![CDATA[improve resume based on job description using Google Gemini Pro]]></summary></entry><entry><title type="html">Compare classification models</title><link href="https://19revey.github.io/blog/2022/mlcomp/" rel="alternate" type="text/html" title="Compare classification models"/><published>2022-05-01T12:57:00+00:00</published><updated>2022-05-01T12:57:00+00:00</updated><id>https://19revey.github.io/blog/2022/mlcomp</id><content type="html" xml:base="https://19revey.github.io/blog/2022/mlcomp/"><![CDATA[<h3 id="compare-different-classification-models-using-the-titanic-disaster-data">Compare different classification models using the titanic disaster data</h3> <p><br/></p> <table> <thead> <tr> <th>Model</th> <th>F1 score</th> </tr> </thead> <tbody> <tr> <td>Random Forest</td> <td>0.7917</td> </tr> <tr> <td>Logistic Regression</td> <td>0.7586</td> </tr> <tr> <td>Neural Network</td> <td>0.7536</td> </tr> <tr> <td>K-Neighbors</td> <td>0.7500</td> </tr> <tr> <td>Naive Bayes</td> <td>0.7436</td> </tr> <tr> <td>XGBClassifier</td> <td>0.7347</td> </tr> <tr> <td>Decision Tree</td> <td>0.7020</td> </tr> </tbody> </table> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/ml_compare.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="machine_learning"/><category term="jupyter-notebook"/><summary type="html"><![CDATA[based on the titanic disaster data]]></summary></entry><entry><title type="html">Docker</title><link href="https://19revey.github.io/blog/2022/docker/" rel="alternate" type="text/html" title="Docker"/><published>2022-03-01T08:13:00+00:00</published><updated>2022-03-01T08:13:00+00:00</updated><id>https://19revey.github.io/blog/2022/docker</id><content type="html" xml:base="https://19revey.github.io/blog/2022/docker/"><![CDATA[<p><br/></p> <h3 id="dockerfile-example">Dockerfile example</h3> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Use a base image</span>
<span class="c">#FROM python:3.11-slim</span>
<span class="k">FROM</span><span class="s"> nvidia/cuda:12.1.0-base-ubuntu22.04</span>
<span class="c"># Set environment variables</span>
<span class="k">ENV</span><span class="s"> MY_VARIABLE=my_value</span>
<span class="c"># Install dependencies</span>
<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="se">\
</span>    build-essential <span class="se">\
</span>    software-properties-common <span class="se">\
</span>    git <span class="se">\
</span>    python3-pip <span class="se">\
</span>    <span class="o">&amp;&amp;</span> <span class="nb">rm</span> <span class="nt">-rf</span> /var/lib/apt/lists/<span class="k">*</span>
<span class="c"># Set the working directory</span>
<span class="k">WORKDIR</span><span class="s"> /app</span>
<span class="c"># Copy files into the image</span>
<span class="k">COPY</span><span class="s"> . /app</span>
<span class="c"># Expose a port</span>
<span class="k">EXPOSE</span><span class="s"> 8051</span>
<span class="c"># Install Python dependencies based on the requirements.txt file</span>
<span class="k">RUN </span>pip3 <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
<span class="c"># Configure a container that will run as an executable</span>
<span class="k">ENTRYPOINT</span><span class="s"> ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]</span>
<span class="c"># Define a command to run on container startup</span>
<span class="k">CMD</span><span class="s"> ["./start.sh"]</span>
<span class="c"># Keep container running</span>
<span class="k">CMD</span><span class="s"> ["tail -f /dev/null"]</span>
</code></pre></div></div> <table> <tbody> <tr> <td>Build image</td> <td><code class="language-plaintext highlighter-rouge">docker build -t image_name .</code></td> </tr> <tr> <td>Run container</td> <td><code class="language-plaintext highlighter-rouge">docker run -b -p 8501:8501 -e API="XXX" --name image_container image_name</code></td> </tr> <tr> <td>Remove all container.</td> <td><code class="language-plaintext highlighter-rouge">docker rm $(docker ps -a -q)</code></td> </tr> <tr> <td>Remove all iamges.</td> <td><code class="language-plaintext highlighter-rouge">docker image rm $(docker images -q)</code></td> </tr> </tbody> </table> <p><br/></p> <h3 id="the-process-of-building-and-running-docker-containers-can-be-automated-using-tools-like-docker-compose">The process of building and running Docker containers can be automated using tools like Docker Compose.</h3> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code>services:
  web:  #service name
    build: .
    shm_size: 8gb <span class="c"># increase shared memory</span>
    ports:
      - "8501:8501"
    volumes:
      - .:/app
    environment:
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - NVIDIA_VISIBLE_DEVICES=all
    command: tail -f /dev/null  <span class="c">#keep container running in the background</span>
</code></pre></div></div> <table> <tbody> <tr> <td>Build image and run container</td> <td><code class="language-plaintext highlighter-rouge">docker compose up --detach</code></td> </tr> <tr> <td>remove container</td> <td><code class="language-plaintext highlighter-rouge">docker compose down</code></td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="computer_science"/><category term="coding"/><summary type="html"><![CDATA[some docker commands]]></summary></entry><entry><title type="html">Git &amp;amp; Github</title><link href="https://19revey.github.io/blog/2022/code/" rel="alternate" type="text/html" title="Git &amp;amp; Github"/><published>2022-01-21T22:13:00+00:00</published><updated>2022-01-21T22:13:00+00:00</updated><id>https://19revey.github.io/blog/2022/code</id><content type="html" xml:base="https://19revey.github.io/blog/2022/code/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_git/gitworkflow-480.webp 480w,/assets/img/blog_git/gitworkflow-800.webp 800w,/assets/img/blog_git/gitworkflow-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_git/gitworkflow.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="tell-git-who-you-are">Tell Git who you are</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>Configure the author name.</td> <td><code class="language-plaintext highlighter-rouge">git config --global user.name "&lt;username&gt;"</code></td> </tr> <tr> <td>Configure the author email address.</td> <td><code class="language-plaintext highlighter-rouge">git config --global user.email &lt;email address&gt;</code></td> </tr> </tbody> </table> <h3 id="getting--creating-projects">Getting &amp; Creating Projects</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>Initialize a local Git repository</td> <td><code class="language-plaintext highlighter-rouge">git init</code></td> </tr> <tr> <td>Create a local copy of a remote repository</td> <td><code class="language-plaintext highlighter-rouge">git clone ssh://git@github.com/&lt;username&gt;/&lt;repository-name&gt;.git</code></td> </tr> </tbody> </table> <h3 id="basic-snapshotting">Basic Snapshotting</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>Check status</td> <td><code class="language-plaintext highlighter-rouge">git status</code></td> </tr> <tr> <td>Add a file to the staging area</td> <td><code class="language-plaintext highlighter-rouge">git add &lt;file-name.txt&gt;</code></td> </tr> <tr> <td>Add all new and changed files to the staging area</td> <td><code class="language-plaintext highlighter-rouge">git add -A</code> or <br/> <code class="language-plaintext highlighter-rouge">git add .</code></td> </tr> <tr> <td>Commit changes</td> <td><code class="language-plaintext highlighter-rouge">git commit -m "&lt;commit message&gt;"</code></td> </tr> <tr> <td>Remove a file (or folder)</td> <td><code class="language-plaintext highlighter-rouge">git rm -r &lt;file-name.txt&gt;</code></td> </tr> </tbody> </table> <h3 id="inspection--comparison">Inspection &amp; Comparison</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>View changes</td> <td><code class="language-plaintext highlighter-rouge">git log</code></td> </tr> <tr> <td>View changes (detailed)</td> <td><code class="language-plaintext highlighter-rouge">git log --summary</code></td> </tr> <tr> <td>View changes in one line (briefly)</td> <td><code class="language-plaintext highlighter-rouge">git log --oneline</code> or <br/> <code class="language-plaintext highlighter-rouge">git log --pretty=oneline</code> or<br/> <code class="language-plaintext highlighter-rouge">git log --pretty=short</code></td> </tr> </tbody> </table> <h3 id="undo-to-previous-file">Undo to previous file</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>List of all commit with commit id and commit message)</td> <td><code class="language-plaintext highlighter-rouge">git log --oneline</code></td> </tr> <tr> <td>Return to previous commit <commit></commit></td> <td><code class="language-plaintext highlighter-rouge">git checkout&lt;commit id&gt;</code></td> </tr> <tr> <td>Revert commit <commit> (undo one particular commit)</commit></td> <td><code class="language-plaintext highlighter-rouge">git revert &lt;commit id&gt;</code></td> </tr> <tr> <td>Reset to previous commit <commit> (remove history of all commit after <commit> )</commit></commit></td> <td><code class="language-plaintext highlighter-rouge">git reset --hard &lt;commit id&gt;</code></td> </tr> <tr> <td>Stop a file being tracked</td> <td><code class="language-plaintext highlighter-rouge">git rm --cached &lt;file/folder&gt;</code></td> </tr> <tr> <td>Restore a file to a previous commit</td> <td><code class="language-plaintext highlighter-rouge">git checkout &lt;file/to/restore&gt;</code></td> </tr> </tbody> </table> <h3 id="branching--merging">Branching &amp; Merging</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>List branches (the asterisk denotes the current branch)</td> <td><code class="language-plaintext highlighter-rouge">git branch</code></td> </tr> <tr> <td>List all branches (local and remote)</td> <td><code class="language-plaintext highlighter-rouge">git branch -a</code></td> </tr> <tr> <td>Create a new branch</td> <td><code class="language-plaintext highlighter-rouge">git branch &lt;branch name&gt;</code></td> </tr> <tr> <td>Create a new branch and switch to it</td> <td><code class="language-plaintext highlighter-rouge">git checkout -b &lt;branch name&gt;</code></td> </tr> <tr> <td>Clone a remote branch and switch to it</td> <td><code class="language-plaintext highlighter-rouge">git checkout -b &lt;branch name&gt; origin/&lt;branch name&gt;</code></td> </tr> <tr> <td>Rename a local branch</td> <td><code class="language-plaintext highlighter-rouge">git branch -m &lt;old branch name&gt; &lt;new branch name&gt;</code></td> </tr> <tr> <td>Switch to a branch</td> <td><code class="language-plaintext highlighter-rouge">git checkout &lt;branch name&gt;</code></td> </tr> <tr> <td>Switch to the branch last checked out</td> <td><code class="language-plaintext highlighter-rouge">git checkout -</code></td> </tr> <tr> <td>Discard changes to a file</td> <td><code class="language-plaintext highlighter-rouge">git checkout -- &lt;file-name.txt&gt;</code></td> </tr> <tr> <td>Delete a branch</td> <td><code class="language-plaintext highlighter-rouge">git branch -d &lt;branch name&gt;</code></td> </tr> <tr> <td>Delete a remote branch</td> <td><code class="language-plaintext highlighter-rouge">git push origin --delete &lt;branch name&gt;</code></td> </tr> <tr> <td>Preview changes before merging</td> <td><code class="language-plaintext highlighter-rouge">git diff &lt;source branch&gt; &lt;target branch&gt;</code></td> </tr> <tr> <td>Merge a branch into the active branch</td> <td><code class="language-plaintext highlighter-rouge">git merge &lt;branch name&gt;</code></td> </tr> <tr> <td>Merge a branch into a target branch</td> <td><code class="language-plaintext highlighter-rouge">git merge &lt;source branch&gt; &lt;target branch&gt;</code></td> </tr> <tr> <td>Stash changes in a dirty working directory</td> <td><code class="language-plaintext highlighter-rouge">git stash</code></td> </tr> <tr> <td>Remove all stashed entries</td> <td><code class="language-plaintext highlighter-rouge">git stash clear</code></td> </tr> </tbody> </table> <h3 id="sharing--updating-projects">Sharing &amp; Updating Projects</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>Push a branch to your remote repository</td> <td><code class="language-plaintext highlighter-rouge">git push origin &lt;branch name&gt;</code></td> </tr> <tr> <td>Push changes to remote repository (and remember the branch)</td> <td><code class="language-plaintext highlighter-rouge">git push -u origin &lt;branch name&gt;</code></td> </tr> <tr> <td>Push changes to remote repository (remembered branch)</td> <td><code class="language-plaintext highlighter-rouge">git push</code></td> </tr> <tr> <td>Push changes to remote repository all branch</td> <td><code class="language-plaintext highlighter-rouge">git push --all</code></td> </tr> <tr> <td>Push changes to remote repository (Force)</td> <td><code class="language-plaintext highlighter-rouge">git push -f</code></td> </tr> <tr> <td>Delete a remote branch</td> <td><code class="language-plaintext highlighter-rouge">git push origin --delete &lt;branch name&gt;</code></td> </tr> <tr> <td>Update local repository to the newest commit</td> <td><code class="language-plaintext highlighter-rouge">git pull</code></td> </tr> <tr> <td>Pull changes from remote repository</td> <td><code class="language-plaintext highlighter-rouge">git pull origin &lt;branch name&gt;</code></td> </tr> <tr> <td>Add a remote repository</td> <td><code class="language-plaintext highlighter-rouge">git remote add origin ssh://git@github.com/&lt;username&gt;/&lt;repository-name&gt;.git</code></td> </tr> <tr> <td>Set a repositoryâ€™s origin branch to SSH</td> <td><code class="language-plaintext highlighter-rouge">git remote set-url origin ssh://git@github.com/&lt;username&gt;/&lt;repository-name&gt;.git</code></td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="computer_science"/><category term="coding"/><summary type="html"><![CDATA[some git commands]]></summary></entry></feed>