<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://19revey.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://19revey.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-22T16:45:25+00:00</updated><id>https://19revey.github.io/feed.xml</id><title type="html">Yifei Duan</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">API Async Calls and Speed Optimization</title><link href="https://19revey.github.io/blog/2025/async/" rel="alternate" type="text/html" title="API Async Calls and Speed Optimization"/><published>2025-11-22T08:07:00+00:00</published><updated>2025-11-22T08:07:00+00:00</updated><id>https://19revey.github.io/blog/2025/async</id><content type="html" xml:base="https://19revey.github.io/blog/2025/async/"><![CDATA[<p>This tutorial demonstrates how to use Ollama’s API with async/await and various techniques to dramatically speed up your API calls. Ollama provides an OpenAI-compatible API, allowing us to use the familiar OpenAI client to interact with locally hosted models.</p> <h2 id="prerequisites">Prerequisites</h2> <p>Before starting, make sure you have:</p> <ol> <li><strong>Ollama installed</strong>: Download from <a href="https://ollama.ai">https://ollama.ai</a></li> <li><strong>A model pulled</strong>: Run <code class="language-plaintext highlighter-rouge">ollama pull llama3.2</code> (or mistral, codellama, etc.)</li> <li><strong>Ollama running</strong>: Start the server with <code class="language-plaintext highlighter-rouge">ollama serve</code></li> <li><strong>Required packages</strong>: Install with <code class="language-plaintext highlighter-rouge">pip install openai aiohttp</code></li> </ol> <p><br/></p> <h2 id="setup-and-configuration">Setup and Configuration</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">asyncio</span>
<span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">aiohttp</span>
<span class="kn">from</span> <span class="n">openai</span> <span class="kn">import</span> <span class="n">AsyncOpenAI</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>

<span class="c1"># Initialize the async client for Ollama
# Ollama runs on localhost:11434 by default and provides OpenAI-compatible API
</span><span class="n">client</span> <span class="o">=</span> <span class="nc">AsyncOpenAI</span><span class="p">(</span>
    <span class="n">base_url</span><span class="o">=</span><span class="sh">"</span><span class="s">http://localhost:11434/v1</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Ollama's API endpoint
</span>    <span class="n">api_key</span><span class="o">=</span><span class="sh">"</span><span class="s">ollama</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Ollama doesn't require a real API key, but the client needs one
</span><span class="p">)</span>

<span class="c1"># Default model to use (change this to any model you have pulled with Ollama)
# Common models: llama2, mistral, codellama, phi, gemma, llama3.2, etc.
</span><span class="n">DEFAULT_MODEL</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llama3.2</span><span class="sh">"</span>
</code></pre></div></div> <p><br/></p> <h2 id="part-0-helper-functions">Part 0: Helper Functions</h2> <p>First, let’s create a helper function to list available models:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">list_available_models</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">
    List all available Ollama models on your system.
    This uses Ollama</span><span class="sh">'</span><span class="s">s native API endpoint.
    </span><span class="sh">"""</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">async</span> <span class="k">with</span> <span class="n">aiohttp</span><span class="p">.</span><span class="nc">ClientSession</span><span class="p">()</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
            <span class="k">async</span> <span class="k">with</span> <span class="n">session</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">http://localhost:11434/api/tags</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">response</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">response</span><span class="p">.</span><span class="n">status</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
                    <span class="n">data</span> <span class="o">=</span> <span class="k">await</span> <span class="n">response</span><span class="p">.</span><span class="nf">json</span><span class="p">()</span>
                    <span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="p">[</span><span class="sh">"</span><span class="s">name</span><span class="sh">"</span><span class="p">]</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">data</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">models</span><span class="sh">"</span><span class="p">,</span> <span class="p">[])]</span>
                    <span class="k">return</span> <span class="n">models</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Error fetching models: </span><span class="si">{</span><span class="n">response</span><span class="p">.</span><span class="n">status</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
                    <span class="k">return</span> <span class="p">[]</span>
    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Error connecting to Ollama: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Make sure Ollama is running: ollama serve</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[]</span>
</code></pre></div></div> <p><br/></p> <h2 id="part-1-basic-async-api-call">Part 1: Basic Async API Call</h2> <p>The foundation of all async operations is a basic async function:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">basic_async_call</span><span class="p">(</span><span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">DEFAULT_MODEL</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Basic async call to Ollama API.
    This is the foundation for all async operations.
    
    Args:
        prompt: The prompt to send to the model
        model: The Ollama model to use (default: llama3.2)
    </span><span class="sh">"""</span>
    <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">client</span><span class="p">.</span><span class="n">chat</span><span class="p">.</span><span class="n">completions</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span>
        <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span>
</code></pre></div></div> <p><br/></p> <h2 id="part-2-sequential-vs-concurrent---performance-comparison">Part 2: Sequential vs Concurrent - Performance Comparison</h2> <p>The key to speed optimization is understanding the difference between sequential and concurrent processing:</p> <h3 id="sequential-approach-slow">Sequential Approach (SLOW)</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">sequential_calls</span><span class="p">(</span><span class="n">prompts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">
    Sequential approach: Process one request at a time.
    This is SLOW because each request waits for the previous one to complete.
    </span><span class="sh">"""</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">basic_async_call</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="n">results</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div> <h3 id="concurrent-approach-fast">Concurrent Approach (FAST)</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">concurrent_calls</span><span class="p">(</span><span class="n">prompts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">
    Concurrent approach: Process all requests simultaneously.
    This is FAST because requests are sent in parallel.
    </span><span class="sh">"""</span>
    <span class="c1"># Create tasks for all prompts
</span>    <span class="n">tasks</span> <span class="o">=</span> <span class="p">[</span><span class="nf">basic_async_call</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">]</span>
    <span class="c1"># Wait for all tasks to complete
</span>    <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div> <h3 id="performance-comparison-demo">Performance Comparison Demo</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">compare_performance</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">
    Compare the performance of sequential vs concurrent calls.
    </span><span class="sh">"""</span>
    <span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">What is Python?</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">What is JavaScript?</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">What is Rust?</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">What is Go?</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">What is TypeScript?</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">]</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Performance Comparison: Sequential vs Concurrent</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    
    <span class="c1"># Sequential approach
</span>    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">sequential_results</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">sequential_calls</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>
    <span class="n">sequential_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Sequential approach: </span><span class="si">{</span><span class="n">sequential_time</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Concurrent approach
</span>    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">concurrent_results</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">concurrent_calls</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>
    <span class="n">concurrent_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Concurrent approach: </span><span class="si">{</span><span class="n">concurrent_time</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="n">speedup</span> <span class="o">=</span> <span class="n">sequential_time</span> <span class="o">/</span> <span class="n">concurrent_time</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Speedup: </span><span class="si">{</span><span class="n">speedup</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">x faster with concurrent calls!</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Expected Result</strong>: Concurrent calls are typically <strong>3-5x faster</strong> for 5 prompts!</p> <p><br/></p> <h2 id="part-3-advanced-speed-optimization-techniques">Part 3: Advanced Speed Optimization Techniques</h2> <h3 id="technique-1-batch-processing-with-semaphore">Technique 1: Batch Processing with Semaphore</h3> <p>Use a semaphore to limit concurrent requests and prevent overwhelming the API:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">batch_with_semaphore</span><span class="p">(</span>
    <span class="n">prompts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> 
    <span class="n">max_concurrent</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">
    Use a semaphore to limit concurrent requests.
    This prevents overwhelming the API and helps with rate limits.
    
    Args:
        prompts: List of prompts to process
        max_concurrent: Maximum number of concurrent requests
    </span><span class="sh">"""</span>
    <span class="n">semaphore</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="nc">Semaphore</span><span class="p">(</span><span class="n">max_concurrent</span><span class="p">)</span>
    
    <span class="k">async</span> <span class="k">def</span> <span class="nf">bounded_call</span><span class="p">(</span><span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">async</span> <span class="k">with</span> <span class="n">semaphore</span><span class="p">:</span>  <span class="c1"># Limits concurrent execution
</span>            <span class="k">return</span> <span class="k">await</span> <span class="nf">basic_async_call</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
    
    <span class="n">tasks</span> <span class="o">=</span> <span class="p">[</span><span class="nf">bounded_call</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">]</span>
    <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div> <h3 id="technique-2-error-handling-with-retry-logic">Technique 2: Error Handling with Retry Logic</h3> <p>Robust error handling ensures speed without sacrificing reliability:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">batch_with_error_handling</span><span class="p">(</span>
    <span class="n">prompts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">max_retries</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
    <span class="sh">"""</span><span class="s">
    Process prompts with retry logic and error handling.
    This ensures robustness while maintaining speed.
    </span><span class="sh">"""</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">call_with_retry</span><span class="p">(</span><span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">for</span> <span class="n">attempt</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_retries</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">basic_async_call</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
                <span class="k">return</span> <span class="p">{</span>
                    <span class="sh">"</span><span class="s">index</span><span class="sh">"</span><span class="p">:</span> <span class="n">index</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">result</span><span class="sh">"</span><span class="p">:</span> <span class="n">result</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">success</span><span class="sh">"</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">attempts</span><span class="sh">"</span><span class="p">:</span> <span class="n">attempt</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="p">}</span>
            <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">attempt</span> <span class="o">==</span> <span class="n">max_retries</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">return</span> <span class="p">{</span>
                        <span class="sh">"</span><span class="s">index</span><span class="sh">"</span><span class="p">:</span> <span class="n">index</span><span class="p">,</span>
                        <span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
                        <span class="sh">"</span><span class="s">result</span><span class="sh">"</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
                        <span class="sh">"</span><span class="s">success</span><span class="sh">"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
                        <span class="sh">"</span><span class="s">error</span><span class="sh">"</span><span class="p">:</span> <span class="nf">str</span><span class="p">(</span><span class="n">e</span><span class="p">),</span>
                        <span class="sh">"</span><span class="s">attempts</span><span class="sh">"</span><span class="p">:</span> <span class="n">attempt</span> <span class="o">+</span> <span class="mi">1</span>
                    <span class="p">}</span>
                <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">attempt</span><span class="p">)</span>  <span class="c1"># Exponential backoff
</span>    
    <span class="n">tasks</span> <span class="o">=</span> <span class="p">[</span><span class="nf">call_with_retry</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">prompts</span><span class="p">)]</span>
    <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div> <h3 id="technique-3-streaming-for-better-user-experience">Technique 3: Streaming for Better User Experience</h3> <p>Stream responses for improved perceived performance:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">streaming_response</span><span class="p">(</span><span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">DEFAULT_MODEL</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Stream responses for better perceived performance.
    Users see results as they arrive, not all at once.
    
    Args:
        prompt: The prompt to send to the model
        model: The Ollama model to use (default: llama3.2)
    </span><span class="sh">"""</span>
    <span class="n">stream</span> <span class="o">=</span> <span class="k">await</span> <span class="n">client</span><span class="p">.</span><span class="n">chat</span><span class="p">.</span><span class="n">completions</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span>
        <span class="n">stream</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Streaming response for: </span><span class="sh">'</span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="sh">'"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="n">full_response</span> <span class="o">=</span> <span class="sh">""</span>
    <span class="k">async</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">chunk</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">delta</span><span class="p">.</span><span class="n">content</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">content</span> <span class="o">=</span> <span class="n">chunk</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">delta</span><span class="p">.</span><span class="n">content</span>
            <span class="nf">print</span><span class="p">(</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="sh">""</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">full_response</span> <span class="o">+=</span> <span class="n">content</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">full_response</span>
</code></pre></div></div> <p><br/></p> <h2 id="part-4-practical-example---document-processing">Part 4: Practical Example - Document Processing</h2> <p>Here’s a real-world example of processing multiple documents concurrently:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">process_documents_async</span><span class="p">(</span><span class="n">documents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">
    Practical example: Process multiple documents concurrently.
    </span><span class="sh">"""</span>
    <span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sa">f</span><span class="sh">"</span><span class="s">Summarize the following text in one sentence:</span><span class="se">\n\n</span><span class="si">{</span><span class="n">doc</span><span class="si">}</span><span class="sh">"</span>
        <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span>
    <span class="p">]</span>
    
    <span class="c1"># Use semaphore to limit concurrent requests (respect rate limits)
</span>    <span class="n">summaries</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">batch_with_semaphore</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">max_concurrent</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">summaries</span>
</code></pre></div></div> <p><br/></p> <h2 id="part-5-best-practices-and-optimization-tips">Part 5: Best Practices and Optimization Tips</h2> <h3 id="speed-optimization-checklist">Speed Optimization Checklist</h3> <ol> <li><strong>Use <code class="language-plaintext highlighter-rouge">asyncio.gather()</code> for parallel requests</strong> <ul> <li>Don’t await each call individually</li> <li>Create all tasks first, then gather them</li> </ul> </li> <li><strong>Use Semaphores to control concurrency</strong> <ul> <li>Prevents overwhelming the API</li> <li>Helps respect rate limits</li> <li>Typical values: 5-20 concurrent requests</li> </ul> </li> <li><strong>Implement retry logic with exponential backoff</strong> <ul> <li>Handles transient errors</li> <li>Prevents losing progress on failures</li> </ul> </li> <li><strong>Use streaming for long responses</strong> <ul> <li>Better user experience</li> <li>Perceived performance improvement</li> </ul> </li> <li><strong>Batch similar requests together</strong> <ul> <li>Reduces overhead</li> <li>More efficient resource usage</li> </ul> </li> <li><strong>Monitor resource usage</strong> <ul> <li>Ollama runs locally, limited by your hardware</li> <li>Use semaphores to prevent overwhelming your system</li> <li>Monitor GPU/CPU usage for concurrent requests</li> </ul> </li> <li><strong>Choose the right model</strong> <ul> <li>Smaller models (phi, gemma) are faster</li> <li>Larger models (llama2, mistral) are more capable</li> <li>Use the fastest model that meets your needs</li> </ul> </li> <li><strong>Cache responses when possible</strong> <ul> <li>Don’t re-request identical prompts</li> <li>Use memoization for repeated queries</li> </ul> </li> </ol> <p><br/></p> <h2 id="part-6-complete-working-example">Part 6: Complete Working Example</h2> <p>Here’s a complete example demonstrating all techniques:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">
    Main function demonstrating various async techniques with Ollama.
    </span><span class="sh">"""</span>
    <span class="k">global</span> <span class="n">DEFAULT_MODEL</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Ollama API Async Tutorial - Examples</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    
    <span class="c1"># List available models
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">0. Available Ollama Models:</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">available_models</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">list_available_models</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">available_models</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Found </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">available_models</span><span class="p">)</span><span class="si">}</span><span class="s"> model(s):</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">available_models</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  - </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">No models found. Pull a model first: ollama pull llama3.2</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Using model: </span><span class="si">{</span><span class="n">DEFAULT_MODEL</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">DEFAULT_MODEL</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">available_models</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Warning: </span><span class="si">{</span><span class="n">DEFAULT_MODEL</span><span class="si">}</span><span class="s"> not found in available models!</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">available_models</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Using first available model: </span><span class="si">{</span><span class="n">available_models</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">DEFAULT_MODEL</span> <span class="o">=</span> <span class="n">available_models</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    
    <span class="c1"># Example 1: Basic async call
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">1. Basic Async Call:</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">basic_async_call</span><span class="p">(</span><span class="sh">"</span><span class="s">Explain async/await in Python in one sentence.</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Result: </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Example 2: Performance comparison
</span>    <span class="k">await</span> <span class="nf">compare_performance</span><span class="p">()</span>
    
    <span class="c1"># Example 3: Batch processing with semaphore
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">3. Batch Processing with Semaphore:</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">test_prompts</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">What is machine learning?</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">What is deep learning?</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">What is neural network?</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">batch_results</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">batch_with_semaphore</span><span class="p">(</span><span class="n">test_prompts</span><span class="p">,</span> <span class="n">max_concurrent</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Processed </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">test_prompts</span><span class="p">)</span><span class="si">}</span><span class="s"> prompts in </span><span class="si">{</span><span class="n">batch_time</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">batch_results</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Prompt </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="si">:</span><span class="mi">100</span><span class="p">]</span><span class="si">}</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Example 4: Error handling
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">4. Batch Processing with Error Handling:</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">results_with_errors</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">batch_with_error_handling</span><span class="p">(</span><span class="n">test_prompts</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results_with_errors</span><span class="p">:</span>
        <span class="n">status</span> <span class="o">=</span> <span class="sh">"</span><span class="s">✓</span><span class="sh">"</span> <span class="k">if</span> <span class="n">result</span><span class="p">[</span><span class="sh">"</span><span class="s">success</span><span class="sh">"</span><span class="p">]</span> <span class="k">else</span> <span class="sh">"</span><span class="s">✗</span><span class="sh">"</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">status</span><span class="si">}</span><span class="s"> Prompt </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="sh">'</span><span class="s">index</span><span class="sh">'</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="sh">'</span><span class="s">success</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Example 5: Streaming
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">5. Streaming Response:</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">await</span> <span class="nf">streaming_response</span><span class="p">(</span><span class="sh">"</span><span class="s">Explain async/await in Python in 2-3 sentences.</span><span class="sh">"</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="c1"># Run the async main function
</span>    <span class="n">asyncio</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="nf">main</span><span class="p">())</span>
</code></pre></div></div> <p><br/></p> <h2 id="running-the-tutorial">Running the Tutorial</h2> <p>To run this tutorial:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 1. Make sure Ollama is running</span>
ollama serve

<span class="c"># 2. Pull a model if you haven't already</span>
ollama pull llama3.2

<span class="c"># 3. Install required packages</span>
pip <span class="nb">install </span>openai aiohttp

<span class="c"># 4. Run the script</span>
python ollama_async_tutorial.py
</code></pre></div></div> <p><br/></p> <h2 id="key-takeaways">Key Takeaways</h2> <ul> <li><strong>Async is powerful</strong>: Concurrent API calls can be 3-5x faster than sequential</li> <li><strong>Control concurrency</strong>: Use semaphores to prevent overwhelming resources</li> <li><strong>Handle errors gracefully</strong>: Implement retry logic with exponential backoff</li> <li><strong>Stream for UX</strong>: Streaming improves perceived performance</li> <li><strong>Monitor resources</strong>: Local models are hardware-limited</li> <li><strong>Choose wisely</strong>: Balance model size with speed requirements</li> </ul> <p>With these techniques, you can build fast, robust applications that leverage local LLMs efficiently!</p>]]></content><author><name></name></author><category term="computer_science"/><category term="python"/><category term="async"/><category term="ollama"/><category term="api"/><category term="machine-learning"/><summary type="html"><![CDATA[Complete guide to using Ollama's API with async/await and speed optimization techniques]]></summary></entry><entry><title type="html">High-Performance LLM Inference with vLLM and TGI</title><link href="https://19revey.github.io/blog/2025/vllm/" rel="alternate" type="text/html" title="High-Performance LLM Inference with vLLM and TGI"/><published>2025-11-22T08:07:00+00:00</published><updated>2025-11-22T08:07:00+00:00</updated><id>https://19revey.github.io/blog/2025/vllm</id><content type="html" xml:base="https://19revey.github.io/blog/2025/vllm/"><![CDATA[<p>This comprehensive tutorial covers high-performance LLM inference hosting using vLLM and TGI (Text Generation Inference), with a strong focus on optimizing inference speed. Both frameworks are production-ready inference servers that provide high throughput, low latency, and efficient GPU memory utilization.</p> <h2 id="introduction-to-vllm-and-tgi">Introduction to vLLM and TGI</h2> <h3 id="vllm-very-large-language-model">vLLM (Very Large Language Model)</h3> <ul> <li><strong>Developer</strong>: UC Berkeley</li> <li><strong>Key Innovation</strong>: PagedAttention algorithm for efficient memory management</li> <li><strong>Best For</strong>: High-throughput scenarios with batch processing</li> <li><strong>Language</strong>: Python-based, easy to integrate</li> <li><strong>Supports</strong>: Most popular models (LLaMA, Mistral, GPT, etc.)</li> </ul> <h3 id="tgi-text-generation-inference">TGI (Text Generation Inference)</h3> <ul> <li><strong>Developer</strong>: Hugging Face</li> <li><strong>Key Innovation</strong>: Written in Rust for maximum performance</li> <li><strong>Best For</strong>: Low-latency scenarios and real-time applications</li> <li><strong>Features</strong>: Built-in Flash Attention, extensive quantization support</li> <li><strong>Supports</strong>: GPTQ, AWQ, bitsandbytes quantization</li> </ul> <h3 id="key-speed-optimization-techniques">Key Speed Optimization Techniques</h3> <p>Both frameworks implement several critical optimization techniques:</p> <ol> <li><strong>Continuous Batching</strong> - Process multiple requests in parallel efficiently</li> <li><strong>KV Cache Optimization</strong> - Efficient attention computation and memory usage</li> <li><strong>Quantization</strong> - Reduce model size (4-bit, 8-bit) for faster inference</li> <li><strong>Tensor Parallelism</strong> - Distribute model across multiple GPUs</li> <li><strong>Speculative Decoding</strong> - Use smaller models to predict tokens</li> <li><strong>PagedAttention (vLLM)</strong> - Eliminates memory fragmentation</li> <li><strong>Flash Attention</strong> - Optimized attention computation</li> </ol> <p><br/></p> <h2 id="part-1-vllm-setup-and-installation">Part 1: vLLM Setup and Installation</h2> <h3 id="installation">Installation</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install vLLM</span>
pip <span class="nb">install </span>vllm

<span class="c"># For development</span>
pip <span class="nb">install </span>vllm[dev]
</code></pre></div></div> <h3 id="starting-vllm-server---basic-configuration">Starting vLLM Server - Basic Configuration</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Basic server startup</span>
python <span class="nt">-m</span> vllm.entrypoints.openai.api_server <span class="se">\</span>
    <span class="nt">--model</span> meta-llama/Llama-2-7b-chat-hf <span class="se">\</span>
    <span class="nt">--port</span> 8000 <span class="se">\</span>
    <span class="nt">--tensor-parallel-size</span> 1
</code></pre></div></div> <h3 id="advanced-vllm-server-configurations">Advanced vLLM Server Configurations</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># With quantization (faster, less memory)</span>
python <span class="nt">-m</span> vllm.entrypoints.openai.api_server <span class="se">\</span>
    <span class="nt">--model</span> meta-llama/Llama-2-7b-chat-hf <span class="se">\</span>
    <span class="nt">--quantization</span> awq <span class="se">\</span>
    <span class="nt">--port</span> 8000

<span class="c"># With multiple GPUs (tensor parallelism)</span>
python <span class="nt">-m</span> vllm.entrypoints.openai.api_server <span class="se">\</span>
    <span class="nt">--model</span> meta-llama/Llama-2-7b-chat-hf <span class="se">\</span>
    <span class="nt">--tensor-parallel-size</span> 2 <span class="se">\</span>
    <span class="nt">--port</span> 8000

<span class="c"># With continuous batching optimization</span>
python <span class="nt">-m</span> vllm.entrypoints.openai.api_server <span class="se">\</span>
    <span class="nt">--model</span> meta-llama/Llama-2-7b-chat-hf <span class="se">\</span>
    <span class="nt">--max-num-seqs</span> 256 <span class="se">\</span>
    <span class="nt">--gpu-memory-utilization</span> 0.95 <span class="se">\</span>
    <span class="nt">--port</span> 8000
</code></pre></div></div> <h3 id="vllm-client-implementation">vLLM Client Implementation</h3> <p>vLLM provides an OpenAI-compatible API, making it easy to integrate:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">asyncio</span>
<span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="n">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span><span class="p">,</span> <span class="n">AsyncOpenAI</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>

<span class="k">class</span> <span class="nc">VLLMClient</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Client for interacting with vLLM server.
    vLLM provides an OpenAI-compatible API.
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">base_url</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://localhost:8000/v1</span><span class="sh">"</span><span class="p">,</span> <span class="n">api_key</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">dummy</span><span class="sh">"</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">client</span> <span class="o">=</span> <span class="nc">OpenAI</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="n">base_url</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">base_url</span> <span class="o">=</span> <span class="n">base_url</span>
    
    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">meta-llama/Llama-2-7b-chat-hf</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.7</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">stream</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Generate text using vLLM.
        
        Args:
            prompt: Input prompt
            model: Model name
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            top_p: Nucleus sampling parameter
            stream: Whether to stream the response
        </span><span class="sh">"""</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="p">[{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}]</span>
        
        <span class="n">response</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">client</span><span class="p">.</span><span class="n">chat</span><span class="p">.</span><span class="n">completions</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
            <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
            <span class="n">stream</span><span class="o">=</span><span class="n">stream</span>
        <span class="p">)</span>
        
        <span class="k">if</span> <span class="n">stream</span><span class="p">:</span>
            <span class="n">full_response</span> <span class="o">=</span> <span class="sh">""</span>
            <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">response</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">chunk</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">delta</span><span class="p">.</span><span class="n">content</span><span class="p">:</span>
                    <span class="n">full_response</span> <span class="o">+=</span> <span class="n">chunk</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">delta</span><span class="p">.</span><span class="n">content</span>
            <span class="k">return</span> <span class="n">full_response</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">response</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span>
    
    <span class="k">def</span> <span class="nf">batch_generate</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">prompts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">meta-llama/Llama-2-7b-chat-hf</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.7</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="sh">"""</span><span class="s">
        Generate text for multiple prompts using vLLM</span><span class="sh">'</span><span class="s">s continuous batching.
        This is more efficient than sequential calls.
        </span><span class="sh">"""</span>
        <span class="n">messages_list</span> <span class="o">=</span> <span class="p">[[{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}]</span> <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">]</span>
        
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">messages</span> <span class="ow">in</span> <span class="n">messages_list</span><span class="p">:</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">client</span><span class="p">.</span><span class="n">chat</span><span class="p">.</span><span class="n">completions</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>
                <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span>
            <span class="p">)</span>
            <span class="n">results</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">results</span>
    
    <span class="k">def</span> <span class="nf">async_batch_generate</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">prompts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">meta-llama/Llama-2-7b-chat-hf</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.7</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="sh">"""</span><span class="s">
        Generate text for multiple prompts asynchronously.
        This leverages vLLM</span><span class="sh">'</span><span class="s">s continuous batching for maximum throughput.
        </span><span class="sh">"""</span>
        <span class="k">async</span> <span class="k">def</span> <span class="nf">generate_single</span><span class="p">(</span><span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">client</span> <span class="o">=</span> <span class="nc">AsyncOpenAI</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">base_url</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="sh">"</span><span class="s">dummy</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">messages</span> <span class="o">=</span> <span class="p">[{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}]</span>
            <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">client</span><span class="p">.</span><span class="n">chat</span><span class="p">.</span><span class="n">completions</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>
                <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">response</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span>
        
        <span class="k">async</span> <span class="k">def</span> <span class="nf">generate_all</span><span class="p">():</span>
            <span class="n">tasks</span> <span class="o">=</span> <span class="p">[</span><span class="nf">generate_single</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">]</span>
            <span class="k">return</span> <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="nf">generate_all</span><span class="p">())</span>
</code></pre></div></div> <p><br/></p> <h2 id="part-2-tgi-setup-and-installation">Part 2: TGI Setup and Installation</h2> <h3 id="installation-with-docker-recommended">Installation with Docker (Recommended)</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Basic TGI server with Docker</span>
docker run <span class="nt">--gpus</span> all <span class="nt">-p</span> 8080:80 <span class="se">\</span>
    ghcr.io/huggingface/text-generation-inference:latest <span class="se">\</span>
    <span class="nt">--model-id</span> meta-llama/Llama-2-7b-chat-hf
</code></pre></div></div> <h3 id="advanced-tgi-server-configurations">Advanced TGI Server Configurations</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># With quantization (4-bit for speed)</span>
docker run <span class="nt">--gpus</span> all <span class="nt">-p</span> 8080:80 <span class="se">\</span>
    ghcr.io/huggingface/text-generation-inference:latest <span class="se">\</span>
    <span class="nt">--model-id</span> meta-llama/Llama-2-7b-chat-hf <span class="se">\</span>
    <span class="nt">--quantize</span> bitsandbytes <span class="se">\</span>
    <span class="nt">--num-shard</span> 1

<span class="c"># With multiple GPUs (sharding)</span>
docker run <span class="nt">--gpus</span> all <span class="nt">-p</span> 8080:80 <span class="se">\</span>
    ghcr.io/huggingface/text-generation-inference:latest <span class="se">\</span>
    <span class="nt">--model-id</span> meta-llama/Llama-2-7b-chat-hf <span class="se">\</span>
    <span class="nt">--num-shard</span> 2

<span class="c"># With custom batching configuration</span>
docker run <span class="nt">--gpus</span> all <span class="nt">-p</span> 8080:80 <span class="se">\</span>
    <span class="nt">-v</span> <span class="nv">$PWD</span>/data:/data <span class="se">\</span>
    ghcr.io/huggingface/text-generation-inference:latest <span class="se">\</span>
    <span class="nt">--model-id</span> meta-llama/Llama-2-7b-chat-hf <span class="se">\</span>
    <span class="nt">--max-batch-total-tokens</span> 4096 <span class="se">\</span>
    <span class="nt">--num-shard</span> 1 <span class="se">\</span>
    <span class="nt">--port</span> 80
</code></pre></div></div> <h3 id="tgi-client-implementation">TGI Client Implementation</h3> <p>TGI provides a REST API (not OpenAI-compatible by default):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">requests</span>
<span class="kn">import</span> <span class="n">json</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>

<span class="k">class</span> <span class="nc">TGIClient</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Client for interacting with TGI server.
    TGI provides a REST API (not OpenAI-compatible by default).
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">base_url</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://localhost:8080</span><span class="sh">"</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">base_url</span> <span class="o">=</span> <span class="n">base_url</span>
    
    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.7</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.95</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
        <span class="n">stream</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Generate text using TGI.
        
        Args:
            prompt: Input prompt
            max_new_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            top_p: Nucleus sampling parameter
            top_k: Top-k sampling parameter
            do_sample: Whether to use sampling
            stream: Whether to stream the response
        </span><span class="sh">"""</span>
        <span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">base_url</span><span class="si">}</span><span class="s">/generate</span><span class="sh">"</span>
        <span class="n">payload</span> <span class="o">=</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">inputs</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">parameters</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
                <span class="sh">"</span><span class="s">max_new_tokens</span><span class="sh">"</span><span class="p">:</span> <span class="n">max_new_tokens</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">temperature</span><span class="sh">"</span><span class="p">:</span> <span class="n">temperature</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">top_p</span><span class="sh">"</span><span class="p">:</span> <span class="n">top_p</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">top_k</span><span class="sh">"</span><span class="p">:</span> <span class="n">top_k</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">do_sample</span><span class="sh">"</span><span class="p">:</span> <span class="n">do_sample</span>
            <span class="p">}</span>
        <span class="p">}</span>
        
        <span class="k">if</span> <span class="n">stream</span><span class="p">:</span>
            <span class="n">payload</span><span class="p">[</span><span class="sh">"</span><span class="s">stream</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">payload</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">full_response</span> <span class="o">=</span> <span class="sh">""</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="nf">iter_lines</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">line</span><span class="p">:</span>
                    <span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">loads</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
                    <span class="k">if</span> <span class="sh">"</span><span class="s">token</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
                        <span class="n">full_response</span> <span class="o">+=</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">token</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">full_response</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">payload</span><span class="p">)</span>
            <span class="n">response</span><span class="p">.</span><span class="nf">raise_for_status</span><span class="p">()</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">json</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">result</span><span class="p">[</span><span class="sh">"</span><span class="s">generated_text</span><span class="sh">"</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">batch_generate</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">prompts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">max_new_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.7</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="sh">"""</span><span class="s">
        Generate text for multiple prompts using TGI</span><span class="sh">'</span><span class="s">s batching.
        </span><span class="sh">"""</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
                <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
                <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_new_tokens</span><span class="p">,</span>
                <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span>
            <span class="p">)</span>
            <span class="n">results</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div> <p><br/></p> <h2 id="part-3-vllm-speed-optimization-strategies">Part 3: vLLM Speed Optimization Strategies</h2> <h3 id="1-continuous-batching-automatic">1. Continuous Batching (Automatic)</h3> <p>vLLM automatically batches requests for optimal throughput:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Tune batching with max-num-seqs parameter</span>
python <span class="nt">-m</span> vllm.entrypoints.openai.api_server <span class="se">\</span>
    <span class="nt">--model</span> meta-llama/Llama-2-7b-chat-hf <span class="se">\</span>
    <span class="nt">--max-num-seqs</span> 256 <span class="se">\</span>
    <span class="nt">--port</span> 8000
</code></pre></div></div> <p><strong>Configuration Guide</strong>:</p> <ul> <li>Higher <code class="language-plaintext highlighter-rouge">max-num-seqs</code> = More throughput, more memory usage</li> <li>Typical values: 64-256 depending on GPU memory</li> <li>Monitor GPU memory utilization to find optimal value</li> </ul> <h3 id="2-pagedattention-automatic">2. PagedAttention (Automatic)</h3> <p>PagedAttention is vLLM’s key innovation for efficient memory management:</p> <ul> <li><strong>No configuration needed</strong> - automatically enabled</li> <li>Eliminates memory fragmentation</li> <li>Enables higher batch sizes</li> <li>Up to 24x higher throughput vs traditional methods</li> </ul> <h3 id="3-quantization-for-speed">3. Quantization for Speed</h3> <p>Quantization reduces model size and increases inference speed:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># AWQ quantization (recommended for speed)</span>
python <span class="nt">-m</span> vllm.entrypoints.openai.api_server <span class="se">\</span>
    <span class="nt">--model</span> meta-llama/Llama-2-7b-chat-hf <span class="se">\</span>
    <span class="nt">--quantization</span> awq <span class="se">\</span>
    <span class="nt">--port</span> 8000

<span class="c"># GPTQ quantization</span>
python <span class="nt">-m</span> vllm.entrypoints.openai.api_server <span class="se">\</span>
    <span class="nt">--model</span> TheBloke/Llama-2-7B-Chat-GPTQ <span class="se">\</span>
    <span class="nt">--quantization</span> gptq <span class="se">\</span>
    <span class="nt">--port</span> 8000
</code></pre></div></div> <p><strong>Benefits</strong>:</p> <ul> <li>2-3x speedup</li> <li>4x memory reduction</li> <li>Minimal quality loss (&lt;2% typically)</li> </ul> <h3 id="4-tensor-parallelism-multi-gpu">4. Tensor Parallelism (Multi-GPU)</h3> <p>Distribute model across multiple GPUs:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Use 2 GPUs</span>
python <span class="nt">-m</span> vllm.entrypoints.openai.api_server <span class="se">\</span>
    <span class="nt">--model</span> meta-llama/Llama-2-7b-chat-hf <span class="se">\</span>
    <span class="nt">--tensor-parallel-size</span> 2 <span class="se">\</span>
    <span class="nt">--port</span> 8000

<span class="c"># Use 4 GPUs for larger models</span>
python <span class="nt">-m</span> vllm.entrypoints.openai.api_server <span class="se">\</span>
    <span class="nt">--model</span> meta-llama/Llama-2-70b-chat-hf <span class="se">\</span>
    <span class="nt">--tensor-parallel-size</span> 4 <span class="se">\</span>
    <span class="nt">--port</span> 8000
</code></pre></div></div> <p><strong>Performance</strong>:</p> <ul> <li>Near-linear scaling up to 4 GPUs</li> <li>Best for large models (70B+)</li> <li>Requires compatible GPUs</li> </ul> <h3 id="5-kv-cache-optimization">5. KV Cache Optimization</h3> <p>Optimize GPU memory utilization for faster inference:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> vllm.entrypoints.openai.api_server <span class="se">\</span>
    <span class="nt">--model</span> meta-llama/Llama-2-7b-chat-hf <span class="se">\</span>
    <span class="nt">--gpu-memory-utilization</span> 0.95 <span class="se">\</span>
    <span class="nt">--port</span> 8000
</code></pre></div></div> <p><strong>Tuning Guide</strong>:</p> <ul> <li>Default: 0.9 (90% of GPU memory)</li> <li>Higher values (0.95) = More caching, faster inference</li> <li>Leave some headroom to prevent OOM errors</li> </ul> <h3 id="6-complete-optimization-example">6. Complete Optimization Example</h3> <p>Here’s a fully optimized vLLM configuration:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Maximum speed configuration</span>
python <span class="nt">-m</span> vllm.entrypoints.openai.api_server <span class="se">\</span>
    <span class="nt">--model</span> meta-llama/Llama-2-7b-chat-hf <span class="se">\</span>
    <span class="nt">--quantization</span> awq <span class="se">\</span>
    <span class="nt">--tensor-parallel-size</span> 2 <span class="se">\</span>
    <span class="nt">--max-num-seqs</span> 256 <span class="se">\</span>
    <span class="nt">--gpu-memory-utilization</span> 0.95 <span class="se">\</span>
    <span class="nt">--port</span> 8000
</code></pre></div></div> <p><strong>Expected Performance</strong>:</p> <ul> <li>3-5x faster than baseline</li> <li>50-100 requests/second (depending on prompt length)</li> <li>&lt;100ms latency for short prompts</li> </ul> <p><br/></p> <h2 id="part-4-tgi-speed-optimization-strategies">Part 4: TGI Speed Optimization Strategies</h2> <h3 id="1-continuous-batching">1. Continuous Batching</h3> <p>TGI automatically batches requests:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">--gpus</span> all <span class="nt">-p</span> 8080:80 <span class="se">\</span>
    ghcr.io/huggingface/text-generation-inference:latest <span class="se">\</span>
    <span class="nt">--model-id</span> meta-llama/Llama-2-7b-chat-hf <span class="se">\</span>
    <span class="nt">--max-batch-total-tokens</span> 4096
</code></pre></div></div> <p><strong>Configuration</strong>:</p> <ul> <li>Higher <code class="language-plaintext highlighter-rouge">max-batch-total-tokens</code> = More throughput</li> <li>Typical values: 2048-8192</li> </ul> <h3 id="2-flash-attention-automatic">2. Flash Attention (Automatic)</h3> <p>Flash Attention is enabled by default for supported models:</p> <ul> <li><strong>No configuration needed</strong></li> <li>2-4x faster attention computation</li> <li>Reduced memory usage</li> <li>Works with most modern architectures</li> </ul> <h3 id="3-quantization">3. Quantization</h3> <p>TGI supports multiple quantization methods:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># bitsandbytes (4-bit or 8-bit)</span>
docker run <span class="nt">--gpus</span> all <span class="nt">-p</span> 8080:80 <span class="se">\</span>
    ghcr.io/huggingface/text-generation-inference:latest <span class="se">\</span>
    <span class="nt">--model-id</span> meta-llama/Llama-2-7b-chat-hf <span class="se">\</span>
    <span class="nt">--quantize</span> bitsandbytes

<span class="c"># GPTQ quantization</span>
docker run <span class="nt">--gpus</span> all <span class="nt">-p</span> 8080:80 <span class="se">\</span>
    ghcr.io/huggingface/text-generation-inference:latest <span class="se">\</span>
    <span class="nt">--model-id</span> TheBloke/Llama-2-7B-Chat-GPTQ <span class="se">\</span>
    <span class="nt">--quantize</span> gptq

<span class="c"># AWQ quantization</span>
docker run <span class="nt">--gpus</span> all <span class="nt">-p</span> 8080:80 <span class="se">\</span>
    ghcr.io/huggingface/text-generation-inference:latest <span class="se">\</span>
    <span class="nt">--model-id</span> TheBloke/Llama-2-7B-Chat-AWQ <span class="se">\</span>
    <span class="nt">--quantize</span> awq
</code></pre></div></div> <h3 id="4-model-sharding-multi-gpu">4. Model Sharding (Multi-GPU)</h3> <p>Distribute model across GPUs:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Use 2 GPUs</span>
docker run <span class="nt">--gpus</span> all <span class="nt">-p</span> 8080:80 <span class="se">\</span>
    ghcr.io/huggingface/text-generation-inference:latest <span class="se">\</span>
    <span class="nt">--model-id</span> meta-llama/Llama-2-7b-chat-hf <span class="se">\</span>
    <span class="nt">--num-shard</span> 2
</code></pre></div></div> <h3 id="5-token-streaming">5. Token Streaming</h3> <p>Stream tokens for better perceived latency:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Client code for streaming
</span><span class="k">def</span> <span class="nf">generate_stream</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">base_url</span><span class="si">}</span><span class="s">/generate_stream</span><span class="sh">"</span>
    <span class="n">payload</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">inputs</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">parameters</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">max_new_tokens</span><span class="sh">"</span><span class="p">:</span> <span class="mi">100</span><span class="p">}</span>
    <span class="p">}</span>
    
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">payload</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="nf">iter_lines</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">line</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">loads</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
            <span class="k">if</span> <span class="sh">"</span><span class="s">token</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">token</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">]</span>
</code></pre></div></div> <h3 id="6-complete-optimization-example-1">6. Complete Optimization Example</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Maximum speed configuration</span>
docker run <span class="nt">--gpus</span> all <span class="nt">-p</span> 8080:80 <span class="se">\</span>
    ghcr.io/huggingface/text-generation-inference:latest <span class="se">\</span>
    <span class="nt">--model-id</span> meta-llama/Llama-2-7b-chat-hf <span class="se">\</span>
    <span class="nt">--quantize</span> bitsandbytes <span class="se">\</span>
    <span class="nt">--num-shard</span> 2 <span class="se">\</span>
    <span class="nt">--max-batch-total-tokens</span> 4096 <span class="se">\</span>
    <span class="nt">--port</span> 80
</code></pre></div></div> <p><br/></p> <h2 id="part-5-performance-benchmarking">Part 5: Performance Benchmarking</h2> <h3 id="benchmarking-framework">Benchmarking Framework</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">statistics</span>

<span class="k">def</span> <span class="nf">benchmark_inference_speed</span><span class="p">(</span>
    <span class="n">client</span><span class="p">,</span>
    <span class="n">prompts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">num_runs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">
    Benchmark inference speed with multiple runs.
    
    Args:
        client: vLLM or TGI client
        prompts: List of prompts to test
        num_runs: Number of benchmark runs
    </span><span class="sh">"""</span>
    <span class="n">latencies</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">throughputs</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">run</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_runs</span><span class="p">):</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
        
        <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">client</span><span class="p">,</span> <span class="n">VLLMClient</span><span class="p">):</span>
            <span class="n">results</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">async_batch_generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">client</span><span class="p">,</span> <span class="n">TGIClient</span><span class="p">):</span>
            <span class="n">results</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">batch_generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Unknown client type</span><span class="sh">"</span><span class="p">)</span>
        
        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
        <span class="n">total_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
        
        <span class="n">latencies</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">total_time</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">))</span>
        <span class="n">throughputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span> <span class="o">/</span> <span class="n">total_time</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">avg_latency</span><span class="sh">"</span><span class="p">:</span> <span class="n">statistics</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">latencies</span><span class="p">),</span>
        <span class="sh">"</span><span class="s">std_latency</span><span class="sh">"</span><span class="p">:</span> <span class="n">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span> <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">avg_throughput</span><span class="sh">"</span><span class="p">:</span> <span class="n">statistics</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">throughputs</span><span class="p">),</span>
        <span class="sh">"</span><span class="s">std_throughput</span><span class="sh">"</span><span class="p">:</span> <span class="n">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="n">throughputs</span><span class="p">)</span> <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">throughputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">min_latency</span><span class="sh">"</span><span class="p">:</span> <span class="nf">min</span><span class="p">(</span><span class="n">latencies</span><span class="p">),</span>
        <span class="sh">"</span><span class="s">max_latency</span><span class="sh">"</span><span class="p">:</span> <span class="nf">max</span><span class="p">(</span><span class="n">latencies</span><span class="p">),</span>
        <span class="sh">"</span><span class="s">num_prompts</span><span class="sh">"</span><span class="p">:</span> <span class="nf">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">),</span>
        <span class="sh">"</span><span class="s">num_runs</span><span class="sh">"</span><span class="p">:</span> <span class="n">num_runs</span>
    <span class="p">}</span>
</code></pre></div></div> <h3 id="speed-comparison-utility">Speed Comparison Utility</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">InferenceSpeedOptimizer</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Demonstrates various techniques to improve inference speed.
    </span><span class="sh">"""</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">measure_latency</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sh">"""</span><span class="s">
        Measure the latency of a function call.
        </span><span class="sh">"""</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
        <span class="n">result</span> <span class="o">=</span> <span class="nf">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
        
        <span class="k">return</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">result</span><span class="sh">"</span><span class="p">:</span> <span class="n">result</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">latency</span><span class="sh">"</span><span class="p">:</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">tokens_per_second</span><span class="sh">"</span><span class="p">:</span> <span class="nf">len</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="nf">split</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span> <span class="k">if</span> <span class="n">result</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="p">}</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">measure_throughput</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">prompts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sh">"""</span><span class="s">
        Measure throughput (requests per second) for batch processing.
        </span><span class="sh">"""</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
        <span class="n">results</span> <span class="o">=</span> <span class="nf">func</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
        
        <span class="n">total_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">results</span><span class="sh">"</span><span class="p">:</span> <span class="n">results</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">total_time</span><span class="sh">"</span><span class="p">:</span> <span class="n">total_time</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">throughput</span><span class="sh">"</span><span class="p">:</span> <span class="nf">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span> <span class="o">/</span> <span class="n">total_time</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">avg_latency</span><span class="sh">"</span><span class="p">:</span> <span class="n">total_time</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>
        <span class="p">}</span>
</code></pre></div></div> <p><br/></p> <h2 id="part-6-optimization-configuration-examples">Part 6: Optimization Configuration Examples</h2> <h3 id="vllm-configurations">vLLM Configurations</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">vllm_optimization_examples</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">
    Example vLLM server configurations for different optimization goals.
    </span><span class="sh">"""</span>
    <span class="n">examples</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">max_speed</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">command</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"""</span><span class="s">python -m vllm.entrypoints.openai.api_server </span><span class="se">\\</span><span class="s">
    --model meta-llama/Llama-2-7b-chat-hf </span><span class="se">\\</span><span class="s">
    --quantization awq </span><span class="se">\\</span><span class="s">
    --tensor-parallel-size 2 </span><span class="se">\\</span><span class="s">
    --max-num-seqs 256 </span><span class="se">\\</span><span class="s">
    --gpu-memory-utilization 0.95 </span><span class="se">\\</span><span class="s">
    --port 8000</span><span class="sh">"""</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Maximum speed: quantization + tensor parallelism + high batching</span><span class="sh">"</span>
        <span class="p">},</span>
        
        <span class="sh">"</span><span class="s">balanced</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">command</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"""</span><span class="s">python -m vllm.entrypoints.openai.api_server </span><span class="se">\\</span><span class="s">
    --model meta-llama/Llama-2-7b-chat-hf </span><span class="se">\\</span><span class="s">
    --max-num-seqs 128 </span><span class="se">\\</span><span class="s">
    --gpu-memory-utilization 0.9 </span><span class="se">\\</span><span class="s">
    --port 8000</span><span class="sh">"""</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Balanced: good speed and quality</span><span class="sh">"</span>
        <span class="p">},</span>
        
        <span class="sh">"</span><span class="s">low_memory</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">command</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"""</span><span class="s">python -m vllm.entrypoints.openai.api_server </span><span class="se">\\</span><span class="s">
    --model meta-llama/Llama-2-7b-chat-hf </span><span class="se">\\</span><span class="s">
    --quantization awq </span><span class="se">\\</span><span class="s">
    --max-num-seqs 64 </span><span class="se">\\</span><span class="s">
    --gpu-memory-utilization 0.8 </span><span class="se">\\</span><span class="s">
    --port 8000</span><span class="sh">"""</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Low memory: quantization + reduced batching</span><span class="sh">"</span>
        <span class="p">},</span>
        
        <span class="sh">"</span><span class="s">high_quality</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">command</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"""</span><span class="s">python -m vllm.entrypoints.openai.api_server </span><span class="se">\\</span><span class="s">
    --model meta-llama/Llama-2-7b-chat-hf </span><span class="se">\\</span><span class="s">
    --max-num-seqs 64 </span><span class="se">\\</span><span class="s">
    --gpu-memory-utilization 0.85 </span><span class="se">\\</span><span class="s">
    --dtype float16 </span><span class="se">\\</span><span class="s">
    --port 8000</span><span class="sh">"""</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">High quality: no quantization, lower batching</span><span class="sh">"</span>
        <span class="p">}</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">examples</span>
</code></pre></div></div> <h3 id="tgi-configurations">TGI Configurations</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">tgi_optimization_examples</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">
    Example TGI server configurations for different optimization goals.
    </span><span class="sh">"""</span>
    <span class="n">examples</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">max_speed</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">command</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"""</span><span class="s">docker run --gpus all -p 8080:80 </span><span class="se">\\</span><span class="s">
    ghcr.io/huggingface/text-generation-inference:latest </span><span class="se">\\</span><span class="s">
    --model-id meta-llama/Llama-2-7b-chat-hf </span><span class="se">\\</span><span class="s">
    --quantize bitsandbytes </span><span class="se">\\</span><span class="s">
    --num-shard 2 </span><span class="se">\\</span><span class="s">
    --max-batch-total-tokens 4096 </span><span class="se">\\</span><span class="s">
    --port 80</span><span class="sh">"""</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Maximum speed: quantization + sharding + high batching</span><span class="sh">"</span>
        <span class="p">},</span>
        
        <span class="sh">"</span><span class="s">balanced</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">command</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"""</span><span class="s">docker run --gpus all -p 8080:80 </span><span class="se">\\</span><span class="s">
    ghcr.io/huggingface/text-generation-inference:latest </span><span class="se">\\</span><span class="s">
    --model-id meta-llama/Llama-2-7b-chat-hf </span><span class="se">\\</span><span class="s">
    --num-shard 1 </span><span class="se">\\</span><span class="s">
    --max-batch-total-tokens 2048 </span><span class="se">\\</span><span class="s">
    --port 80</span><span class="sh">"""</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Balanced: good speed and quality</span><span class="sh">"</span>
        <span class="p">},</span>
        
        <span class="sh">"</span><span class="s">low_memory</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">command</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"""</span><span class="s">docker run --gpus all -p 8080:80 </span><span class="se">\\</span><span class="s">
    ghcr.io/huggingface/text-generation-inference:latest </span><span class="se">\\</span><span class="s">
    --model-id meta-llama/Llama-2-7b-chat-hf </span><span class="se">\\</span><span class="s">
    --quantize bitsandbytes </span><span class="se">\\</span><span class="s">
    --num-shard 1 </span><span class="se">\\</span><span class="s">
    --max-batch-total-tokens 1024 </span><span class="se">\\</span><span class="s">
    --port 80</span><span class="sh">"""</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Low memory: quantization + reduced batching</span><span class="sh">"</span>
        <span class="p">},</span>
        
        <span class="sh">"</span><span class="s">high_quality</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">command</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"""</span><span class="s">docker run --gpus all -p 8080:80 </span><span class="se">\\</span><span class="s">
    ghcr.io/huggingface/text-generation-inference:latest </span><span class="se">\\</span><span class="s">
    --model-id meta-llama/Llama-2-7b-chat-hf </span><span class="se">\\</span><span class="s">
    --num-shard 1 </span><span class="se">\\</span><span class="s">
    --max-batch-total-tokens 1024 </span><span class="se">\\</span><span class="s">
    --dtype float16 </span><span class="se">\\</span><span class="s">
    --port 80</span><span class="sh">"""</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">High quality: no quantization, lower batching</span><span class="sh">"</span>
        <span class="p">}</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">examples</span>
</code></pre></div></div> <p><br/></p> <h2 id="part-7-best-practices-for-speed-optimization">Part 7: Best Practices for Speed Optimization</h2> <h3 id="15-essential-optimization-techniques">15 Essential Optimization Techniques</h3> <ol> <li><strong>Choose the Right Inference Server</strong> <ul> <li>vLLM: Best for high-throughput batch processing</li> <li>TGI: Best for low-latency real-time applications</li> <li>Both support continuous batching and modern optimizations</li> </ul> </li> <li><strong>Use Quantization</strong> <ul> <li>4-bit quantization (AWQ, GPTQ, bitsandbytes)</li> <li>2-3x speedup with 4x memory reduction</li> <li>Minimal quality loss (&lt;2% typically)</li> <li>AWQ recommended for vLLM, bitsandbytes for TGI</li> </ul> </li> <li><strong>Optimize Batching</strong> <ul> <li>Increase <code class="language-plaintext highlighter-rouge">max-num-seqs</code> (vLLM) or <code class="language-plaintext highlighter-rouge">max-batch-total-tokens</code> (TGI)</li> <li>Higher values = more throughput</li> <li>Monitor GPU memory usage</li> <li>Find sweet spot for your hardware</li> </ul> </li> <li><strong>Use Multiple GPUs</strong> <ul> <li>Tensor parallelism (vLLM) or sharding (TGI)</li> <li>2-4x speedup with 2-4 GPUs</li> <li>Best for models 13B+</li> <li>Requires compatible GPUs</li> </ul> </li> <li><strong>Optimize KV Cache</strong> <ul> <li>Increase <code class="language-plaintext highlighter-rouge">gpu-memory-utilization</code> (vLLM)</li> <li>Increase <code class="language-plaintext highlighter-rouge">max-total-tokens</code> (TGI)</li> <li>More cache = faster inference</li> <li>Balance with available memory</li> </ul> </li> <li><strong>Use Async Requests</strong> <ul> <li>Send multiple requests concurrently</li> <li>Leverage continuous batching</li> <li>Use asyncio or threading</li> <li>Don’t wait for sequential processing</li> </ul> </li> <li><strong>Stream Responses</strong> <ul> <li>Better perceived latency</li> <li>Users see results faster</li> <li>Reduces time-to-first-token</li> <li>Improves user experience</li> </ul> </li> <li><strong>Choose Appropriate Model Size</strong> <ul> <li>Smaller models = faster inference</li> <li>7B models for most tasks</li> <li>13B for complex reasoning</li> <li>70B only when necessary</li> </ul> </li> <li><strong>Monitor Resource Usage</strong> <ul> <li>Watch GPU memory utilization</li> <li>Monitor GPU compute usage</li> <li>Adjust parameters based on bottlenecks</li> <li>Use <code class="language-plaintext highlighter-rouge">nvidia-smi</code> for monitoring</li> </ul> </li> <li><strong>Cache Frequent Prompts</strong> <ul> <li>Cache identical prompts</li> <li>Use memoization or Redis</li> <li>Reduces redundant computation</li> <li>Significant speedup for repeated queries</li> </ul> </li> <li><strong>Optimize Prompt Length</strong> <ul> <li>Shorter prompts = faster inference</li> <li>Remove unnecessary context</li> <li>Use prompt compression techniques</li> <li>KV cache size matters</li> </ul> </li> <li><strong>Tune Generation Parameters</strong> <ul> <li>Lower <code class="language-plaintext highlighter-rouge">max_tokens</code> = faster inference</li> <li>Adjust temperature, top_p, top_k</li> <li>Use greedy decoding (temperature=0) for speed</li> <li>Balance quality vs speed</li> </ul> </li> <li><strong>Use Appropriate Data Types</strong> <ul> <li>float16 instead of float32</li> <li>Reduces memory usage</li> <li>Increases speed</li> <li>Minimal quality impact</li> </ul> </li> <li><strong>Benchmark and Iterate</strong> <ul> <li>Test different configurations</li> <li>Measure latency and throughput</li> <li>Optimize for your specific workload</li> <li>Use profiling tools</li> </ul> </li> <li><strong>Hardware Considerations</strong> <ul> <li>Use latest GPU architectures (A100, H100)</li> <li>Ensure sufficient PCIe bandwidth</li> <li>Use NVMe storage for model loading</li> <li>Consider CPU-GPU memory transfer</li> </ul> </li> </ol> <p><br/></p> <h2 id="part-8-complete-working-example">Part 8: Complete Working Example</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">example_usage</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">
    Complete example demonstrating vLLM and TGI usage.
    </span><span class="sh">"""</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">LLM Inference Hosting Tutorial - Example Usage</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    
    <span class="c1"># Example prompts for testing
</span>    <span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">Explain machine learning in one sentence.</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">What is the capital of France?</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Write a haiku about programming.</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">]</span>
    
    <span class="c1"># vLLM Example
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">1. vLLM Client Example:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">vllm_client</span> <span class="o">=</span> <span class="nc">VLLMClient</span><span class="p">()</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">vllm_client</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
            <span class="n">prompt</span><span class="o">=</span><span class="sh">"</span><span class="s">What is Python?</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">max_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span>
        <span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Result: </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        
        <span class="c1"># Batch generation
</span>        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Batch generation:</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">vllm_client</span><span class="p">.</span><span class="nf">async_batch_generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">results</span><span class="p">):</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Prompt </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="si">:</span><span class="mi">100</span><span class="p">]</span><span class="si">}</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>
            
    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Error (vLLM server may not be running): </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Start vLLM server with:</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">python -m vllm.entrypoints.openai.api_server </span><span class="se">\\</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">  --model meta-llama/Llama-2-7b-chat-hf </span><span class="se">\\</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">  --port 8000</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># TGI Example
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">2. TGI Client Example:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">tgi_client</span> <span class="o">=</span> <span class="nc">TGIClient</span><span class="p">()</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">tgi_client</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
            <span class="n">prompt</span><span class="o">=</span><span class="sh">"</span><span class="s">What is Python?</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span>
        <span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Result: </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        
        <span class="c1"># Batch generation
</span>        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Batch generation:</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">tgi_client</span><span class="p">.</span><span class="nf">batch_generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">results</span><span class="p">):</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Prompt </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="si">:</span><span class="mi">100</span><span class="p">]</span><span class="si">}</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>
            
    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Error (TGI server may not be running): </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Start TGI server with Docker:</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">docker run --gpus all -p 8080:80 </span><span class="se">\\</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">  ghcr.io/huggingface/text-generation-inference:latest </span><span class="se">\\</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">  --model-id meta-llama/Llama-2-7b-chat-hf</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Optimization Examples
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">3. Optimization Configuration Examples:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">vLLM Optimizations:</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">vllm_examples</span> <span class="o">=</span> <span class="nf">vllm_optimization_examples</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">config</span> <span class="ow">in</span> <span class="n">vllm_examples</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="si">{</span><span class="n">name</span><span class="p">.</span><span class="nf">upper</span><span class="p">()</span><span class="si">}</span><span class="s">:</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  </span><span class="si">{</span><span class="n">config</span><span class="p">[</span><span class="sh">'</span><span class="s">description</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  </span><span class="si">{</span><span class="n">config</span><span class="p">[</span><span class="sh">'</span><span class="s">command</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n\n</span><span class="s">TGI Optimizations:</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">tgi_examples</span> <span class="o">=</span> <span class="nf">tgi_optimization_examples</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">config</span> <span class="ow">in</span> <span class="n">tgi_examples</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="si">{</span><span class="n">name</span><span class="p">.</span><span class="nf">upper</span><span class="p">()</span><span class="si">}</span><span class="s">:</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  </span><span class="si">{</span><span class="n">config</span><span class="p">[</span><span class="sh">'</span><span class="s">description</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  </span><span class="si">{</span><span class="n">config</span><span class="p">[</span><span class="sh">'</span><span class="s">command</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="nf">example_usage</span><span class="p">()</span>
</code></pre></div></div> <p><br/></p> <h2 id="part-9-vllm-vs-tgi-performance-comparison">Part 9: vLLM vs TGI Performance Comparison</h2> <h3 id="when-to-choose-vllm">When to Choose vLLM</h3> <p><strong>Advantages</strong>:</p> <ul> <li>✅ Higher throughput for batch processing (2-3x)</li> <li>✅ Better memory efficiency with PagedAttention</li> <li>✅ Easier Python integration</li> <li>✅ More flexible configuration options</li> <li>✅ Better for high-concurrency scenarios</li> <li>✅ Active development and community support</li> </ul> <p><strong>Best For</strong>:</p> <ul> <li>API serving with high request volume</li> <li>Batch processing workflows</li> <li>Research and experimentation</li> <li>Python-first environments</li> </ul> <h3 id="when-to-choose-tgi">When to Choose TGI</h3> <p><strong>Advantages</strong>:</p> <ul> <li>✅ Lower latency for single requests (10-20% faster)</li> <li>✅ Rust-based for maximum performance</li> <li>✅ Better Docker integration</li> <li>✅ Built-in Flash Attention</li> <li>✅ Extensive quantization support</li> <li>✅ Production-ready with Hugging Face backing</li> </ul> <p><strong>Best For</strong>:</p> <ul> <li>Real-time chat applications</li> <li>Interactive user experiences</li> <li>Low-latency requirements</li> <li>Docker/Kubernetes deployments</li> </ul> <h3 id="performance-metrics-same-hardware">Performance Metrics (Same Hardware)</h3> <table> <thead> <tr> <th>Metric</th> <th>vLLM</th> <th>TGI</th> </tr> </thead> <tbody> <tr> <td>Throughput (batch)</td> <td><strong>100-150 req/s</strong></td> <td>70-100 req/s</td> </tr> <tr> <td>Single request latency</td> <td>100-120ms</td> <td><strong>80-100ms</strong></td> </tr> <tr> <td>Memory efficiency</td> <td><strong>Excellent</strong> (PagedAttention)</td> <td>Good</td> </tr> <tr> <td>GPU utilization</td> <td><strong>95%+</strong></td> <td>90%+</td> </tr> <tr> <td>Concurrent users</td> <td><strong>High</strong> (256+)</td> <td>Medium (128+)</td> </tr> </tbody> </table> <h3 id="recommendation">Recommendation</h3> <ul> <li><strong>Choose vLLM if</strong>: You need maximum throughput and batch processing efficiency</li> <li><strong>Choose TGI if</strong>: You need lowest possible latency for real-time applications</li> <li><strong>Both are excellent</strong>: Production-ready, well-maintained, and highly optimized</li> </ul> <p><br/></p> <h2 id="key-takeaways">Key Takeaways</h2> <ol> <li><strong>Both frameworks are production-ready</strong> with excellent performance</li> <li><strong>Quantization is critical</strong> - 2-3x speedup with minimal quality loss</li> <li><strong>Continuous batching</strong> enables efficient multi-request processing</li> <li><strong>GPU utilization</strong> should be monitored and optimized</li> <li><strong>Choose based on use case</strong>: vLLM for throughput, TGI for latency</li> <li><strong>Start simple, then optimize</strong> - measure before adding complexity</li> <li><strong>Hardware matters</strong> - invest in modern GPUs for best results</li> <li><strong>Cache aggressively</strong> - eliminate redundant computation</li> <li><strong>Stream for UX</strong> - improve perceived performance</li> <li><strong>Monitor and iterate</strong> - continuous optimization is key</li> </ol> <p>With these techniques, you can achieve <strong>3-5x speedup</strong> over baseline inference and serve hundreds of requests per second efficiently!</p>]]></content><author><name></name></author><category term="computer_science"/><category term="python"/><category term="machine-learning"/><category term="llm"/><category term="inference"/><category term="vllm"/><category term="tgi"/><category term="optimization"/><summary type="html"><![CDATA[Complete guide to deploying and optimizing LLM inference using vLLM and Text Generation Inference]]></summary></entry><entry><title type="html">UV - Fast Python Package Manager</title><link href="https://19revey.github.io/blog/2025/uv/" rel="alternate" type="text/html" title="UV - Fast Python Package Manager"/><published>2025-03-10T08:13:00+00:00</published><updated>2025-03-10T08:13:00+00:00</updated><id>https://19revey.github.io/blog/2025/uv</id><content type="html" xml:base="https://19revey.github.io/blog/2025/uv/"><![CDATA[<p><br/></p> <h3 id="install-uv">Install UV</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># On macOS and Linux</span>
curl <span class="nt">-LsSf</span> https://astral.sh/uv/install.sh | sh

<span class="c"># On Windows</span>
powershell <span class="nt">-ExecutionPolicy</span> ByPass <span class="nt">-c</span> <span class="s2">"irm https://astral.sh/uv/install.ps1 | iex"</span>

<span class="c"># Using pip</span>
pip <span class="nb">install </span>uv

<span class="c"># Using pipx</span>
pipx <span class="nb">install </span>uv
</code></pre></div></div> <p><br/></p> <h3 id="project-management">Project Management</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Initialize a new project</span>
uv init example
<span class="nb">cd </span>example


<span class="c"># Add dependencies</span>
uv add ruff
uv add requests pandas

<span class="c"># Create/update lockfile</span>
uv lock

<span class="c"># Sync dependencies from lockfile</span>
uv <span class="nb">sync</span>
</code></pre></div></div> <p><br/></p> <h3 id="tool-management">Tool Management</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Run tool in ephemeral environment</span>
uvx pycowsay <span class="s1">'hello world!'</span>

<span class="c"># Install tool permanently</span>
uv tool <span class="nb">install </span>ruff

<span class="c"># Run installed tool</span>
ruff <span class="nt">--version</span>
</code></pre></div></div> <p><br/></p> <h3 id="script-management">Script Management</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Add dependencies to a script</span>
uv add <span class="nt">--script</span> example.py requests

<span class="c"># Run script in isolated environment</span>
uv run example.py
</code></pre></div></div> <p><br/></p> <h3 id="python-version-management">Python Version Management</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install Python versions</span>
uv python <span class="nb">install </span>3.10 3.11 3.12

<span class="c"># Create venv with specific Python version</span>
uv venv <span class="nt">--python</span> 3.12.0

<span class="c"># Pin Python version for project</span>
uv python pin 3.11
</code></pre></div></div> <p><br/></p> <h3 id="pip-interface-commands">Pip Interface Commands</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Compile requirements</span>
uv pip compile requirements.in <span class="nt">--output-file</span> requirements.txt

<span class="c"># Create virtual environment</span>
uv venv

<span class="c"># Install from requirements</span>
uv pip <span class="nb">sync </span>requirements.txt
</code></pre></div></div> <table> <thead> <tr> <th>Command</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">uv init &lt;name&gt;</code></td> <td>Initialize new project</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">uv add &lt;package&gt;</code></td> <td>Add dependency to project</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">uv lock</code></td> <td>Generate lockfile</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">uv sync</code></td> <td>Install from lockfile</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">uv run &lt;script&gt;</code></td> <td>Run Python script</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">uv venv</code></td> <td>Create virtual environment</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">uvx &lt;tool&gt;</code></td> <td>Run tool in ephemeral env</td> </tr> </tbody> </table> <p>UV is an extremely fast Python package and project manager written in Rust. It aims to replace multiple tools like pip, pip-tools, pipx, poetry, pyenv, and virtualenv with a single unified solution. The tool offers 10-100x faster performance compared to pip and includes features like universal lockfiles, workspace support, and global dependency deduplication.</p>]]></content><author><name></name></author><category term="computer_science"/><category term="python"/><category term="package-manager"/><category term="rust"/><summary type="html"><![CDATA[Guide to using UV package manager for Python]]></summary></entry><entry><title type="html">Physics Informed Neural Network</title><link href="https://19revey.github.io/blog/2024/pinn/" rel="alternate" type="text/html" title="Physics Informed Neural Network"/><published>2024-03-20T12:57:00+00:00</published><updated>2024-03-20T12:57:00+00:00</updated><id>https://19revey.github.io/blog/2024/pinn</id><content type="html" xml:base="https://19revey.github.io/blog/2024/pinn/"><![CDATA[ <h2 style="color:purple;font-size: 2em;">Overview</h2> <ul> <li><a href="#section1">1. Introduction</a></li> <li><a href="#section2">2. Neural Netowork</a></li> <li><a href="#section3">3. Model Training</a></li> <li><a href="#section4">4. Results</a></li> <li><a href="#section5">5. Improvements</a></li> </ul> <h5 id="links-to-run-the-code">Links to run the code</h5> <ul> <li><a href="https://colab.research.google.com/github/19revey/PINN_granular_segregation/blob/main/notebook/solver_segregation.ipynb">Google colab</a></li> <li><a href="https://github.com/19revey/PINN_granular_segregation">Github repo</a></li> </ul> <h5 id="understand-the-granular-segregation-and-the-transport-equation">Understand the granular segregation and the transport equation</h5> <ul> <li><a href="https://arxiv.org/pdf/2309.13273.pdf">[1] General Model For Segregation Forces in Flowing Granular Mixtures</a></li> <li><a href="https://arxiv.org/pdf/1809.08089.pdf">[2] Diffusion, mixing, and segregation in confined granular flows</a></li> <li><a href="https://pubs.acs.org/doi/10.1021/acs.iecr.5b01268">[3] On Mixing and Segregation: From Fluids and Maps to Granular Solids and Advection–Diffusion Systems</a></li> </ul> <h5 id="pinn-implementation">PINN implementation</h5> <ul> <li><a href="https://github.com/nanditadoloi/PINN">https://github.com/nanditadoloi/PINN</a></li> <li><a href="https://github.com/omniscientoctopus/Physics-Informed-Neural-Networks">https://github.com/omniscientoctopus/Physics-Informed-Neural-Networks</a></li> <li><a href="https://github.com/maziarraissi/PINNs">https://github.com/maziarraissi/PINNs</a></li> </ul> <p>Here is a li</p> <p><a class="anchor" id="section1"></a></p> <h2 style="color:purple;font-size: 2em;">1. Introduction</h2> <p>An advection-diffusion transport equation has been successfully used to model the segregation. Within this continuum framework, the concentration of species \(i\) can be expressed as</p> \[\frac{\partial c_i}{\partial t} + {\nabla \cdot (\pmb u c_i)}={\nabla \cdot (D\nabla c_i)}.\] <p>With assumption of incompressible flow and negligible vertical acceleration, the above equation in the \(z\) direction can be written as</p> \[\frac{\partial c_i}{\partial t} +\frac{\partial (w+w_{i})c_i}{\partial z}=\frac{\partial}{\partial z} \Big( D\frac{\partial c_i}{\partial z} \Big),\] <p>or, rearranging, as</p> \[w_{i}c_i-D\frac{\partial c_i}{\partial z}=0,\] <p>where \(w_{i}\) is the segregation velocity relative to the bulk velocity \(w\).</p> <table> <thead> <tr> <th> </th> <th> </th> <th>full model</th> <th>simplified model</th> </tr> </thead> <tbody> <tr> <td>intruder scaled segregation force</td> <td>\(F_{i,0}\)</td> <td>\(-f^g(R)\frac{\partial{p}}{\partial{z}}V_i+f^k(R)\frac{p}{\dot\gamma}\frac{\partial\dot\gamma}{\partial{z}}V_i\)</td> <td> </td> </tr> <tr> <td>Mixture scaled segregation force</td> <td>\(\hat F_{l}\) <br/> \(\hat{F}_{s}\)</td> <td>\((\hat{F}_{l,0}-\cos{\theta})\textrm{tanh}\Big( \frac{\cos{\theta}-\hat{F}_{s,0}}{\hat{F}_{l,0}-\cos{\theta}}\frac{c_s}{c_l} \Big)\) <br/> \(-(\hat{F}_{l,0}-\cos{\theta}){\frac{c_l}{c_s}}\textrm{tanh}\Big( \frac{\cos{\theta}-\hat{F}_{s,0}}{\hat{F}_{l,0}-\cos{\theta}}\frac{c_s}{c_l} \Big)\)</td> <td> </td> </tr> <tr> <td>effective friction</td> <td>\(\mu_{eff}\)</td> <td>\(\mu_s+\frac{\mu_2-\mu_s}{I_c/I+1}\)</td> <td> </td> </tr> <tr> <td>viscosity</td> <td>\(\eta\)</td> <td>\(\mu_{eff} \frac{P}{\dot\gamma}\)</td> <td> </td> </tr> <tr> <td>drag coefficient</td> <td>\(c_{d,l}\) <br/> \(c_{d,s}\)</td> <td>\([k_1-k_2\exp(-k_3 R)]+s_1 I R +s_2 I (R_\rho-1)\) <br/> \(c_{d,l}/R^2\)</td> <td> </td> </tr> <tr> <td>segregation velocity</td> <td>\(w_l\) <br/> \(w_s\)</td> <td>\(\frac{ \hat F_{l} m_l g_0}{c_{d,l} \pi \eta d_l}\) <br/> \(-\frac{ \hat F_s m_s g_0}{c_{d,s} \pi \eta d_s}\)</td> <td>\(0.26 d_s \ln R \dot\gamma (1-c_i)\)</td> </tr> <tr> <td>diffusion coefficient</td> <td>\(D\)</td> <td>\(0.042 \dot \gamma (c_ld_l+c_sd_s)^2\)</td> <td>\(0.042 \dot \gamma {\bar d}^2\)</td> </tr> </tbody> </table> <p><a class="anchor" id="section2"></a></p> <h2 style="color:purple;font-size: 2em;">2. Physics Informed Neural Network</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="n">device</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
    <span class="k">else</span> <span class="sh">"</span><span class="s">mps</span><span class="sh">"</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
    <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>

        <span class="c1"># 6 layer neural network
</span>        <span class="nf">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ln1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ln2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ln3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ln4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">hidden_layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_layer3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_layer4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_layer5</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mse</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>
        
        <span class="c1"># particle properties in S.I unit
</span>        <span class="n">self</span><span class="p">.</span><span class="n">rd</span><span class="o">=</span><span class="mi">2</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dl</span><span class="o">=</span><span class="mf">0.004</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rho</span><span class="o">=</span><span class="mi">1000</span>
        <span class="n">self</span><span class="p">.</span><span class="n">c_diffusion</span><span class="o">=</span><span class="mf">0.042</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ds</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dl</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rds</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ml</span><span class="o">=</span><span class="mi">4</span><span class="o">/</span><span class="mi">3</span><span class="o">*</span><span class="n">math</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="mf">0.002</span><span class="o">**</span><span class="mi">3</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">rho</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ms</span><span class="o">=</span><span class="mi">4</span><span class="o">/</span><span class="mi">3</span><span class="o">*</span><span class="n">math</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="mf">0.001</span><span class="o">**</span><span class="mi">3</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">rho</span>
        
        
        <span class="c1"># flow configuration (uniform shear)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span><span class="o">=</span><span class="mi">100</span>
        <span class="n">self</span><span class="p">.</span><span class="n">phi</span><span class="o">=</span><span class="mf">0.55</span>
        <span class="n">self</span><span class="p">.</span><span class="n">g</span><span class="o">=</span><span class="mf">9.81</span>
        <span class="n">self</span><span class="p">.</span><span class="n">h0</span><span class="o">=</span><span class="mf">0.01</span>
        <span class="n">self</span><span class="p">.</span><span class="n">p0</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">h0</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">rho</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">g</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">phi</span>

        <span class="c1"># segregation force calculation
</span>        <span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  
        <span class="c1">#  Duan et al. 2024
</span>        <span class="n">_intruder_l</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mf">1.43</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span><span class="o">/</span><span class="mf">0.92</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="mf">3.55</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span><span class="o">/</span><span class="mf">2.94</span><span class="p">))</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">phi</span>
        <span class="n">_intruder_s</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mf">1.43</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">rds</span><span class="o">/</span><span class="mf">0.92</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="mf">3.55</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">rds</span><span class="o">/</span><span class="mf">2.94</span><span class="p">))</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">phi</span>

        <span class="n">self</span><span class="p">.</span><span class="n">intruder_l</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">_intruder_l</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">intruder_s</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">_intruder_s</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> 

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="c1"># if time dimensino is considered, concatenated first, i.e. torch.cat([z,t],axis=1) 
</span>        <span class="n">layer1_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ln1</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">hidden_layer1</span><span class="p">(</span><span class="n">z</span><span class="p">)))</span>
        <span class="n">layer2_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ln2</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">hidden_layer2</span><span class="p">(</span><span class="n">layer1_out</span><span class="p">)))</span>
        <span class="n">layer3_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ln3</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">hidden_layer3</span><span class="p">(</span><span class="n">layer2_out</span><span class="p">)))</span>
        <span class="n">layer4_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ln4</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">hidden_layer4</span><span class="p">(</span><span class="n">layer3_out</span><span class="p">)))</span>
        <span class="n">layer5_out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">hidden_layer5</span><span class="p">(</span><span class="n">layer4_out</span><span class="p">))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">output_layer</span><span class="p">(</span><span class="n">layer5_out</span><span class="p">)</span> <span class="c1">## For regression, no activation is used in output layer
</span>        <span class="k">return</span> <span class="n">output</span>
    
    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>


        <span class="c1"># PDE loss    
</span>        <span class="n">z_collocation</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10001</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

        <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">z_collocation</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> 

        <span class="n">c</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> 
        
        <span class="n">p</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">rho</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">phi</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">g</span><span class="o">*</span><span class="n">z</span><span class="o">+</span><span class="n">self</span><span class="p">.</span><span class="n">p0</span>
        <span class="n">inert</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">gamma</span><span class="o">*</span><span class="p">(</span><span class="n">c</span><span class="o">*</span><span class="mf">0.004</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mf">0.004</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span><span class="p">))</span><span class="o">/</span><span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">p</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">rho</span><span class="p">);</span>
        <span class="n">mu_eff</span><span class="o">=</span><span class="mf">0.364</span><span class="o">+</span><span class="p">(</span><span class="mf">0.772</span><span class="o">-</span><span class="mf">0.364</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mf">0.434</span><span class="o">/</span><span class="n">inert</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">eta</span><span class="o">=</span><span class="n">mu_eff</span><span class="o">*</span><span class="n">p</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">gamma</span>

        <span class="n">mixture_l</span><span class="o">=</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">intruder_l</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="p">)</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">intruder_s</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">intruder_l</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="p">)</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">ml</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">ms</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">)</span><span class="o">/</span><span class="n">c</span><span class="p">)</span>
        <span class="n">mixture_s</span><span class="o">=-</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">intruder_l</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="p">)</span><span class="o">*</span><span class="n">c</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">)</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">ms</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">ml</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">intruder_s</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">intruder_l</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="p">)</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">ml</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">ms</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">)</span><span class="o">/</span><span class="n">c</span><span class="p">)</span>
        

        <span class="n">cd</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="o">-</span><span class="mi">7</span><span class="o">*</span><span class="n">math</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">2.6</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span><span class="p">))</span><span class="o">+</span><span class="mf">0.57</span><span class="o">*</span><span class="n">inert</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span>

        <span class="n">wseg</span><span class="o">=</span><span class="n">mixture_l</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">ml</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">g</span> <span class="o">/</span> <span class="p">(</span><span class="n">cd</span><span class="o">*</span><span class="n">math</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">eta</span><span class="o">*</span><span class="mf">0.004</span><span class="p">)</span>

        <span class="n">c_z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="nf">sum</span><span class="p">(),</span> <span class="n">z</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Duan et al. 2024  
</span>        <span class="n">pde</span><span class="o">=</span><span class="p">(</span><span class="n">wseg</span><span class="o">*</span><span class="n">c</span><span class="o">-</span><span class="mf">0.042</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">gamma</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">square</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">)</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">ds</span><span class="o">+</span><span class="n">c</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">dl</span><span class="p">)</span><span class="o">*</span><span class="n">c_z</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span>

        <span class="c1"># Schlick et al. 2015
</span>        <span class="c1"># simplified with constant diffusion coefficient
</span>        <span class="c1"># pde = (1/0.1 *(1-c)*c - c_z )*10
</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">pde</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">pde_loss</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">mse</span><span class="p">(</span><span class="n">pde</span><span class="p">,</span><span class="n">target</span><span class="p">)</span>


        <span class="c1"># Mass conservation loss
</span>        <span class="n">x_bc</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">10001</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">x_bc</span> <span class="o">=</span> <span class="n">x_bc</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">u_bc</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">x_bc</span><span class="p">)</span>
        <span class="n">u_bc</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">u_bc</span><span class="p">)</span>
        
        <span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">u_bc</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">+</span><span class="mf">0.5</span>
        <span class="n">mass_loss</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">mse</span><span class="p">(</span><span class="n">u_bc</span><span class="p">,</span><span class="n">target</span><span class="p">)</span>
   
        <span class="k">return</span> <span class="n">mass_loss</span>  <span class="o">+</span> <span class="n">pde_loss</span>
</code></pre></div></div> <p><a class="anchor" id="section3"></a></p> <h2 style="color:purple;font-size: 2em;">3. Train </h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">net</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">mse_cost_function</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span> <span class="c1"># Mean squared error
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.002</span><span class="p">)</span>


<span class="n">iterations</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">previous_validation_loss</span> <span class="o">=</span> <span class="mf">99999999.0</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span> <span class="c1"># to make the gradients zero
</span>    <span class="n">loss</span><span class="o">=</span><span class="n">net</span><span class="p">.</span><span class="nf">loss</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span> <span class="c1"># This is for computing gradients using backward propagation
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span> <span class="c1"># This is equivalent to : theta_new = theta_old - alpha * derivative of J w.r.t theta
</span>    <span class="k">if</span> <span class="n">loss</span><span class="p">.</span><span class="n">data</span><span class="o">&lt;</span><span class="mf">1e-6</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    	<span class="nf">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span><span class="sh">"</span><span class="s">Traning Loss:</span><span class="sh">"</span><span class="p">,</span><span class="n">loss</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
</code></pre></div></div> <p><a class="anchor" id="section4"></a></p> <h2 style="color:purple;font-size: 2em;">4. Results </h2> <p>The full model prediction is compared to the previous model by Schlick et al. 2015 with different values of \(\lambda\).</p> \[\frac{d_c}{d_z}=\frac{1}{\lambda}c_l(1-c_l)\] <h2 style="color:purple;font-size: 1em;">4.1 Uniform shear </h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_img/comparison001-480.webp 480w,/assets/img/blog_img/comparison001-800.webp 800w,/assets/img/blog_img/comparison001-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_img/comparison001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_img/comparison01-480.webp 480w,/assets/img/blog_img/comparison01-800.webp 800w,/assets/img/blog_img/comparison01-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_img/comparison01.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 style="color:purple;font-size: 1em;">4.2 Exponential shear </h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_img/comparison001exp-480.webp 480w,/assets/img/blog_img/comparison001exp-800.webp 800w,/assets/img/blog_img/comparison001exp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_img/comparison001exp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_img/comparison01exp-480.webp 480w,/assets/img/blog_img/comparison01exp-800.webp 800w,/assets/img/blog_img/comparison01exp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_img/comparison01exp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a class="anchor" id="section5"></a></p> <h2 style="color:purple;font-size: 2em;">5. Improvements </h2> <ul> <li>Add loss function for \(c\) out of range 0 to 1</li> <li>Add a learning rate scheduler to reduce learning rate at loss plateau (lr reduced from 1e-3 to 1e-5)</li> </ul> <p>After these changes, the overall loss is reduced to 1e-8</p> <h2 style="color:purple;font-size: 1em;">5.1 Exponential shear </h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_img/comparison001exp_a-480.webp 480w,/assets/img/blog_img/comparison001exp_a-800.webp 800w,/assets/img/blog_img/comparison001exp_a-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_img/comparison001exp_a.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_img/comparison01exp_a-480.webp 480w,/assets/img/blog_img/comparison01exp_a-800.webp 800w,/assets/img/blog_img/comparison01exp_a-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_img/comparison01exp_a.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="machine_learning"/><summary type="html"><![CDATA[PINN for solving segregation problem]]></summary></entry><entry><title type="html">PINN for 2D segregation</title><link href="https://19revey.github.io/blog/2024/2dpinn/" rel="alternate" type="text/html" title="PINN for 2D segregation"/><published>2024-03-20T12:57:00+00:00</published><updated>2024-03-20T12:57:00+00:00</updated><id>https://19revey.github.io/blog/2024/2dpinn</id><content type="html" xml:base="https://19revey.github.io/blog/2024/2dpinn/"><![CDATA[ <h5 id="understand-the-granular-segregation-and-the-transport-equation">Understand the granular segregation and the transport equation</h5> <ul> <li><a href="https://arxiv.org/pdf/2309.13273.pdf">[1] General Model For Segregation Forces in Flowing Granular Mixtures</a></li> <li><a href="https://arxiv.org/pdf/1809.08089.pdf">[2] Diffusion, mixing, and segregation in confined granular flows</a></li> <li><a href="https://pubs.acs.org/doi/10.1021/acs.iecr.5b01268">[3] On Mixing and Segregation: From Fluids and Maps to Granular Solids and Advection–Diffusion Systems</a></li> </ul> <h2 id="introduction-">Introduction <a name="intro"></a></h2> <p>In this project, a PINN is trained to solve a 2D advection-segregation-diffusion problem.</p> \[\frac{\partial c_i}{\partial t} +\frac{\partial (w_{i}c_i)}{\partial z}=\frac{\partial}{\partial z} \Big( D\frac{\partial c_i}{\partial z} \Big),\] <p>with the following boundary conditions: \(\begin{cases} c_i(t=0, z) = 0.5 \\ w_{i}c_i|_{(t, z=0)}-D\frac{\partial c_i}{\partial z} |_{(t, z=H)} =0\\ w_{i}c_i|_{(t, z=0)}-D\frac{\partial c_i}{\partial z} |_{(t, z=H)}=0\\ \end{cases}\)<br/> The objective is to predict the transient concentration profile:</p> <p align="center"> <img src="/assets/img/proj_pinn/dem.png" alt="Description of the image" style="width: 50%;"/> </p> <h2 id="simplified-advection-diffusion-segregation">Simplified Advection-Diffusion-Segregation</h2> <p>Sample the domain with data points and build a Neural Network to minimize the loss function:</p> \[Loss_{domain}=\frac{\partial c_i}{\partial t} +\frac{\partial (w_{i}c_i)}{\partial z} -\frac{\partial}{\partial z} \Big( D\frac{\partial c_i}{\partial z} \Big),\] <p>Loss function for boundary data points:</p> \[Loss_{BC}=(w_{i}c_i) - D\frac{\partial c_i}{\partial z}.\] <p align="center"> <img src="/assets/img/proj_pinn/collocation.png" alt="Description of the image" style="width: 50%;"/> </p> <h3 id="linear-segregation">Linear Segregation</h3> <p>Assuming linear segregation velocity (Fan et al. 2014) and constant diffusion coefficient. \(\begin{cases} w_i=A\dot\gamma(1-c_i)\\ D=0.042\dot\gamma d^2\\ \end{cases}\)</p> <p>Large particle concentration profiles:</p> <p align="center"> <img src="/assets/img/proj_pinn/c_pinn.png" alt="Description of the image" style="width: 50%;"/> </p> <p align="center"> <img src="/assets/img/proj_pinn/c_profiles.png" alt="Description of the image" style="width: 100%;"/> </p> <h3 id="pressure-corrected-linear-segregation">Pressure-corrected Linear Segregation</h3> <p>Assuming linear segregation velocity (Fan et al. 2014) and constant diffusion coefficient. \(\begin{equation} \begin{cases} w_i=A\dot\gamma(1-c_i) \sqrt{\frac{P_0}{P}}\\ D=0.042\dot\gamma d^2\\ \end{cases} \end{equation}\)</p> <p>Large particle concentration profiles:</p> <p align="center"> <img src="/assets/img/proj_pinn/c_1_pinn.png" alt="Description of the image" style="width: 50%;"/> </p> <p align="center"> <img src="/assets/img/proj_pinn/c_1_profiles.png" alt="Description of the image" style="width: 100%;"/> </p> <h3 id="pressure-corrected-linear-segregation--concentration-dependent-diffusion-coefficient">Pressure-corrected Linear Segregation + concentration dependent diffusion coefficient</h3> <p>Assuming linear segregation velocity (Fan et al. 2014) and constant diffusion coefficient. \(\begin{equation} \begin{cases} w_i=A\dot\gamma(1-c_i) \sqrt{\frac{P_0}{P}}\\ D=0.042\dot\gamma (\sum c_id_i)^2\\ \end{cases} \end{equation}\)</p> <p>Large particle concentration profiles:</p> <p align="center"> <img src="/assets/img/proj_pinn/c_2_pinn.png" alt="Description of the image" style="width: 50%;"/> </p> <p align="center"> <img src="/assets/img/proj_pinn/c_2_profiles.png" alt="Description of the image" style="width: 100%;"/> </p> <h1 id="dem-informed-nn">DEM-informed NN</h1> <p>DEM simulations (\(R_d=2,~R_\rho=1,~t=37\,s\)):</p> <p align="center"> <img src="/assets/img/proj_pinn/dem_simulation.png" alt="Description of the image" style="width: 70%;"/> </p> <p>Segregation flux is formed with unkown variables to identify: \(\begin{equation} \begin{cases} w_i=F_{x1} \tanh(F_{x2} \frac{c_s}{c_l})/C_d\eta\\ \Phi=F_{x3}(w_i-0.042\dot\gamma (\sum c_id_i)^2 \frac{\partial c_l}{\partial z})\\ \end{cases} \end{equation}\)</p> <p align="center"> <img src="/assets/img/proj_pinn/pinn_dem.png" alt="Description of the image" style="width: 100%;"/> </p> <p>The variables are identified as: \(\begin{equation} \begin{cases} F_{x1}=2.1619\\ F_{x2}=1.3354\\ F_{x3}=2.4698\\ \end{cases} \end{equation}\)<br/> This implies a concentration dependence of force:</p> <p align="center"> <img src="/assets/img/proj_pinn/force.png" alt="Description of the image" style="width: 30%;"/> </p> <p align="center"> <img src="/assets/img/proj_pinn/pinn_dem_profiles.png" alt="Description of the image" style="width: 100%;"/> </p> <h3 id="dem-informed-nn---reduce-sample-size">DEM-informed NN - reduce sample size</h3> <p>DEM simulations (\(R_d=2,~R_\rho=1,~t=10\,s\)):</p> <p align="center"> <img src="/assets/img/proj_pinn/pinn_dem_reduced.png" alt="Description of the image" style="width: 100%;"/> </p> <p>The variables are identified as: \(\begin{equation} \begin{cases} F_{x1}=2.4664\\ F_{x2}=1.1263\\ F_{x3}=2.8693\\ \end{cases} \end{equation}\)</p> <p align="center"> <img src="/assets/img/proj_pinn/pinn_dem_profiles_reduced.png" alt="Description of the image" style="width: 100%;"/> </p> <h3 id="does-cubic-segregation-velocity-model-works-better">Does cubic segregation velocity model works better?</h3> \[\begin{equation} \begin{cases} w_i=F_{x1} c_l+ F_{x2}c_l^2+F_{x3}c_l^3\\ \Phi=F_{x4}(w_i-0.042\dot\gamma (\sum c_id_i)^2 \frac{\partial c_l}{\partial z})\\ \end{cases} \end{equation}\] <p>Cubic segregation velocity model cannot capture the DEM profiles.</p> <p align="center"> <img src="/assets/img/proj_pinn/pinn_dem_profiles_reduced1.png" alt="Description of the image" style="width: 100%;"/> </p>]]></content><author><name></name></author><category term="machine_learning"/><summary type="html"><![CDATA[PINN for 2D advection-diffusion-segregation equation]]></summary></entry><entry><title type="html">Frequently Used Tensor Operation</title><link href="https://19revey.github.io/blog/2024/torch/" rel="alternate" type="text/html" title="Frequently Used Tensor Operation"/><published>2024-01-07T12:00:00+00:00</published><updated>2024-01-07T12:00:00+00:00</updated><id>https://19revey.github.io/blog/2024/torch</id><content type="html" xml:base="https://19revey.github.io/blog/2024/torch/"><![CDATA[<h1 id="pytorch-tensor-manipulation-methods">PyTorch Tensor Manipulation Methods</h1> <p>This document summarizes commonly used PyTorch tensor manipulation functions, their purposes, and examples.</p> <h2 id="1-squeeze">1. <code class="language-plaintext highlighter-rouge">squeeze</code></h2> <p><strong>Function</strong>: Removes dimensions of size 1 from a tensor.</p> <p><strong>Example</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">tensor_squeezed</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">()</span>  <span class="c1"># Removes all dimensions of size 1, resulting shape: (3, 4)
</span><span class="n">tensor_squeezed_dim</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Removes size 1 from the 0th dimension, resulting shape: (3, 4, 1)
</span></code></pre></div></div> <h2 id="2-view">2. <code class="language-plaintext highlighter-rouge">view</code></h2> <p><strong>Function</strong>: Reshapes a tensor without changing its data storage order.</p> <p><strong>Example</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">tensor_reshaped</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>  <span class="c1"># Reshapes to shape: (2, 8)
</span></code></pre></div></div> <h2 id="3-reshape">3. <code class="language-plaintext highlighter-rouge">reshape</code></h2> <p><strong>Function</strong>: Similar to <code class="language-plaintext highlighter-rouge">view</code>, reshapes a tensor, but may create a copy of the data if necessary.</p> <p><strong>Example</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">tensor_reshaped</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>  <span class="c1"># Reshapes to shape: (2, 8)
</span></code></pre></div></div> <h2 id="4-permute">4. <code class="language-plaintext highlighter-rouge">permute</code></h2> <p><strong>Function</strong>: Reorders the dimensions of a tensor.</p> <p><strong>Example</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">tensor_permuted</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Changes dimension order, resulting shape: (4, 2, 3)
</span></code></pre></div></div> <h2 id="5-transpose">5. <code class="language-plaintext highlighter-rouge">transpose</code></h2> <p><strong>Function</strong>: Swaps two specified dimensions of a tensor.</p> <p><strong>Example</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">tensor_transposed</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Swaps the 1st and 2nd dimensions, resulting shape: (2, 4, 3)
</span></code></pre></div></div> <h2 id="6-expand-and-expand_as">6. <code class="language-plaintext highlighter-rouge">expand</code> and <code class="language-plaintext highlighter-rouge">expand_as</code></h2> <p><strong>Function</strong>: Expands a tensor to match a specified shape without copying data, useful for broadcasting.</p> <p><strong>Example</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">tensor_expanded</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">expand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># Expands to shape: (4, 3)
</span></code></pre></div></div> <h2 id="7-repeat">7. <code class="language-plaintext highlighter-rouge">repeat</code></h2> <p><strong>Function</strong>: Duplicates tensor data and repeats it along specified dimensions.</p> <p><strong>Example</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">tensor_repeated</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># Results in shape: (4, 6)
</span></code></pre></div></div> <h2 id="8-flatten">8. <code class="language-plaintext highlighter-rouge">flatten</code></h2> <p><strong>Function</strong>: Flattens a tensor into one dimension.</p> <p><strong>Example</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">tensor_flattened</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()</span>  <span class="c1"># Results in shape: (24,)
</span></code></pre></div></div> <p>These tensor manipulation methods help reshape, reorder, and expand tensors efficiently to fit different computational needs and network architectures.</p>]]></content><author><name></name></author><category term="machine_learning"/><summary type="html"><![CDATA[PyTorch]]></summary></entry><entry><title type="html">Transformer using PyTorch</title><link href="https://19revey.github.io/blog/2023/transformer/" rel="alternate" type="text/html" title="Transformer using PyTorch"/><published>2023-07-11T12:57:00+00:00</published><updated>2023-07-11T12:57:00+00:00</updated><id>https://19revey.github.io/blog/2023/transformer</id><content type="html" xml:base="https://19revey.github.io/blog/2023/transformer/"><![CDATA[ <h2 style="color:purple;font-size: 2em;">Overview</h2> <p>Basic transformer with an encoder-decoder architecture for language translation models.</p> <ul> <li>Understanding transformers <ul> <li><a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">attention is all you need</a></li> </ul> </li> <li>Pytorch implementation <ul> <li><a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html">https://pytorch.org/tutorials/beginner/transformer_tutorial.html</a></li> <li><a href="https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch">https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch</a></li> </ul> </li> </ul> <p><a class="anchor" id="section1"></a></p> <h2 style="color:purple;font-size: 2em;">1. Introduction</h2> <p><img src="https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png" width="330" height="470" img="" style="float: right;"/></p> <ul> <li><a href="#section1">1. Introduction</a></li> <li><a href="#section2">2. Import libraries</a></li> <li><a href="#section3">3. Basic components</a> <ul> <li><a href="#section4">Word Embeddings</a></li> <li><a href="#section5">Positional Encoding</a></li> <li><a href="#section6">Self Attention</a></li> <li><a href="#section7">Transformer Block</a></li> </ul> </li> <li><a href="#section8">4. Encoder</a></li> <li><a href="#section9">5. Decoder</a></li> <li><a href="#section10">6. Transformer</a></li> </ul> <p><a class="anchor" id="section2"></a></p> <h2 style="color:purple;font-size: 2em;">2. Import Libraries</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">__version__</span><span class="p">)</span>

<span class="n">device</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
    <span class="k">else</span> <span class="sh">"</span><span class="s">mps</span><span class="sh">"</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
    <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="p">)</span>
</code></pre></div></div> <p><a class="anchor" id="section3"></a></p> <h2 style="color:purple;font-size: 2em;">3. Basic components</h2> <p><a class="anchor" id="section4"></a></p> <h2 style="color:purple;font-size: 1.5em;">Word Embeddings</h2> <p>Each word will be mapped to corresponding \(d_{model}=512\) embedding vector. Suppose we have batch_size of 32 and sequence_length of 10 (10 words). The the output will be Batch_size X sequence_length X embedding_dim (32X10X512).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Embedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            vocab_size: size of vocabulary
            embed_dim: dimension of embeddings, i.e. d_model
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Embedding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            x: input vector, i.e. (batch, seq_len, vocab_size)
        Returns:
            out: embedding vector (batch, seq_len, embed_dim)
        </span><span class="sh">"""</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div> <p><a class="anchor" id="section5"></a></p> <h2 style="color:purple;font-size: 1.5em;"> Positional Encoding</h2> \[PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\] \[PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})\] <p>Here \(pos\) is the position of the word in the sentence, and \(i\) refers to position along embedding vector dimension.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PositionalEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">max_seq_len</span><span class="p">,</span><span class="n">embed_model_dim</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            seq_len: length of input sequence
            embed_model_dim: demension of embedding, d_model
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">PositionalEmbedding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_model_dim</span>

        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">,</span><span class="mi">2</span><span class="p">):</span>
                <span class="n">pe</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10000</span> <span class="o">**</span> <span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">)</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)))</span>
                <span class="n">pe</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10000</span> <span class="o">**</span> <span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)))</span>

        <span class="c1"># adding batch dimension for broadcasting
</span>        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> 

        <span class="c1"># register buffer in Pytorch -&gt;
</span>        <span class="c1"># If you have parameters in your model, which should be saved and restored in the state_dict,
</span>        <span class="c1"># but not trained by the optimizer, you should register them as buffers.
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">pe</span><span class="sh">'</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            x: input vector
        Returns:
            x: output
        </span><span class="sh">"""</span>
        <span class="c1"># make embeddings relatively larger
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="c1">#add constant to embedding
</span>        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">].</span><span class="nf">detach</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p><a class="anchor" id="section6"></a></p> <h2 style="color:purple; font-size: 1.5em;"> Self Attention</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            embed_dim: dimension of embeding vector output
            n_heads: number of self attention heads
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>    <span class="c1">#512 dim
</span>        <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>   <span class="c1">#8
</span>        <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">)</span>   <span class="c1">#512/8 = 64  . each key,query, value will be of 64d
</span>       
        <span class="c1">#key,query and value matrixes    #64 x 64   
</span>        <span class="n">self</span><span class="p">.</span><span class="n">query_matrix</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># single key matrix for all 8 keys #512x512
</span>        <span class="n">self</span><span class="p">.</span><span class="n">key_matrix</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span>  <span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value_matrix</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span> 

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">key</span><span class="p">,</span><span class="n">query</span><span class="p">,</span><span class="n">value</span><span class="p">,</span><span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>    <span class="c1">#batch_size x sequence_length x embedding_dim    # 32 x 10 x 512
</span>        
        <span class="sh">"""</span><span class="s">
        Args:
           key : key vector
           query : query vector
           value : value vector
           mask: mask for decoder
        
        Returns:
           output vector from multihead attention
        </span><span class="sh">"""</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">seq_length</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># query dimension can change in decoder during inference. 
</span>        <span class="c1"># so we cant take general seq_length
</span>        <span class="n">seq_length_query</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># 32x10x512
</span>        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="p">)</span>  <span class="c1">#batch_size x sequence_length x n_heads x single_head_dim = (32x10x8x64)
</span>        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length_query</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="p">)</span> <span class="c1">#(32x10x8x64)
</span>        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="p">)</span> <span class="c1">#(32x10x8x64)
</span>       
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key_matrix</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>       <span class="c1"># (32x10x8x64)
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query_matrix</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>   
        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value_matrix</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, n_heads, seq_len, single_head_dim)    # (32 x 8 x 10 x 64)
</span>        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, n_heads, seq_len, single_head_dim)
</span>        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, n_heads, seq_len, single_head_dim)
</span>       
        <span class="c1"># computes attention
</span>        <span class="c1"># adjust key for matrix multiplication
</span>        <span class="n">k_adjusted</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1">#(batch_size, n_heads, single_head_dim, seq_ken)  #(32 x 8 x 64 x 10)
</span>        <span class="n">product</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k_adjusted</span><span class="p">)</span>  <span class="c1">#(32 x 8 x 10 x 64) x (32 x 8 x 64 x 10) = #(32x8x10x10)
</span>        
        <span class="c1"># fill those positions of product matrix as (-1e20) where mask positions are 0
</span>        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
             <span class="n">product</span> <span class="o">=</span> <span class="n">product</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">"</span><span class="s">-1e20</span><span class="sh">"</span><span class="p">))</span>

        <span class="c1">#divising by square root of key dimension
</span>        <span class="n">product</span> <span class="o">=</span> <span class="n">product</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="p">)</span> <span class="c1"># / sqrt(64)
</span>
        <span class="c1">#applying softmax
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">product</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
 
        <span class="c1">#mutiply with value matrix
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>  <span class="c1">##(32x8x 10x 10) x (32 x 8 x 10 x 64) = (32 x 8 x 10 x 64) 
</span>        
        <span class="c1">#concatenated output
</span>        <span class="n">concat</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length_query</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">)</span>  <span class="c1"># (32x8x10x64) -&gt; (32x10x8x64)  -&gt; (32,10,512)
</span>        
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">out</span><span class="p">(</span><span class="n">concat</span><span class="p">)</span> <span class="c1">#(32,10,512) -&gt; (32,10,512)
</span>       
        <span class="k">return</span> <span class="n">output</span>

</code></pre></div></div> <p><a class="anchor" id="section7"></a></p> <h2 style="color:purple; font-size: 1.5em;"> Transformer Block</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="sh">"""</span><span class="s">
        Args:
           embed_dim: dimension of the embedding
           expansion_factor: fator ehich determines output dimension of linear layer
           n_heads: number of attention heads
        
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                          <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">*</span><span class="n">embed_dim</span><span class="p">),</span>
                          <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
                          <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">expansion_factor</span><span class="o">*</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">key</span><span class="p">,</span><span class="n">query</span><span class="p">,</span><span class="n">value</span><span class="p">):</span>
        
        <span class="sh">"""</span><span class="s">
        Args:
           key: key vector
           query: query vector
           value: value vector
           norm2_out: output of transformer block
        
        </span><span class="sh">"""</span>
        
        <span class="n">attention_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">key</span><span class="p">,</span><span class="n">query</span><span class="p">,</span><span class="n">value</span><span class="p">)</span>  <span class="c1">#32x10x512
</span>        <span class="n">attention_residual_out</span> <span class="o">=</span> <span class="n">attention_out</span> <span class="o">+</span> <span class="n">value</span>  <span class="c1">#32x10x512
</span>        <span class="n">norm1_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout1</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">norm1</span><span class="p">(</span><span class="n">attention_residual_out</span><span class="p">))</span> <span class="c1">#32x10x512
</span>
        <span class="n">feed_fwd_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feed_forward</span><span class="p">(</span><span class="n">norm1_out</span><span class="p">)</span> <span class="c1">#32x10x512 -&gt; #32x10x2048 -&gt; 32x10x512
</span>        <span class="n">feed_fwd_residual_out</span> <span class="o">=</span> <span class="n">feed_fwd_out</span> <span class="o">+</span> <span class="n">norm1_out</span> <span class="c1">#32x10x512
</span>        <span class="n">norm2_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout2</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">norm2</span><span class="p">(</span><span class="n">feed_fwd_residual_out</span><span class="p">))</span> <span class="c1">#32x10x512
</span>
        <span class="k">return</span> <span class="n">norm2_out</span>

</code></pre></div></div> <p><a class="anchor" id="section8"></a></p> <h2 style="color:purple;font-size: 2em;"> 4. Encoder</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Args:
        seq_len : length of input sequence
        embed_dim: dimension of embedding
        num_layers: number of encoder layers
        expansion_factor: factor which determines number of linear layers in feed forward layer
        n_heads: number of heads in multihead attention
        
    Returns:
        out: output of the encoder
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerEncoder</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">embedding_layer</span> <span class="o">=</span> <span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">positional_encoder</span> <span class="o">=</span> <span class="nc">PositionalEmbedding</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)])</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">embed_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">positional_encoder</span><span class="p">(</span><span class="n">embed_out</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">out</span><span class="p">,</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>  <span class="c1">#32x10x512
</span></code></pre></div></div> <p><a class="anchor" id="section9"></a></p> <h2 style="color:purple;font-size: 2em;"> 5. Decoder</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DecoderBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">DecoderBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="sh">"""</span><span class="s">
        Args:
           embed_dim: dimension of the embedding
           expansion_factor: fator ehich determines output dimension of linear layer
           n_heads: number of attention heads
        
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">transformer_block</span> <span class="o">=</span> <span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>       
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span><span class="n">mask</span><span class="p">):</span>
        
        <span class="sh">"""</span><span class="s">
        Args:
           key: key vector
           query: query vector
           value: value vector
           mask: mask to be given for multi head attention 
        Returns:
           out: output of transformer block
    
        </span><span class="sh">"""</span>
        <span class="c1">#we need to pass mask mask only to fst attention
</span>        <span class="n">attention</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span> <span class="c1">#32x10x512
</span>        <span class="n">value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">attention</span> <span class="o">+</span> <span class="n">x</span><span class="p">))</span>
        
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transformer_block</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">out</span>


<span class="k">class</span> <span class="nc">TransformerDecoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerDecoder</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="sh">"""</span><span class="s">  
        Args:
           target_vocab_size: vocabulary size of taget
           embed_dim: dimension of embedding
           seq_len : length of input sequence
           num_layers: number of encoder layers
           expansion_factor: factor which determines number of linear layers in feed forward layer
           n_heads: number of heads in multihead attention
        
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">word_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">position_embedding</span> <span class="o">=</span> <span class="nc">PositionalEmbedding</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="nc">DecoderBlock</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span> 
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
            <span class="p">]</span>

        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_out</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        
        <span class="sh">"""</span><span class="s">
        Args:
            x: input vector from target
            enc_out : output from encoder layer
            trg_mask: mask for decoder self attention
        Returns:
            out: output vector
        </span><span class="sh">"""</span>     
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">word_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1">#32x10x512
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">position_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#32x10x512
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
     
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">enc_out</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_out</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> 

        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc_out</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">out</span>

</code></pre></div></div> <p><a class="anchor" id="section10"></a></p> <h2 style="color:purple;font-size: 2em;"> 6. Transformer</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span><span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="sh">"""</span><span class="s">  
        Args:
           embed_dim:  dimension of embedding 
           src_vocab_size: vocabulary size of source
           target_vocab_size: vocabulary size of target
           seq_length : length of input sequence
           num_layers: number of encoder layers
           expansion_factor: factor which determines number of linear layers in feed forward layer
           n_heads: number of heads in multihead attention
        
        </span><span class="sh">"""</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">target_vocab_size</span> <span class="o">=</span> <span class="n">target_vocab_size</span>

        <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="n">expansion_factor</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="n">n_heads</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="nc">TransformerDecoder</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="n">expansion_factor</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="n">n_heads</span><span class="p">)</span>
        
    
    <span class="k">def</span> <span class="nf">make_trg_mask</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">trg</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            trg: target sequence
        Returns:
            trg_mask: target mask
        </span><span class="sh">"""</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">trg_len</span> <span class="o">=</span> <span class="n">trg</span><span class="p">.</span><span class="n">shape</span>
        <span class="c1"># returns the lower triangular part of matrix filled with ones
</span>        <span class="n">trg_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">trg_len</span><span class="p">,</span> <span class="n">trg_len</span><span class="p">))).</span><span class="nf">expand</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">trg_len</span><span class="p">,</span> <span class="n">trg_len</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">trg_mask</span>    

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">src</span><span class="p">,</span><span class="n">trg</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        for inference
        Args:
            src: input to encoder 
            trg: input to decoder
        out:
            out_labels : returns final prediction of sequence
        </span><span class="sh">"""</span>
        <span class="n">trg_mask</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">make_trg_mask</span><span class="p">(</span><span class="n">trg</span><span class="p">)</span>
        <span class="n">enc_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
        <span class="n">out_labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">batch_size</span><span class="p">,</span><span class="n">seq_len</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">src</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1">#outputs = torch.zeros(seq_len, batch_size, self.target_vocab_size)
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">trg</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span> <span class="c1">#10
</span>            <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">enc_out</span><span class="p">,</span><span class="n">trg_mask</span><span class="p">)</span> <span class="c1">#bs x seq_len x vocab_dim
</span>            <span class="c1"># taking the last token
</span>            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span>
     
            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">out_labels</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
          
        
        <span class="k">return</span> <span class="n">out_labels</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">trg</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            src: input to encoder 
            trg: input to decoder
        out:
            out: final vector which returns probabilities of each target word
        </span><span class="sh">"""</span>
        <span class="n">trg_mask</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">make_trg_mask</span><span class="p">(</span><span class="n">trg</span><span class="p">)</span>
        <span class="n">enc_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
   
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(</span><span class="n">trg</span><span class="p">,</span> <span class="n">enc_out</span><span class="p">,</span> <span class="n">trg_mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outputs</span>


</code></pre></div></div>]]></content><author><name></name></author><category term="machine_learning"/><summary type="html"><![CDATA[based on the paper "Attention is all you need"]]></summary></entry><entry><title type="html">U-Net using PyTorch</title><link href="https://19revey.github.io/blog/2023/unet/" rel="alternate" type="text/html" title="U-Net using PyTorch"/><published>2023-05-15T12:57:00+00:00</published><updated>2023-05-15T12:57:00+00:00</updated><id>https://19revey.github.io/blog/2023/unet</id><content type="html" xml:base="https://19revey.github.io/blog/2023/unet/"><![CDATA[ <p><a class="anchor" id="section1"></a></p> <h2 style="color:purple;font-size: 2em;">1. Introduction</h2> <p>The U-Net architecture is characterized by its unique symmetric U-shaped design, which consists of a contracting path (encoder) and an expansive path (decoder), connected by skip connections. This architecture enables the network to capture both local and global features while preserving spatial information, making it highly effective for pixel-level segmentation tasks.</p> <ul> <li><a href="https://arxiv.org/pdf/1505.04597">Reference: U-Net: Convolutional Networks for Biomedical Image Segmentation</a> <ul> <li>Weighted loss for seperation of touching objects of the same class</li> <li>Overalp-tile strategy for arbitrary large images</li> </ul> </li> </ul> <p><img src="https://miro.medium.com/v2/resize:fit:1400/1*f7YOaE4TWubwaFF7Z1fzNw.png" width="700" height="457" img="" style="float: top;"/></p> <p><a class="anchor" id="section3"></a></p> <h2 style="color:purple;font-size: 2em;">2. Components</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DoubleConv</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">(convolution =&gt; [BN] =&gt; ReLU) * 2</span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">mid_channels</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">mid_channels</span><span class="p">:</span>
            <span class="n">mid_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="n">self</span><span class="p">.</span><span class="n">double_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">mid_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">mid_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">mid_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">double_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Down</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Downscaling with maxpool then double conv</span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">maxpool_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="nc">DoubleConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">maxpool_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Up</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Upscaling then double conv</span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">bilinear</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1"># if bilinear, use the normal convolutions to reduce the number of channels
</span>        <span class="k">if</span> <span class="n">bilinear</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">up</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">bilinear</span><span class="sh">'</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="nc">DoubleConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">up</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ConvTranspose2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="nc">DoubleConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="c1"># input is CHW
</span>        <span class="n">diffY</span> <span class="o">=</span> <span class="n">x2</span><span class="p">.</span><span class="nf">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">x1</span><span class="p">.</span><span class="nf">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">diffX</span> <span class="o">=</span> <span class="n">x2</span><span class="p">.</span><span class="nf">size</span><span class="p">()[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">x1</span><span class="p">.</span><span class="nf">size</span><span class="p">()[</span><span class="mi">3</span><span class="p">]</span>

        <span class="n">x1</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="p">[</span><span class="n">diffX</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">diffX</span> <span class="o">-</span> <span class="n">diffX</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span>
                        <span class="n">diffY</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">diffY</span> <span class="o">-</span> <span class="n">diffY</span> <span class="o">//</span> <span class="mi">2</span><span class="p">])</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">x2</span><span class="p">,</span> <span class="n">x1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">OutConv</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">OutConv</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

</code></pre></div></div> <p><a class="anchor" id="section8"></a></p> <h2 style="color:purple;font-size: 2em;"> 3. UNet</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">UNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_channels</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">bilinear</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">UNet</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_channels</span> <span class="o">=</span> <span class="n">n_channels</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_classes</span> <span class="o">=</span> <span class="n">n_classes</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bilinear</span> <span class="o">=</span> <span class="n">bilinear</span>

        <span class="n">self</span><span class="p">.</span><span class="n">inc</span> <span class="o">=</span> <span class="p">(</span><span class="nc">DoubleConv</span><span class="p">(</span><span class="n">n_channels</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down1</span> <span class="o">=</span> <span class="p">(</span><span class="nc">Down</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down2</span> <span class="o">=</span> <span class="p">(</span><span class="nc">Down</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down3</span> <span class="o">=</span> <span class="p">(</span><span class="nc">Down</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">))</span>
        <span class="n">factor</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">bilinear</span> <span class="k">else</span> <span class="mi">1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down4</span> <span class="o">=</span> <span class="p">(</span><span class="nc">Down</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span> <span class="o">//</span> <span class="n">factor</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up1</span> <span class="o">=</span> <span class="p">(</span><span class="nc">Up</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">512</span> <span class="o">//</span> <span class="n">factor</span><span class="p">,</span> <span class="n">bilinear</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up2</span> <span class="o">=</span> <span class="p">(</span><span class="nc">Up</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span> <span class="o">//</span> <span class="n">factor</span><span class="p">,</span> <span class="n">bilinear</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up3</span> <span class="o">=</span> <span class="p">(</span><span class="nc">Up</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span> <span class="o">//</span> <span class="n">factor</span><span class="p">,</span> <span class="n">bilinear</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up4</span> <span class="o">=</span> <span class="p">(</span><span class="nc">Up</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">bilinear</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">outc</span> <span class="o">=</span> <span class="p">(</span><span class="nc">OutConv</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">inc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down1</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down2</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
        <span class="n">x4</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down3</span><span class="p">(</span><span class="n">x3</span><span class="p">)</span>
        <span class="n">x5</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down4</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up1</span><span class="p">(</span><span class="n">x5</span><span class="p">,</span> <span class="n">x4</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x3</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up4</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">outc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>

    <span class="k">def</span> <span class="nf">use_checkpointing</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">inc</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">inc</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down3</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down4</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down4</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up3</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up4</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up4</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">outc</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">outc</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="machine_learning"/><summary type="html"><![CDATA[based on the paper "U-Net Convolutional Networks for Biomedical Image Segmentation"]]></summary></entry><entry><title type="html">Improve resume using LLM</title><link href="https://19revey.github.io/blog/2023/resume/" rel="alternate" type="text/html" title="Improve resume using LLM"/><published>2023-01-02T17:39:00+00:00</published><updated>2023-01-02T17:39:00+00:00</updated><id>https://19revey.github.io/blog/2023/resume</id><content type="html" xml:base="https://19revey.github.io/blog/2023/resume/"><![CDATA[<h2 id="the-app-is-hosted-on-streamlit-cloud-httpsllmresumestreamlitapp">The app is hosted on streamlit cloud: <a href="https://llmresume.streamlit.app/">https://llmresume.streamlit.app/</a></h2> <p><br/></p> <h3 id="paste-the-job-description-and-upload-your-resume-to-obtain-insights-including">Paste the job description and upload your resume to obtain insights, including:</h3> <ul> <li>an overall match percentage;</li> <li>key skills that should be highlighted in your resume;</li> <li>identification of keywords from the job description that are not present in your resume.</li> </ul> <p><br/></p> <h4 id="to-run-it-locally">To run it locally</h4> <p>Make sure docker is installed, otherwise run installdocker.sh first:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sh installdocker.sh
</code></pre></div></div> <p>Download source code:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/19revey/LLM_resume.git
</code></pre></div></div> <p>Build docker image and start container:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker compose up
</code></pre></div></div>]]></content><author><name></name></author><category term="machine_learning"/><summary type="html"><![CDATA[improve resume based on job description using Google Gemini Pro]]></summary></entry><entry><title type="html">Running VScode an IPad</title><link href="https://19revey.github.io/blog/2022/ipad/" rel="alternate" type="text/html" title="Running VScode an IPad"/><published>2022-05-16T01:57:00+00:00</published><updated>2022-05-16T01:57:00+00:00</updated><id>https://19revey.github.io/blog/2022/ipad</id><content type="html" xml:base="https://19revey.github.io/blog/2022/ipad/"><![CDATA[<h1 align="left" style="color:purple;font-size: 2em;">Overview</h1> <p>The iPad pro boasts a powerful chip, but its productivity features remain somewhat limited. While iPadOS doesn’t currently support VSCode, there are several workarounds to enable coding on the iPad, taking advantage of its exceptional touch support and stunning HDR screen.</p> <p>https://github.com/coder/code-server</p> <p>https://docs.linuxserver.io/images/docker-code-server/</p> <p><br/></p> <p><a class="anchor" id="section1"></a></p> <h2 style="color:purple;font-size: 2em;">Setting up docker running code-server</h2> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> lscr.io/linuxserver/code-server:latest</span>

<span class="k">ENV</span><span class="s"> DEBIAN_FRONTEND=noninteractive </span>

<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="se">\
</span>    build-essential <span class="se">\
</span>    software-properties-common <span class="se">\
</span>    git <span class="se">\
</span>    python3-pip <span class="se">\
</span>    <span class="o">&amp;&amp;</span> apt-get clean <span class="o">&amp;&amp;</span> <span class="nb">rm</span> <span class="nt">-rf</span> /var/lib/apt/lists/<span class="k">*</span> /var/cache/apt/archives/<span class="k">*</span>

<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="k">COPY</span><span class="s"> . /app</span>

<span class="k">RUN </span>pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip
<span class="k">RUN </span>pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt

</code></pre></div></div> <p>Automated building and running using Docker Compose</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code>services:

  code-server:
    # image: lscr.io/linuxserver/code-server:latest
    build: .
    container_name: code-server
    runtime: nvidia
    environment:

      - PUID=1000

      - PGID=1000

      - TZ=Etc/UTC

      - PASSWORD= #optional

      - HASHED_PASSWORD= #optional

      - SUDO_PASSWORD= #optional

      - SUDO_PASSWORD_HASH= #optional

      - PROXY_DOMAIN=code-server.my.domain <span class="c">#optional</span>

      - DEFAULT_WORKSPACE=/config/workspace <span class="c">#optional</span>

      - NVIDIA_VISIBLE_DEVICES=all
    
    volumes:
      - /path/to/appdata/config:/config

      - ../../:/config/workspace
    ports:
      - 8443:8443
</code></pre></div></div>]]></content><author><name></name></author><category term="machine_learning"/><summary type="html"><![CDATA[taking advantage of IPad's exceptional touch support and stunning tandem OLED screen]]></summary></entry></feed>