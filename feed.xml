<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://19revey.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://19revey.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-18T21:34:34+00:00</updated><id>https://19revey.github.io/feed.xml</id><title type="html">Yifei Duan</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">How to Code on an IPad</title><link href="https://19revey.github.io/blog/2024/ipad/" rel="alternate" type="text/html" title="How to Code on an IPad"/><published>2024-05-16T01:57:00+00:00</published><updated>2024-05-16T01:57:00+00:00</updated><id>https://19revey.github.io/blog/2024/ipad</id><content type="html" xml:base="https://19revey.github.io/blog/2024/ipad/"><![CDATA[<h1 align="left" style="color:purple;font-size: 2em;">Overview</h1> <h3 id="the-ipad-2024-boasts-a-powerful-m4-chip-but-its-productivity-features-remain-somewhat-limited-while-ipados-doesnt-currently-support-vscode-there-are-several-workarounds-to-enable-coding-on-the-ipad-taking-advantage-of-its-exceptional-touch-support-and-stunning-tandem-oled-screen">The iPad 2024 boasts a powerful M4 chip, but its productivity features remain somewhat limited. While iPadOS doesn’t currently support VSCode, there are several workarounds to enable coding on the iPad, taking advantage of its exceptional touch support and stunning tandem OLED screen.</h3> <p><a class="anchor" id="section1"></a></p> <h2 style="color:purple;font-size: 2em;">Setting up docker running code-server</h2> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> lscr.io/linuxserver/code-server:latest</span>

<span class="k">ENV</span><span class="s"> DEBIAN_FRONTEND=noninteractive </span>

<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="se">\
</span>    build-essential <span class="se">\
</span>    software-properties-common <span class="se">\
</span>    git <span class="se">\
</span>    python3-pip <span class="se">\
</span>    <span class="o">&amp;&amp;</span> apt-get clean <span class="o">&amp;&amp;</span> <span class="nb">rm</span> <span class="nt">-rf</span> /var/lib/apt/lists/<span class="k">*</span> /var/cache/apt/archives/<span class="k">*</span>

<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="k">COPY</span><span class="s"> . /app</span>

<span class="k">RUN </span>pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip
<span class="k">RUN </span>pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt

</code></pre></div></div> <p>Automated building and running using Docker Compose</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code>services:

  code-server:
    # image: lscr.io/linuxserver/code-server:latest
    build: .
    container_name: code-server
    runtime: nvidia
    environment:

      - PUID=1000

      - PGID=1000

      - TZ=Etc/UTC

      - PASSWORD= #optional

      - HASHED_PASSWORD= #optional

      - SUDO_PASSWORD= #optional

      - SUDO_PASSWORD_HASH= #optional

      - PROXY_DOMAIN=code-server.my.domain <span class="c">#optional</span>

      - DEFAULT_WORKSPACE=/config/workspace <span class="c">#optional</span>

      - NVIDIA_VISIBLE_DEVICES=all
    
    volumes:
      - /path/to/appdata/config:/config

      - ../../:/config/workspace
    ports:
      - 8443:8443
</code></pre></div></div> <p><a class="anchor" id="section2"></a></p> <h2 style="color:purple;font-size: 2em;">2. Import Libraries and define crop function</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="n">torchvision.utils</span> <span class="kn">import</span> <span class="n">make_grid</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">__version__</span><span class="p">)</span>

<span class="n">device</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
    <span class="k">else</span> <span class="sh">"</span><span class="s">mps</span><span class="sh">"</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
    <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">crop</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
    Function for cropping an image tensor: Given an image tensor and the new shape,
    crops to the center pixels.
    Parameters:
        image: image tensor of shape (batch size, channels, height, width)
        new_shape: a torch.Size object with the shape you want x to have
    </span><span class="sh">'''</span>
    <span class="n">middle_height</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">middle_width</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">starting_height</span> <span class="o">=</span> <span class="n">middle_height</span> <span class="o">-</span> <span class="nf">round</span><span class="p">(</span><span class="n">new_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">final_height</span> <span class="o">=</span> <span class="n">starting_height</span> <span class="o">+</span> <span class="n">new_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">starting_width</span> <span class="o">=</span> <span class="n">middle_width</span> <span class="o">-</span> <span class="nf">round</span><span class="p">(</span><span class="n">new_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">final_width</span> <span class="o">=</span> <span class="n">starting_width</span> <span class="o">+</span> <span class="n">new_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
    <span class="n">cropped_image</span> <span class="o">=</span> <span class="n">image</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">starting_height</span><span class="p">:</span><span class="n">final_height</span><span class="p">,</span> <span class="n">starting_width</span><span class="p">:</span><span class="n">final_width</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">cropped_image</span>

</code></pre></div></div> <p><a class="anchor" id="section3"></a></p> <h2 style="color:purple;font-size: 2em;">3. Basic components</h2> <p><a class="anchor" id="section4"></a></p> <h2 style="color:purple;font-size: 1.5em;">Contracting Block</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ContractingBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
    ContractingBlock Class
    Performs two convolutions followed by a max pool operation.
    Values:
        input_channels: the number of channels to expect from a given input
    </span><span class="sh">'''</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">,</span> <span class="n">use_dropout</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">use_bn</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ContractingBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">input_channels</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">input_channels</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">use_bn</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">batchnorm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">input_channels</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">use_bn</span> <span class="o">=</span> <span class="n">use_bn</span>
        <span class="k">if</span> <span class="n">use_dropout</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">use_dropout</span> <span class="o">=</span> <span class="n">use_dropout</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s">
        Function for completing a forward pass of ContractingBlock: 
        Given an image tensor, completes a contracting block and returns the transformed tensor.
        Parameters:
            x: image tensor of shape (batch size, channels, height, width)
        </span><span class="sh">'''</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">use_bn</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">batchnorm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">use_dropout</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">use_bn</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">batchnorm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">use_dropout</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p><a class="anchor" id="section5"></a></p> <h2 style="color:purple;font-size: 1.5em;"> Expanding Block</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ExpandingBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
    ExpandingBlock Class:
    Performs an upsampling, a convolution, a concatenation of its two inputs,
    followed by two more convolutions with optional dropout
    Values:
        input_channels: the number of channels to expect from a given input
    </span><span class="sh">'''</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">,</span> <span class="n">use_dropout</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">use_bn</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ExpandingBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">upsample</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">bilinear</span><span class="sh">'</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">input_channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">input_channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">input_channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">input_channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">use_bn</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">batchnorm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">input_channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">use_bn</span> <span class="o">=</span> <span class="n">use_bn</span>
        <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">use_dropout</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">use_dropout</span> <span class="o">=</span> <span class="n">use_dropout</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">skip_con_x</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s">
        Function for completing a forward pass of ExpandingBlock: 
        Given an image tensor, completes an expanding block and returns the transformed tensor.
        Parameters:
            x: image tensor of shape (batch size, channels, height, width)
            skip_con_x: the image tensor from the contracting path (from the opposing block of x)
                    for the skip connection
        </span><span class="sh">'''</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">upsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">skip_con_x</span> <span class="o">=</span> <span class="nf">crop</span><span class="p">(</span><span class="n">skip_con_x</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">skip_con_x</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">use_bn</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">batchnorm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">use_dropout</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">use_bn</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">batchnorm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">use_dropout</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p><a class="anchor" id="section6"></a></p> <h2 style="color:purple; font-size: 1.5em;"> Feature Map Block</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FeatureMapBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
    FeatureMapBlock Class
    The final layer of a U-Net - 
    maps each pixel to a pixel with the correct number of output dimensions
    using a 1x1 convolution.
    Values:
        input_channels: the number of channels to expect from a given input
        output_channels: the number of channels to expect for a given output
    </span><span class="sh">'''</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">FeatureMapBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s">
        Function for completing a forward pass of FeatureMapBlock: 
        Given an image tensor, returns it mapped to the desired number of channels.
        Parameters:
            x: image tensor of shape (batch size, channels, height, width)
        </span><span class="sh">'''</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

</code></pre></div></div> <p><a class="anchor" id="section8"></a></p> <h2 style="color:purple;font-size: 2em;"> 4. UNet</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">UNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
    UNet Class
    A series of 4 contracting blocks followed by 4 expanding blocks to 
    transform an input image into the corresponding paired image, with an upfeature
    layer at the start and a downfeature layer at the end.
    Values:
        input_channels: the number of channels to expect from a given input
        output_channels: the number of channels to expect for a given output
    </span><span class="sh">'''</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">UNet</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">upfeature</span> <span class="o">=</span> <span class="nc">FeatureMapBlock</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">contract1</span> <span class="o">=</span> <span class="nc">ContractingBlock</span><span class="p">(</span><span class="n">hidden_channels</span><span class="p">,</span> <span class="n">use_dropout</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">contract2</span> <span class="o">=</span> <span class="nc">ContractingBlock</span><span class="p">(</span><span class="n">hidden_channels</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">use_dropout</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">contract3</span> <span class="o">=</span> <span class="nc">ContractingBlock</span><span class="p">(</span><span class="n">hidden_channels</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">use_dropout</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">contract4</span> <span class="o">=</span> <span class="nc">ContractingBlock</span><span class="p">(</span><span class="n">hidden_channels</span> <span class="o">*</span> <span class="mi">8</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">contract5</span> <span class="o">=</span> <span class="nc">ContractingBlock</span><span class="p">(</span><span class="n">hidden_channels</span> <span class="o">*</span> <span class="mi">16</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">contract6</span> <span class="o">=</span> <span class="nc">ContractingBlock</span><span class="p">(</span><span class="n">hidden_channels</span> <span class="o">*</span> <span class="mi">32</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">expand0</span> <span class="o">=</span> <span class="nc">ExpandingBlock</span><span class="p">(</span><span class="n">hidden_channels</span> <span class="o">*</span> <span class="mi">64</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">expand1</span> <span class="o">=</span> <span class="nc">ExpandingBlock</span><span class="p">(</span><span class="n">hidden_channels</span> <span class="o">*</span> <span class="mi">32</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">expand2</span> <span class="o">=</span> <span class="nc">ExpandingBlock</span><span class="p">(</span><span class="n">hidden_channels</span> <span class="o">*</span> <span class="mi">16</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">expand3</span> <span class="o">=</span> <span class="nc">ExpandingBlock</span><span class="p">(</span><span class="n">hidden_channels</span> <span class="o">*</span> <span class="mi">8</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">expand4</span> <span class="o">=</span> <span class="nc">ExpandingBlock</span><span class="p">(</span><span class="n">hidden_channels</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">expand5</span> <span class="o">=</span> <span class="nc">ExpandingBlock</span><span class="p">(</span><span class="n">hidden_channels</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">downfeature</span> <span class="o">=</span> <span class="nc">FeatureMapBlock</span><span class="p">(</span><span class="n">hidden_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sigmoid</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s">
        Function for completing a forward pass of UNet: 
        Given an image tensor, passes it through U-Net and returns the output.
        Parameters:
            x: image tensor of shape (batch size, channels, height, width)
        </span><span class="sh">'''</span>
        <span class="n">x0</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">upfeature</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">contract1</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">contract2</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">contract3</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
        <span class="n">x4</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">contract4</span><span class="p">(</span><span class="n">x3</span><span class="p">)</span>
        <span class="n">x5</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">contract5</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>
        <span class="n">x6</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">contract6</span><span class="p">(</span><span class="n">x5</span><span class="p">)</span>
        <span class="n">x7</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">expand0</span><span class="p">(</span><span class="n">x6</span><span class="p">,</span> <span class="n">x5</span><span class="p">)</span>
        <span class="n">x8</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">expand1</span><span class="p">(</span><span class="n">x7</span><span class="p">,</span> <span class="n">x4</span><span class="p">)</span>
        <span class="n">x9</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">expand2</span><span class="p">(</span><span class="n">x8</span><span class="p">,</span> <span class="n">x3</span><span class="p">)</span>
        <span class="n">x10</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">expand3</span><span class="p">(</span><span class="n">x9</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
        <span class="n">x11</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">expand4</span><span class="p">(</span><span class="n">x10</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
        <span class="n">x12</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">expand5</span><span class="p">(</span><span class="n">x11</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>
        <span class="n">xn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">downfeature</span><span class="p">(</span><span class="n">x12</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">xn</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="machine_learning"/><summary type="html"><![CDATA[taking advantage of IPad's exceptional touch support and stunning tandem OLED screen]]></summary></entry><entry><title type="html">Physics Informed Neural Network</title><link href="https://19revey.github.io/blog/2024/pinn/" rel="alternate" type="text/html" title="Physics Informed Neural Network"/><published>2024-03-20T12:57:00+00:00</published><updated>2024-03-20T12:57:00+00:00</updated><id>https://19revey.github.io/blog/2024/pinn</id><content type="html" xml:base="https://19revey.github.io/blog/2024/pinn/"><![CDATA[ <h2 style="color:purple;font-size: 2em;">Overview</h2> <ul> <li><a href="#section1">1. Introduction</a></li> <li><a href="#section2">2. Neural Netowork</a></li> <li><a href="#section3">3. Model Training</a></li> <li><a href="#section4">4. Results</a></li> <li><a href="#section5">5. Improvements</a></li> </ul> <h5 id="links-to-run-the-code">Links to run the code</h5> <ul> <li><a href="https://colab.research.google.com/github/19revey/PINN_granular_segregation/blob/main/notebook/solver_segregation.ipynb">Google colab</a></li> <li><a href="https://github.com/19revey/PINN_granular_segregation">Github repo</a></li> </ul> <h5 id="understand-the-granular-segregation-and-the-transport-equation">Understand the granular segregation and the transport equation</h5> <ul> <li><a href="https://arxiv.org/pdf/2309.13273.pdf">[1] General Model For Segregation Forces in Flowing Granular Mixtures</a></li> <li><a href="https://arxiv.org/pdf/1809.08089.pdf">[2] Diffusion, mixing, and segregation in confined granular flows</a></li> <li><a href="https://pubs.acs.org/doi/10.1021/acs.iecr.5b01268">[3] On Mixing and Segregation: From Fluids and Maps to Granular Solids and Advection–Diffusion Systems</a></li> </ul> <h5 id="pinn-implementation">PINN implementation</h5> <ul> <li><a href="https://github.com/nanditadoloi/PINN">https://github.com/nanditadoloi/PINN</a></li> <li><a href="https://github.com/omniscientoctopus/Physics-Informed-Neural-Networks">https://github.com/omniscientoctopus/Physics-Informed-Neural-Networks</a></li> <li><a href="https://github.com/maziarraissi/PINNs">https://github.com/maziarraissi/PINNs</a></li> </ul> <p>Here is a li</p> <p><a class="anchor" id="section1"></a></p> <h2 style="color:purple;font-size: 2em;">1. Introduction</h2> <p>An advection-diffusion transport equation has been successfully used to model the segregation. Within this continuum framework, the concentration of species \(i\) can be expressed as</p> \[\frac{\partial c_i}{\partial t} + {\nabla \cdot (\pmb u c_i)}={\nabla \cdot (D\nabla c_i)}.\] <p>With assumption of incompressible flow and negligible vertical acceleration, the above equation in the \(z\) direction can be written as</p> \[\frac{\partial c_i}{\partial t} +\frac{\partial (w+w_{i})c_i}{\partial z}=\frac{\partial}{\partial z} \Big( D\frac{\partial c_i}{\partial z} \Big),\] <p>or, rearranging, as</p> \[w_{i}c_i-D\frac{\partial c_i}{\partial z}=0,\] <p>where \(w_{i}\) is the segregation velocity relative to the bulk velocity \(w\).</p> <table> <thead> <tr> <th> </th> <th> </th> <th>full model</th> <th>simplified model</th> </tr> </thead> <tbody> <tr> <td>intruder scaled segregation force</td> <td>\(F_{i,0}\)</td> <td>\(-f^g(R)\frac{\partial{p}}{\partial{z}}V_i+f^k(R)\frac{p}{\dot\gamma}\frac{\partial\dot\gamma}{\partial{z}}V_i\)</td> <td> </td> </tr> <tr> <td>Mixture scaled segregation force</td> <td>\(\hat F_{l}\) <br/> \(\hat{F}_{s}\)</td> <td>\((\hat{F}_{l,0}-\cos{\theta})\textrm{tanh}\Big( \frac{\cos{\theta}-\hat{F}_{s,0}}{\hat{F}_{l,0}-\cos{\theta}}\frac{c_s}{c_l} \Big)\) <br/> \(-(\hat{F}_{l,0}-\cos{\theta}){\frac{c_l}{c_s}}\textrm{tanh}\Big( \frac{\cos{\theta}-\hat{F}_{s,0}}{\hat{F}_{l,0}-\cos{\theta}}\frac{c_s}{c_l} \Big)\)</td> <td> </td> </tr> <tr> <td>effective friction</td> <td>\(\mu_{eff}\)</td> <td>\(\mu_s+\frac{\mu_2-\mu_s}{I_c/I+1}\)</td> <td> </td> </tr> <tr> <td>viscosity</td> <td>\(\eta\)</td> <td>\(\mu_{eff} \frac{P}{\dot\gamma}\)</td> <td> </td> </tr> <tr> <td>drag coefficient</td> <td>\(c_{d,l}\) <br/> \(c_{d,s}\)</td> <td>\([k_1-k_2\exp(-k_3 R)]+s_1 I R +s_2 I (R_\rho-1)\) <br/> \(c_{d,l}/R^2\)</td> <td> </td> </tr> <tr> <td>segregation velocity</td> <td>\(w_l\) <br/> \(w_s\)</td> <td>\(\frac{ \hat F_{l} m_l g_0}{c_{d,l} \pi \eta d_l}\) <br/> \(-\frac{ \hat F_s m_s g_0}{c_{d,s} \pi \eta d_s}\)</td> <td>\(0.26 d_s \ln R \dot\gamma (1-c_i)\)</td> </tr> <tr> <td>diffusion coefficient</td> <td>\(D\)</td> <td>\(0.042 \dot \gamma (c_ld_l+c_sd_s)^2\)</td> <td>\(0.042 \dot \gamma {\bar d}^2\)</td> </tr> </tbody> </table> <p><a class="anchor" id="section2"></a></p> <h2 style="color:purple;font-size: 2em;">2. Physics Informed Neural Network</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="n">device</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
    <span class="k">else</span> <span class="sh">"</span><span class="s">mps</span><span class="sh">"</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
    <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>

        <span class="c1"># 6 layer neural network
</span>        <span class="nf">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ln1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ln2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ln3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ln4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">hidden_layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_layer3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_layer4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_layer5</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mse</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>
        
        <span class="c1"># particle properties in S.I unit
</span>        <span class="n">self</span><span class="p">.</span><span class="n">rd</span><span class="o">=</span><span class="mi">2</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dl</span><span class="o">=</span><span class="mf">0.004</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rho</span><span class="o">=</span><span class="mi">1000</span>
        <span class="n">self</span><span class="p">.</span><span class="n">c_diffusion</span><span class="o">=</span><span class="mf">0.042</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ds</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dl</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rds</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ml</span><span class="o">=</span><span class="mi">4</span><span class="o">/</span><span class="mi">3</span><span class="o">*</span><span class="n">math</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="mf">0.002</span><span class="o">**</span><span class="mi">3</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">rho</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ms</span><span class="o">=</span><span class="mi">4</span><span class="o">/</span><span class="mi">3</span><span class="o">*</span><span class="n">math</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="mf">0.001</span><span class="o">**</span><span class="mi">3</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">rho</span>
        
        
        <span class="c1"># flow configuration (uniform shear)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span><span class="o">=</span><span class="mi">100</span>
        <span class="n">self</span><span class="p">.</span><span class="n">phi</span><span class="o">=</span><span class="mf">0.55</span>
        <span class="n">self</span><span class="p">.</span><span class="n">g</span><span class="o">=</span><span class="mf">9.81</span>
        <span class="n">self</span><span class="p">.</span><span class="n">h0</span><span class="o">=</span><span class="mf">0.01</span>
        <span class="n">self</span><span class="p">.</span><span class="n">p0</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">h0</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">rho</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">g</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">phi</span>

        <span class="c1"># segregation force calculation
</span>        <span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  
        <span class="c1">#  Duan et al. 2024
</span>        <span class="n">_intruder_l</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mf">1.43</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span><span class="o">/</span><span class="mf">0.92</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="mf">3.55</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span><span class="o">/</span><span class="mf">2.94</span><span class="p">))</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">phi</span>
        <span class="n">_intruder_s</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mf">1.43</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">rds</span><span class="o">/</span><span class="mf">0.92</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="mf">3.55</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">rds</span><span class="o">/</span><span class="mf">2.94</span><span class="p">))</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">phi</span>

        <span class="n">self</span><span class="p">.</span><span class="n">intruder_l</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">_intruder_l</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">intruder_s</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">_intruder_s</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> 

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="c1"># if time dimensino is considered, concatenated first, i.e. torch.cat([z,t],axis=1) 
</span>        <span class="n">layer1_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ln1</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">hidden_layer1</span><span class="p">(</span><span class="n">z</span><span class="p">)))</span>
        <span class="n">layer2_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ln2</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">hidden_layer2</span><span class="p">(</span><span class="n">layer1_out</span><span class="p">)))</span>
        <span class="n">layer3_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ln3</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">hidden_layer3</span><span class="p">(</span><span class="n">layer2_out</span><span class="p">)))</span>
        <span class="n">layer4_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ln4</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">hidden_layer4</span><span class="p">(</span><span class="n">layer3_out</span><span class="p">)))</span>
        <span class="n">layer5_out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">hidden_layer5</span><span class="p">(</span><span class="n">layer4_out</span><span class="p">))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">output_layer</span><span class="p">(</span><span class="n">layer5_out</span><span class="p">)</span> <span class="c1">## For regression, no activation is used in output layer
</span>        <span class="k">return</span> <span class="n">output</span>
    
    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>


        <span class="c1"># PDE loss    
</span>        <span class="n">z_collocation</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10001</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

        <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">z_collocation</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> 

        <span class="n">c</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> 
        
        <span class="n">p</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">rho</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">phi</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">g</span><span class="o">*</span><span class="n">z</span><span class="o">+</span><span class="n">self</span><span class="p">.</span><span class="n">p0</span>
        <span class="n">inert</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">gamma</span><span class="o">*</span><span class="p">(</span><span class="n">c</span><span class="o">*</span><span class="mf">0.004</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mf">0.004</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span><span class="p">))</span><span class="o">/</span><span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">p</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">rho</span><span class="p">);</span>
        <span class="n">mu_eff</span><span class="o">=</span><span class="mf">0.364</span><span class="o">+</span><span class="p">(</span><span class="mf">0.772</span><span class="o">-</span><span class="mf">0.364</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mf">0.434</span><span class="o">/</span><span class="n">inert</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">eta</span><span class="o">=</span><span class="n">mu_eff</span><span class="o">*</span><span class="n">p</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">gamma</span>

        <span class="n">mixture_l</span><span class="o">=</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">intruder_l</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="p">)</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">intruder_s</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">intruder_l</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="p">)</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">ml</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">ms</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">)</span><span class="o">/</span><span class="n">c</span><span class="p">)</span>
        <span class="n">mixture_s</span><span class="o">=-</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">intruder_l</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="p">)</span><span class="o">*</span><span class="n">c</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">)</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">ms</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">ml</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">intruder_s</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">intruder_l</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">theta</span><span class="p">)</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">ml</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">ms</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">)</span><span class="o">/</span><span class="n">c</span><span class="p">)</span>
        

        <span class="n">cd</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="o">-</span><span class="mi">7</span><span class="o">*</span><span class="n">math</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">2.6</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span><span class="p">))</span><span class="o">+</span><span class="mf">0.57</span><span class="o">*</span><span class="n">inert</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">rd</span>

        <span class="n">wseg</span><span class="o">=</span><span class="n">mixture_l</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">ml</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">g</span> <span class="o">/</span> <span class="p">(</span><span class="n">cd</span><span class="o">*</span><span class="n">math</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">eta</span><span class="o">*</span><span class="mf">0.004</span><span class="p">)</span>

        <span class="n">c_z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="nf">sum</span><span class="p">(),</span> <span class="n">z</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Duan et al. 2024  
</span>        <span class="n">pde</span><span class="o">=</span><span class="p">(</span><span class="n">wseg</span><span class="o">*</span><span class="n">c</span><span class="o">-</span><span class="mf">0.042</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">gamma</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="nf">square</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">)</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">ds</span><span class="o">+</span><span class="n">c</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">dl</span><span class="p">)</span><span class="o">*</span><span class="n">c_z</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span>

        <span class="c1"># Schlick et al. 2015
</span>        <span class="c1"># simplified with constant diffusion coefficient
</span>        <span class="c1"># pde = (1/0.1 *(1-c)*c - c_z )*10
</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">pde</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">pde_loss</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">mse</span><span class="p">(</span><span class="n">pde</span><span class="p">,</span><span class="n">target</span><span class="p">)</span>


        <span class="c1"># Mass conservation loss
</span>        <span class="n">x_bc</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">10001</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">x_bc</span> <span class="o">=</span> <span class="n">x_bc</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">u_bc</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">x_bc</span><span class="p">)</span>
        <span class="n">u_bc</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">u_bc</span><span class="p">)</span>
        
        <span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">u_bc</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">+</span><span class="mf">0.5</span>
        <span class="n">mass_loss</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">mse</span><span class="p">(</span><span class="n">u_bc</span><span class="p">,</span><span class="n">target</span><span class="p">)</span>
   
        <span class="k">return</span> <span class="n">mass_loss</span>  <span class="o">+</span> <span class="n">pde_loss</span>
</code></pre></div></div> <p><a class="anchor" id="section3"></a></p> <h2 style="color:purple;font-size: 2em;">3. Train </h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">net</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">mse_cost_function</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span> <span class="c1"># Mean squared error
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.002</span><span class="p">)</span>


<span class="n">iterations</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">previous_validation_loss</span> <span class="o">=</span> <span class="mf">99999999.0</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span> <span class="c1"># to make the gradients zero
</span>    <span class="n">loss</span><span class="o">=</span><span class="n">net</span><span class="p">.</span><span class="nf">loss</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span> <span class="c1"># This is for computing gradients using backward propagation
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span> <span class="c1"># This is equivalent to : theta_new = theta_old - alpha * derivative of J w.r.t theta
</span>    <span class="k">if</span> <span class="n">loss</span><span class="p">.</span><span class="n">data</span><span class="o">&lt;</span><span class="mf">1e-6</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    	<span class="nf">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span><span class="sh">"</span><span class="s">Traning Loss:</span><span class="sh">"</span><span class="p">,</span><span class="n">loss</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
</code></pre></div></div> <p><a class="anchor" id="section4"></a></p> <h2 style="color:purple;font-size: 2em;">4. Results </h2> <p>The full model prediction is compared to the previous model by Schlick et al. 2015 with different values of \(\lambda\).</p> \[\frac{d_c}{d_z}=\frac{1}{\lambda}c_l(1-c_l)\] <h2 style="color:purple;font-size: 1em;">4.1 Uniform shear </h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_img/comparison001-480.webp 480w,/assets/img/blog_img/comparison001-800.webp 800w,/assets/img/blog_img/comparison001-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_img/comparison001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_img/comparison01-480.webp 480w,/assets/img/blog_img/comparison01-800.webp 800w,/assets/img/blog_img/comparison01-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_img/comparison01.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 style="color:purple;font-size: 1em;">4.2 Exponential shear </h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_img/comparison001exp-480.webp 480w,/assets/img/blog_img/comparison001exp-800.webp 800w,/assets/img/blog_img/comparison001exp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_img/comparison001exp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_img/comparison01exp-480.webp 480w,/assets/img/blog_img/comparison01exp-800.webp 800w,/assets/img/blog_img/comparison01exp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_img/comparison01exp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a class="anchor" id="section5"></a></p> <h2 style="color:purple;font-size: 2em;">5. Improvements </h2> <ul> <li>Add loss function for \(c\) out of range 0 to 1</li> <li>Add a learning rate scheduler to reduce learning rate at loss plateau (lr reduced from 1e-3 to 1e-5)</li> </ul> <p>After these changes, the overall loss is reduced to 1e-8</p> <h2 style="color:purple;font-size: 1em;">5.1 Exponential shear </h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_img/comparison001exp_a-480.webp 480w,/assets/img/blog_img/comparison001exp_a-800.webp 800w,/assets/img/blog_img/comparison001exp_a-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_img/comparison001exp_a.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_img/comparison01exp_a-480.webp 480w,/assets/img/blog_img/comparison01exp_a-800.webp 800w,/assets/img/blog_img/comparison01exp_a-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_img/comparison01exp_a.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="machine_learning"/><summary type="html"><![CDATA[PINN for solving segregation problem]]></summary></entry><entry><title type="html">Transformer using PyTorch</title><link href="https://19revey.github.io/blog/2023/transformer/" rel="alternate" type="text/html" title="Transformer using PyTorch"/><published>2023-07-11T12:57:00+00:00</published><updated>2023-07-11T12:57:00+00:00</updated><id>https://19revey.github.io/blog/2023/transformer</id><content type="html" xml:base="https://19revey.github.io/blog/2023/transformer/"><![CDATA[ <h2 style="color:purple;font-size: 2em;">Overview</h2> <p>Basic transformer with an encoder-decoder architecture for language translation models.</p> <ul> <li>Understanding transformers <ul> <li><a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">attention is all you need</a></li> </ul> </li> <li>Pytorch implementation <ul> <li><a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html">https://pytorch.org/tutorials/beginner/transformer_tutorial.html</a></li> <li><a href="https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch">https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch</a></li> </ul> </li> </ul> <p><a class="anchor" id="section1"></a></p> <h2 style="color:purple;font-size: 2em;">1. Introduction</h2> <p><img src="https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png" width="330" height="470" img="" style="float: right;"/></p> <ul> <li><a href="#section1">1. Introduction</a></li> <li><a href="#section2">2. Import libraries</a></li> <li><a href="#section3">3. Basic components</a> <ul> <li><a href="#section4">Word Embeddings</a></li> <li><a href="#section5">Positional Encoding</a></li> <li><a href="#section6">Self Attention</a></li> <li><a href="#section7">Transformer Block</a></li> </ul> </li> <li><a href="#section8">4. Encoder</a></li> <li><a href="#section9">5. Decoder</a></li> <li><a href="#section10">6. Transformer</a></li> </ul> <p><a class="anchor" id="section2"></a></p> <h2 style="color:purple;font-size: 2em;">2. Import Libraries</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">__version__</span><span class="p">)</span>

<span class="n">device</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
    <span class="k">else</span> <span class="sh">"</span><span class="s">mps</span><span class="sh">"</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
    <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="p">)</span>
</code></pre></div></div> <p><a class="anchor" id="section3"></a></p> <h2 style="color:purple;font-size: 2em;">3. Basic components</h2> <p><a class="anchor" id="section4"></a></p> <h2 style="color:purple;font-size: 1.5em;">Word Embeddings</h2> <p>Each word will be mapped to corresponding \(d_{model}=512\) embedding vector. Suppose we have batch_size of 32 and sequence_length of 10 (10 words). The the output will be Batch_size X sequence_length X embedding_dim (32X10X512).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Embedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            vocab_size: size of vocabulary
            embed_dim: dimension of embeddings, i.e. d_model
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Embedding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            x: input vector, i.e. (batch, seq_len, vocab_size)
        Returns:
            out: embedding vector (batch, seq_len, embed_dim)
        </span><span class="sh">"""</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div> <p><a class="anchor" id="section5"></a></p> <h2 style="color:purple;font-size: 1.5em;"> Positional Encoding</h2> \[PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\] \[PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})\] <p>Here \(pos\) is the position of the word in the sentence, and \(i\) refers to position along embedding vector dimension.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PositionalEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">max_seq_len</span><span class="p">,</span><span class="n">embed_model_dim</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            seq_len: length of input sequence
            embed_model_dim: demension of embedding, d_model
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">PositionalEmbedding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_model_dim</span>

        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">,</span><span class="mi">2</span><span class="p">):</span>
                <span class="n">pe</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10000</span> <span class="o">**</span> <span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">)</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)))</span>
                <span class="n">pe</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10000</span> <span class="o">**</span> <span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)))</span>

        <span class="c1"># adding batch dimension for broadcasting
</span>        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> 

        <span class="c1"># register buffer in Pytorch -&gt;
</span>        <span class="c1"># If you have parameters in your model, which should be saved and restored in the state_dict,
</span>        <span class="c1"># but not trained by the optimizer, you should register them as buffers.
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">pe</span><span class="sh">'</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            x: input vector
        Returns:
            x: output
        </span><span class="sh">"""</span>
        <span class="c1"># make embeddings relatively larger
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="c1">#add constant to embedding
</span>        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">].</span><span class="nf">detach</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p><a class="anchor" id="section6"></a></p> <h2 style="color:purple; font-size: 1.5em;"> Self Attention</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            embed_dim: dimension of embeding vector output
            n_heads: number of self attention heads
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>    <span class="c1">#512 dim
</span>        <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>   <span class="c1">#8
</span>        <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">)</span>   <span class="c1">#512/8 = 64  . each key,query, value will be of 64d
</span>       
        <span class="c1">#key,query and value matrixes    #64 x 64   
</span>        <span class="n">self</span><span class="p">.</span><span class="n">query_matrix</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># single key matrix for all 8 keys #512x512
</span>        <span class="n">self</span><span class="p">.</span><span class="n">key_matrix</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span>  <span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value_matrix</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span> <span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span> 

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">key</span><span class="p">,</span><span class="n">query</span><span class="p">,</span><span class="n">value</span><span class="p">,</span><span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>    <span class="c1">#batch_size x sequence_length x embedding_dim    # 32 x 10 x 512
</span>        
        <span class="sh">"""</span><span class="s">
        Args:
           key : key vector
           query : query vector
           value : value vector
           mask: mask for decoder
        
        Returns:
           output vector from multihead attention
        </span><span class="sh">"""</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">seq_length</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># query dimension can change in decoder during inference. 
</span>        <span class="c1"># so we cant take general seq_length
</span>        <span class="n">seq_length_query</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># 32x10x512
</span>        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="p">)</span>  <span class="c1">#batch_size x sequence_length x n_heads x single_head_dim = (32x10x8x64)
</span>        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length_query</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="p">)</span> <span class="c1">#(32x10x8x64)
</span>        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="p">)</span> <span class="c1">#(32x10x8x64)
</span>       
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key_matrix</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>       <span class="c1"># (32x10x8x64)
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query_matrix</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>   
        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value_matrix</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, n_heads, seq_len, single_head_dim)    # (32 x 8 x 10 x 64)
</span>        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, n_heads, seq_len, single_head_dim)
</span>        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, n_heads, seq_len, single_head_dim)
</span>       
        <span class="c1"># computes attention
</span>        <span class="c1"># adjust key for matrix multiplication
</span>        <span class="n">k_adjusted</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1">#(batch_size, n_heads, single_head_dim, seq_ken)  #(32 x 8 x 64 x 10)
</span>        <span class="n">product</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k_adjusted</span><span class="p">)</span>  <span class="c1">#(32 x 8 x 10 x 64) x (32 x 8 x 64 x 10) = #(32x8x10x10)
</span>        
        <span class="c1"># fill those positions of product matrix as (-1e20) where mask positions are 0
</span>        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
             <span class="n">product</span> <span class="o">=</span> <span class="n">product</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">"</span><span class="s">-1e20</span><span class="sh">"</span><span class="p">))</span>

        <span class="c1">#divising by square root of key dimension
</span>        <span class="n">product</span> <span class="o">=</span> <span class="n">product</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="p">)</span> <span class="c1"># / sqrt(64)
</span>
        <span class="c1">#applying softmax
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">product</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
 
        <span class="c1">#mutiply with value matrix
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>  <span class="c1">##(32x8x 10x 10) x (32 x 8 x 10 x 64) = (32 x 8 x 10 x 64) 
</span>        
        <span class="c1">#concatenated output
</span>        <span class="n">concat</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length_query</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">single_head_dim</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">)</span>  <span class="c1"># (32x8x10x64) -&gt; (32x10x8x64)  -&gt; (32,10,512)
</span>        
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">out</span><span class="p">(</span><span class="n">concat</span><span class="p">)</span> <span class="c1">#(32,10,512) -&gt; (32,10,512)
</span>       
        <span class="k">return</span> <span class="n">output</span>

</code></pre></div></div> <p><a class="anchor" id="section7"></a></p> <h2 style="color:purple; font-size: 1.5em;"> Transformer Block</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="sh">"""</span><span class="s">
        Args:
           embed_dim: dimension of the embedding
           expansion_factor: fator ehich determines output dimension of linear layer
           n_heads: number of attention heads
        
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                          <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">*</span><span class="n">embed_dim</span><span class="p">),</span>
                          <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
                          <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">expansion_factor</span><span class="o">*</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">key</span><span class="p">,</span><span class="n">query</span><span class="p">,</span><span class="n">value</span><span class="p">):</span>
        
        <span class="sh">"""</span><span class="s">
        Args:
           key: key vector
           query: query vector
           value: value vector
           norm2_out: output of transformer block
        
        </span><span class="sh">"""</span>
        
        <span class="n">attention_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">key</span><span class="p">,</span><span class="n">query</span><span class="p">,</span><span class="n">value</span><span class="p">)</span>  <span class="c1">#32x10x512
</span>        <span class="n">attention_residual_out</span> <span class="o">=</span> <span class="n">attention_out</span> <span class="o">+</span> <span class="n">value</span>  <span class="c1">#32x10x512
</span>        <span class="n">norm1_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout1</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">norm1</span><span class="p">(</span><span class="n">attention_residual_out</span><span class="p">))</span> <span class="c1">#32x10x512
</span>
        <span class="n">feed_fwd_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feed_forward</span><span class="p">(</span><span class="n">norm1_out</span><span class="p">)</span> <span class="c1">#32x10x512 -&gt; #32x10x2048 -&gt; 32x10x512
</span>        <span class="n">feed_fwd_residual_out</span> <span class="o">=</span> <span class="n">feed_fwd_out</span> <span class="o">+</span> <span class="n">norm1_out</span> <span class="c1">#32x10x512
</span>        <span class="n">norm2_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout2</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">norm2</span><span class="p">(</span><span class="n">feed_fwd_residual_out</span><span class="p">))</span> <span class="c1">#32x10x512
</span>
        <span class="k">return</span> <span class="n">norm2_out</span>

</code></pre></div></div> <p><a class="anchor" id="section8"></a></p> <h2 style="color:purple;font-size: 2em;"> 4. Encoder</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Args:
        seq_len : length of input sequence
        embed_dim: dimension of embedding
        num_layers: number of encoder layers
        expansion_factor: factor which determines number of linear layers in feed forward layer
        n_heads: number of heads in multihead attention
        
    Returns:
        out: output of the encoder
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerEncoder</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">embedding_layer</span> <span class="o">=</span> <span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">positional_encoder</span> <span class="o">=</span> <span class="nc">PositionalEmbedding</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)])</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">embed_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">positional_encoder</span><span class="p">(</span><span class="n">embed_out</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">out</span><span class="p">,</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>  <span class="c1">#32x10x512
</span></code></pre></div></div> <p><a class="anchor" id="section9"></a></p> <h2 style="color:purple;font-size: 2em;"> 5. Decoder</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DecoderBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">DecoderBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="sh">"""</span><span class="s">
        Args:
           embed_dim: dimension of the embedding
           expansion_factor: fator ehich determines output dimension of linear layer
           n_heads: number of attention heads
        
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">transformer_block</span> <span class="o">=</span> <span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>       
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span><span class="n">mask</span><span class="p">):</span>
        
        <span class="sh">"""</span><span class="s">
        Args:
           key: key vector
           query: query vector
           value: value vector
           mask: mask to be given for multi head attention 
        Returns:
           out: output of transformer block
    
        </span><span class="sh">"""</span>
        <span class="c1">#we need to pass mask mask only to fst attention
</span>        <span class="n">attention</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span> <span class="c1">#32x10x512
</span>        <span class="n">value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">attention</span> <span class="o">+</span> <span class="n">x</span><span class="p">))</span>
        
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transformer_block</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">out</span>


<span class="k">class</span> <span class="nc">TransformerDecoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerDecoder</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="sh">"""</span><span class="s">  
        Args:
           target_vocab_size: vocabulary size of taget
           embed_dim: dimension of embedding
           seq_len : length of input sequence
           num_layers: number of encoder layers
           expansion_factor: factor which determines number of linear layers in feed forward layer
           n_heads: number of heads in multihead attention
        
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">word_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">position_embedding</span> <span class="o">=</span> <span class="nc">PositionalEmbedding</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="nc">DecoderBlock</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span> 
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
            <span class="p">]</span>

        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_out</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        
        <span class="sh">"""</span><span class="s">
        Args:
            x: input vector from target
            enc_out : output from encoder layer
            trg_mask: mask for decoder self attention
        Returns:
            out: output vector
        </span><span class="sh">"""</span>     
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">word_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1">#32x10x512
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">position_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#32x10x512
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
     
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">enc_out</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_out</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> 

        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc_out</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">out</span>

</code></pre></div></div> <p><a class="anchor" id="section10"></a></p> <h2 style="color:purple;font-size: 2em;"> 6. Transformer</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span><span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="sh">"""</span><span class="s">  
        Args:
           embed_dim:  dimension of embedding 
           src_vocab_size: vocabulary size of source
           target_vocab_size: vocabulary size of target
           seq_length : length of input sequence
           num_layers: number of encoder layers
           expansion_factor: factor which determines number of linear layers in feed forward layer
           n_heads: number of heads in multihead attention
        
        </span><span class="sh">"""</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">target_vocab_size</span> <span class="o">=</span> <span class="n">target_vocab_size</span>

        <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="n">expansion_factor</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="n">n_heads</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="nc">TransformerDecoder</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="o">=</span><span class="n">expansion_factor</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="n">n_heads</span><span class="p">)</span>
        
    
    <span class="k">def</span> <span class="nf">make_trg_mask</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">trg</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            trg: target sequence
        Returns:
            trg_mask: target mask
        </span><span class="sh">"""</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">trg_len</span> <span class="o">=</span> <span class="n">trg</span><span class="p">.</span><span class="n">shape</span>
        <span class="c1"># returns the lower triangular part of matrix filled with ones
</span>        <span class="n">trg_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">trg_len</span><span class="p">,</span> <span class="n">trg_len</span><span class="p">))).</span><span class="nf">expand</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">trg_len</span><span class="p">,</span> <span class="n">trg_len</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">trg_mask</span>    

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">src</span><span class="p">,</span><span class="n">trg</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        for inference
        Args:
            src: input to encoder 
            trg: input to decoder
        out:
            out_labels : returns final prediction of sequence
        </span><span class="sh">"""</span>
        <span class="n">trg_mask</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">make_trg_mask</span><span class="p">(</span><span class="n">trg</span><span class="p">)</span>
        <span class="n">enc_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
        <span class="n">out_labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">batch_size</span><span class="p">,</span><span class="n">seq_len</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">src</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1">#outputs = torch.zeros(seq_len, batch_size, self.target_vocab_size)
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">trg</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span> <span class="c1">#10
</span>            <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">enc_out</span><span class="p">,</span><span class="n">trg_mask</span><span class="p">)</span> <span class="c1">#bs x seq_len x vocab_dim
</span>            <span class="c1"># taking the last token
</span>            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span>
     
            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">out_labels</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
          
        
        <span class="k">return</span> <span class="n">out_labels</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">trg</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            src: input to encoder 
            trg: input to decoder
        out:
            out: final vector which returns probabilities of each target word
        </span><span class="sh">"""</span>
        <span class="n">trg_mask</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">make_trg_mask</span><span class="p">(</span><span class="n">trg</span><span class="p">)</span>
        <span class="n">enc_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
   
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(</span><span class="n">trg</span><span class="p">,</span> <span class="n">enc_out</span><span class="p">,</span> <span class="n">trg_mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outputs</span>


</code></pre></div></div>]]></content><author><name></name></author><category term="machine_learning"/><summary type="html"><![CDATA[based on the paper "Attention is all you need"]]></summary></entry><entry><title type="html">U-Net using PyTorch</title><link href="https://19revey.github.io/blog/2023/unet/" rel="alternate" type="text/html" title="U-Net using PyTorch"/><published>2023-05-15T12:57:00+00:00</published><updated>2023-05-15T12:57:00+00:00</updated><id>https://19revey.github.io/blog/2023/unet</id><content type="html" xml:base="https://19revey.github.io/blog/2023/unet/"><![CDATA[ <p><a class="anchor" id="section1"></a></p> <h2 style="color:purple;font-size: 2em;">1. Introduction</h2> <p>The U-Net architecture is characterized by its unique symmetric U-shaped design, which consists of a contracting path (encoder) and an expansive path (decoder), connected by skip connections. This architecture enables the network to capture both local and global features while preserving spatial information, making it highly effective for pixel-level segmentation tasks.</p> <ul> <li><a href="https://arxiv.org/pdf/1505.04597">Reference: U-Net: Convolutional Networks for Biomedical Image Segmentation</a> <ul> <li>Weighted loss for seperation of touching objects of the same class</li> <li>Overalp-tile strategy for arbitrary large images</li> </ul> </li> </ul> <p><img src="https://miro.medium.com/v2/resize:fit:1400/1*f7YOaE4TWubwaFF7Z1fzNw.png" width="700" height="457" img="" style="float: top;"/></p> <p><a class="anchor" id="section3"></a></p> <h2 style="color:purple;font-size: 2em;">2. Components</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DoubleConv</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">(convolution =&gt; [BN] =&gt; ReLU) * 2</span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">mid_channels</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">mid_channels</span><span class="p">:</span>
            <span class="n">mid_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="n">self</span><span class="p">.</span><span class="n">double_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">mid_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">mid_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">mid_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">double_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Down</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Downscaling with maxpool then double conv</span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">maxpool_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="nc">DoubleConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">maxpool_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Up</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Upscaling then double conv</span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">bilinear</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1"># if bilinear, use the normal convolutions to reduce the number of channels
</span>        <span class="k">if</span> <span class="n">bilinear</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">up</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">bilinear</span><span class="sh">'</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="nc">DoubleConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">up</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ConvTranspose2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="nc">DoubleConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="c1"># input is CHW
</span>        <span class="n">diffY</span> <span class="o">=</span> <span class="n">x2</span><span class="p">.</span><span class="nf">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">x1</span><span class="p">.</span><span class="nf">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">diffX</span> <span class="o">=</span> <span class="n">x2</span><span class="p">.</span><span class="nf">size</span><span class="p">()[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">x1</span><span class="p">.</span><span class="nf">size</span><span class="p">()[</span><span class="mi">3</span><span class="p">]</span>

        <span class="n">x1</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="p">[</span><span class="n">diffX</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">diffX</span> <span class="o">-</span> <span class="n">diffX</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span>
                        <span class="n">diffY</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">diffY</span> <span class="o">-</span> <span class="n">diffY</span> <span class="o">//</span> <span class="mi">2</span><span class="p">])</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">x2</span><span class="p">,</span> <span class="n">x1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">OutConv</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">OutConv</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

</code></pre></div></div> <p><a class="anchor" id="section8"></a></p> <h2 style="color:purple;font-size: 2em;"> 3. UNet</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">UNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_channels</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">bilinear</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">UNet</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_channels</span> <span class="o">=</span> <span class="n">n_channels</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_classes</span> <span class="o">=</span> <span class="n">n_classes</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bilinear</span> <span class="o">=</span> <span class="n">bilinear</span>

        <span class="n">self</span><span class="p">.</span><span class="n">inc</span> <span class="o">=</span> <span class="p">(</span><span class="nc">DoubleConv</span><span class="p">(</span><span class="n">n_channels</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down1</span> <span class="o">=</span> <span class="p">(</span><span class="nc">Down</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down2</span> <span class="o">=</span> <span class="p">(</span><span class="nc">Down</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down3</span> <span class="o">=</span> <span class="p">(</span><span class="nc">Down</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">))</span>
        <span class="n">factor</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">bilinear</span> <span class="k">else</span> <span class="mi">1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down4</span> <span class="o">=</span> <span class="p">(</span><span class="nc">Down</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span> <span class="o">//</span> <span class="n">factor</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up1</span> <span class="o">=</span> <span class="p">(</span><span class="nc">Up</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">512</span> <span class="o">//</span> <span class="n">factor</span><span class="p">,</span> <span class="n">bilinear</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up2</span> <span class="o">=</span> <span class="p">(</span><span class="nc">Up</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span> <span class="o">//</span> <span class="n">factor</span><span class="p">,</span> <span class="n">bilinear</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up3</span> <span class="o">=</span> <span class="p">(</span><span class="nc">Up</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span> <span class="o">//</span> <span class="n">factor</span><span class="p">,</span> <span class="n">bilinear</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up4</span> <span class="o">=</span> <span class="p">(</span><span class="nc">Up</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">bilinear</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">outc</span> <span class="o">=</span> <span class="p">(</span><span class="nc">OutConv</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">inc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down1</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down2</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
        <span class="n">x4</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down3</span><span class="p">(</span><span class="n">x3</span><span class="p">)</span>
        <span class="n">x5</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down4</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up1</span><span class="p">(</span><span class="n">x5</span><span class="p">,</span> <span class="n">x4</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x3</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up4</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">outc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>

    <span class="k">def</span> <span class="nf">use_checkpointing</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">inc</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">inc</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down3</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down4</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down4</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up3</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up4</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up4</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">outc</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">outc</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="machine_learning"/><summary type="html"><![CDATA[based on the paper "U-Net Convolutional Networks for Biomedical Image Segmentation"]]></summary></entry><entry><title type="html">Improve resume using LLM</title><link href="https://19revey.github.io/blog/2023/resume/" rel="alternate" type="text/html" title="Improve resume using LLM"/><published>2023-01-02T17:39:00+00:00</published><updated>2023-01-02T17:39:00+00:00</updated><id>https://19revey.github.io/blog/2023/resume</id><content type="html" xml:base="https://19revey.github.io/blog/2023/resume/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><category term="machine_learning"/><summary type="html"><![CDATA[improve resume based on job description using Google Gemini Pro]]></summary></entry><entry><title type="html">Compare classification models</title><link href="https://19revey.github.io/blog/2022/ml_classification/" rel="alternate" type="text/html" title="Compare classification models"/><published>2022-05-01T12:57:00+00:00</published><updated>2022-05-01T12:57:00+00:00</updated><id>https://19revey.github.io/blog/2022/ml_classification</id><content type="html" xml:base="https://19revey.github.io/blog/2022/ml_classification/"><![CDATA[<h3 id="compare-different-classification-models-using-the-titanic-disaster-data">Compare different classification models using the titanic disaster data</h3> <p><br/></p> <table> <thead> <tr> <th>Model</th> <th>F1 score</th> </tr> </thead> <tbody> <tr> <td>Random Forest</td> <td>0.7917</td> </tr> <tr> <td>Logistic Regression</td> <td>0.7586</td> </tr> <tr> <td>Neural Network</td> <td>0.7536</td> </tr> <tr> <td>K-Neighbors</td> <td>0.7500</td> </tr> <tr> <td>Naive Bayes</td> <td>0.7436</td> </tr> <tr> <td>XGBClassifier</td> <td>0.7347</td> </tr> <tr> <td>Decision Tree</td> <td>0.7020</td> </tr> </tbody> </table> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/ml_compare.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="machine_learning"/><summary type="html"><![CDATA[based on the titanic disaster data]]></summary></entry><entry><title type="html">Batch and layer normalization</title><link href="https://19revey.github.io/blog/2022/ml_components/" rel="alternate" type="text/html" title="Batch and layer normalization"/><published>2022-03-04T22:20:00+00:00</published><updated>2022-03-04T22:20:00+00:00</updated><id>https://19revey.github.io/blog/2022/ml_components</id><content type="html" xml:base="https://19revey.github.io/blog/2022/ml_components/"><![CDATA[<h2 style="color:purple;font-size: 2em;">Overview</h2> <ul> <li><a href="#section1">Batch normalization</a></li> <li><a href="#section2">Layer normalization</a></li> </ul> <h5 id="understand-normalization-in-a-neural-network">Understand normalization in a neural network</h5> <ul> <li><a href="https://arxiv.org/pdf/1502.03167.pdf">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li> <li><a href="https://arxiv.org/pdf/1607.06450.pdf">Layer Normalization</a></li> <li><a href="https://www.pinecone.io/learn/batch-layer-normalization/">https://www.pinecone.io/learn/batch-layer-normalization/</a></li> </ul> <p><a class="anchor" id="section1"></a></p> <h2 style="color:purple;font-size: 2em;">Batch and layer normalization</h2> <table> <thead> <tr> <th> </th> <th>batch normalization</th> <th>layer normalization</th> </tr> </thead> <tbody> <tr> <td>trainable parameter</td> <td>2*num_feature</td> <td>2</td> </tr> <tr> <td>normalization across</td> <td>num_batch</td> <td>num_feature</td> </tr> </tbody> </table> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/normalization.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="machine_learning"/><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">Docker</title><link href="https://19revey.github.io/blog/2022/docker/" rel="alternate" type="text/html" title="Docker"/><published>2022-03-01T08:13:00+00:00</published><updated>2022-03-01T08:13:00+00:00</updated><id>https://19revey.github.io/blog/2022/docker</id><content type="html" xml:base="https://19revey.github.io/blog/2022/docker/"><![CDATA[<p><br/></p> <h3 id="install-docker">Install Docker</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get update <span class="nt">-y</span>
<span class="nb">sudo </span>apt-get upgrade <span class="nt">-y</span>

curl <span class="nt">-fsSL</span> https://get.docker.com <span class="nt">-o</span> get-docker.sh

<span class="nb">sudo </span>sh get-docker.sh <span class="nt">-y</span>

<span class="nb">sudo </span>usermod <span class="nt">-aG</span> docker ubuntu 

newgrp docker
</code></pre></div></div> <p><br/></p> <h3 id="nvidia-container-toolkit"><a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html">Nvidia Container Toolkit</a></h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-fsSL</span> https://nvidia.github.io/libnvidia-container/gpgkey | <span class="nb">sudo </span>gpg <span class="nt">--dearmor</span> <span class="nt">-o</span> /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg <span class="se">\</span>
  <span class="o">&amp;&amp;</span> curl <span class="nt">-s</span> <span class="nt">-L</span> https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | <span class="se">\</span>
    <span class="nb">sed</span> <span class="s1">'s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g'</span> | <span class="se">\</span>
    <span class="nb">sudo tee</span> /etc/apt/sources.list.d/nvidia-container-toolkit.list

<span class="nb">sudo </span>apt-get update
<span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> nvidia-container-toolkit

<span class="nb">sudo </span>nvidia-ctk runtime configure <span class="nt">--runtime</span><span class="o">=</span>docker

<span class="nb">sudo </span>systemctl restart docker
</code></pre></div></div> <p><br/></p> <h3 id="dockerfile-example">Dockerfile example</h3> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Use a base image</span>
<span class="c">#FROM python:3.11-slim</span>
<span class="k">FROM</span><span class="s"> nvidia/cuda:12.1.0-base-ubuntu22.04</span>
<span class="c"># Set environment variables</span>
<span class="k">ENV</span><span class="s"> MY_VARIABLE=my_value</span>
<span class="c"># Install dependencies</span>
<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="se">\
</span>    build-essential <span class="se">\
</span>    software-properties-common <span class="se">\
</span>    git <span class="se">\
</span>    python3-pip <span class="se">\
</span>    <span class="o">&amp;&amp;</span> <span class="nb">rm</span> <span class="nt">-rf</span> /var/lib/apt/lists/<span class="k">*</span>
<span class="c"># Set the working directory</span>
<span class="k">WORKDIR</span><span class="s"> /app</span>
<span class="c"># Copy files into the image</span>
<span class="k">COPY</span><span class="s"> . /app</span>
<span class="c"># Expose a port</span>
<span class="k">EXPOSE</span><span class="s"> 8051</span>
<span class="c"># Install Python dependencies based on the requirements.txt file</span>
<span class="k">RUN </span>pip3 <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
<span class="c"># Configure a container that will run as an executable</span>
<span class="k">ENTRYPOINT</span><span class="s"> ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]</span>
<span class="c"># Define a command to run on container startup</span>
<span class="k">CMD</span><span class="s"> ["./start.sh"]</span>
<span class="c"># Keep container running</span>
<span class="k">CMD</span><span class="s"> ["tail -f /dev/null"]</span>
</code></pre></div></div> <table> <tbody> <tr> <td>Build image</td> <td><code class="language-plaintext highlighter-rouge">docker build -t image_name .</code></td> </tr> <tr> <td>Run container</td> <td><code class="language-plaintext highlighter-rouge">docker run -b -p 8501:8501 -e API="XXX" --name image_container image_name</code></td> </tr> <tr> <td>Remove all container.</td> <td><code class="language-plaintext highlighter-rouge">docker rm $(docker ps -a -q)</code></td> </tr> <tr> <td>Remove all iamges.</td> <td><code class="language-plaintext highlighter-rouge">docker image rm $(docker images -q)</code></td> </tr> </tbody> </table> <p><br/></p> <h3 id="the-process-of-building-and-running-docker-containers-can-be-automated-using-tools-like-docker-compose">The process of building and running Docker containers can be automated using tools like Docker Compose.</h3> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code>services:
  web:  #service name
    build: .
    shm_size: 8gb <span class="c"># increase shared memory</span>
    ports:
      - "8501:8501"
    volumes:
      - .:/app
    environment:
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - NVIDIA_VISIBLE_DEVICES=all
    command: tail -f /dev/null  <span class="c">#keep container running in the background</span>
</code></pre></div></div> <table> <tbody> <tr> <td>Build image and run container</td> <td><code class="language-plaintext highlighter-rouge">docker compose up --detach</code></td> </tr> <tr> <td>remove container</td> <td><code class="language-plaintext highlighter-rouge">docker compose down</code></td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="computer_science"/><summary type="html"><![CDATA[some docker commands]]></summary></entry><entry><title type="html">Git &amp;amp; Github</title><link href="https://19revey.github.io/blog/2022/git/" rel="alternate" type="text/html" title="Git &amp;amp; Github"/><published>2022-01-21T22:13:00+00:00</published><updated>2022-01-21T22:13:00+00:00</updated><id>https://19revey.github.io/blog/2022/git</id><content type="html" xml:base="https://19revey.github.io/blog/2022/git/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_git/gitworkflow-480.webp 480w,/assets/img/blog_git/gitworkflow-800.webp 800w,/assets/img/blog_git/gitworkflow-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_git/gitworkflow.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="tell-git-who-you-are">Tell Git who you are</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>Configure the author name.</td> <td><code class="language-plaintext highlighter-rouge">git config --global user.name "&lt;username&gt;"</code></td> </tr> <tr> <td>Configure the author email address.</td> <td><code class="language-plaintext highlighter-rouge">git config --global user.email &lt;email address&gt;</code></td> </tr> </tbody> </table> <h3 id="getting--creating-projects">Getting &amp; Creating Projects</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>Initialize a local Git repository</td> <td><code class="language-plaintext highlighter-rouge">git init</code></td> </tr> <tr> <td>Create a local copy of a remote repository</td> <td><code class="language-plaintext highlighter-rouge">git clone ssh://git@github.com/&lt;username&gt;/&lt;repository-name&gt;.git</code></td> </tr> </tbody> </table> <h3 id="basic-snapshotting">Basic Snapshotting</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>Check status</td> <td><code class="language-plaintext highlighter-rouge">git status</code></td> </tr> <tr> <td>Add a file to the staging area</td> <td><code class="language-plaintext highlighter-rouge">git add &lt;file-name.txt&gt;</code></td> </tr> <tr> <td>Add all new and changed files to the staging area</td> <td><code class="language-plaintext highlighter-rouge">git add -A</code> or <br/> <code class="language-plaintext highlighter-rouge">git add .</code></td> </tr> <tr> <td>Commit changes</td> <td><code class="language-plaintext highlighter-rouge">git commit -m "&lt;commit message&gt;"</code></td> </tr> <tr> <td>Remove a file (or folder)</td> <td><code class="language-plaintext highlighter-rouge">git rm -r &lt;file-name.txt&gt;</code></td> </tr> </tbody> </table> <h3 id="inspection--comparison">Inspection &amp; Comparison</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>View changes</td> <td><code class="language-plaintext highlighter-rouge">git log</code></td> </tr> <tr> <td>View changes (detailed)</td> <td><code class="language-plaintext highlighter-rouge">git log --summary</code></td> </tr> <tr> <td>View changes in one line (briefly)</td> <td><code class="language-plaintext highlighter-rouge">git log --oneline</code> or <br/> <code class="language-plaintext highlighter-rouge">git log --pretty=oneline</code> or<br/> <code class="language-plaintext highlighter-rouge">git log --pretty=short</code></td> </tr> </tbody> </table> <h3 id="undo-to-previous-file">Undo to previous file</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>List of all commit with commit id and commit message)</td> <td><code class="language-plaintext highlighter-rouge">git log --oneline</code></td> </tr> <tr> <td>Return to previous commit <commit></commit></td> <td><code class="language-plaintext highlighter-rouge">git checkout&lt;commit id&gt;</code></td> </tr> <tr> <td>Revert commit <commit> (undo one particular commit)</commit></td> <td><code class="language-plaintext highlighter-rouge">git revert &lt;commit id&gt;</code></td> </tr> <tr> <td>Reset to previous commit <commit> (remove history of all commit after <commit> )</commit></commit></td> <td><code class="language-plaintext highlighter-rouge">git reset --hard &lt;commit id&gt;</code></td> </tr> <tr> <td>Stop a file being tracked</td> <td><code class="language-plaintext highlighter-rouge">git rm --cached &lt;file/folder&gt;</code></td> </tr> <tr> <td>Restore a file to a previous commit</td> <td><code class="language-plaintext highlighter-rouge">git checkout &lt;file/to/restore&gt;</code></td> </tr> </tbody> </table> <h3 id="branching--merging">Branching &amp; Merging</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>List branches (the asterisk denotes the current branch)</td> <td><code class="language-plaintext highlighter-rouge">git branch</code></td> </tr> <tr> <td>List all branches (local and remote)</td> <td><code class="language-plaintext highlighter-rouge">git branch -a</code></td> </tr> <tr> <td>Create a new branch</td> <td><code class="language-plaintext highlighter-rouge">git branch &lt;branch name&gt;</code></td> </tr> <tr> <td>Create a new branch and switch to it</td> <td><code class="language-plaintext highlighter-rouge">git checkout -b &lt;branch name&gt;</code></td> </tr> <tr> <td>Clone a remote branch and switch to it</td> <td><code class="language-plaintext highlighter-rouge">git checkout -b &lt;branch name&gt; origin/&lt;branch name&gt;</code></td> </tr> <tr> <td>Rename a local branch</td> <td><code class="language-plaintext highlighter-rouge">git branch -m &lt;old branch name&gt; &lt;new branch name&gt;</code></td> </tr> <tr> <td>Switch to a branch</td> <td><code class="language-plaintext highlighter-rouge">git checkout &lt;branch name&gt;</code></td> </tr> <tr> <td>Switch to the branch last checked out</td> <td><code class="language-plaintext highlighter-rouge">git checkout -</code></td> </tr> <tr> <td>Discard changes to a file</td> <td><code class="language-plaintext highlighter-rouge">git checkout -- &lt;file-name.txt&gt;</code></td> </tr> <tr> <td>Delete a branch</td> <td><code class="language-plaintext highlighter-rouge">git branch -d &lt;branch name&gt;</code></td> </tr> <tr> <td>Delete a remote branch</td> <td><code class="language-plaintext highlighter-rouge">git push origin --delete &lt;branch name&gt;</code></td> </tr> <tr> <td>Preview changes before merging</td> <td><code class="language-plaintext highlighter-rouge">git diff &lt;source branch&gt; &lt;target branch&gt;</code></td> </tr> <tr> <td>Merge a branch into the active branch</td> <td><code class="language-plaintext highlighter-rouge">git merge &lt;branch name&gt;</code></td> </tr> <tr> <td>Merge a branch into a target branch</td> <td><code class="language-plaintext highlighter-rouge">git merge &lt;source branch&gt; &lt;target branch&gt;</code></td> </tr> <tr> <td>Stash changes in a dirty working directory</td> <td><code class="language-plaintext highlighter-rouge">git stash</code></td> </tr> <tr> <td>Remove all stashed entries</td> <td><code class="language-plaintext highlighter-rouge">git stash clear</code></td> </tr> </tbody> </table> <h3 id="sharing--updating-projects">Sharing &amp; Updating Projects</h3> <table> <thead> <tr> <th>Description</th> <th>Command</th> </tr> </thead> <tbody> <tr> <td>Push a branch to your remote repository</td> <td><code class="language-plaintext highlighter-rouge">git push origin &lt;branch name&gt;</code></td> </tr> <tr> <td>Push changes to remote repository (and remember the branch)</td> <td><code class="language-plaintext highlighter-rouge">git push -u origin &lt;branch name&gt;</code></td> </tr> <tr> <td>Push changes to remote repository (remembered branch)</td> <td><code class="language-plaintext highlighter-rouge">git push</code></td> </tr> <tr> <td>Push changes to remote repository all branch</td> <td><code class="language-plaintext highlighter-rouge">git push --all</code></td> </tr> <tr> <td>Push changes to remote repository (Force)</td> <td><code class="language-plaintext highlighter-rouge">git push -f</code></td> </tr> <tr> <td>Delete a remote branch</td> <td><code class="language-plaintext highlighter-rouge">git push origin --delete &lt;branch name&gt;</code></td> </tr> <tr> <td>Update local repository to the newest commit</td> <td><code class="language-plaintext highlighter-rouge">git pull</code></td> </tr> <tr> <td>Pull changes from remote repository</td> <td><code class="language-plaintext highlighter-rouge">git pull origin &lt;branch name&gt;</code></td> </tr> <tr> <td>Add a remote repository</td> <td><code class="language-plaintext highlighter-rouge">git remote add origin ssh://git@github.com/&lt;username&gt;/&lt;repository-name&gt;.git</code></td> </tr> <tr> <td>Set a repository’s origin branch to SSH</td> <td><code class="language-plaintext highlighter-rouge">git remote set-url origin ssh://git@github.com/&lt;username&gt;/&lt;repository-name&gt;.git</code></td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="computer_science"/><summary type="html"><![CDATA[some git commands]]></summary></entry></feed>