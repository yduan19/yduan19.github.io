---
layout: post
title: High-Performance LLM Inference with vLLM and TGI
date: 2025-11-22 8:07:00
description: Complete guide to deploying and optimizing LLM inference using vLLM and Text Generation Inference
tags: python machine-learning llm inference vllm tgi optimization
categories: computer_science
featured: false
---

This comprehensive tutorial covers high-performance LLM inference hosting using vLLM and TGI (Text Generation Inference), with a strong focus on optimizing inference speed. Both frameworks are production-ready inference servers that provide high throughput, low latency, and efficient GPU memory utilization.

## Introduction to vLLM and TGI

### vLLM (Very Large Language Model)
- **Developer**: UC Berkeley
- **Key Innovation**: PagedAttention algorithm for efficient memory management
- **Best For**: High-throughput scenarios with batch processing
- **Language**: Python-based, easy to integrate
- **Supports**: Most popular models (LLaMA, Mistral, GPT, etc.)

### TGI (Text Generation Inference)
- **Developer**: Hugging Face
- **Key Innovation**: Written in Rust for maximum performance
- **Best For**: Low-latency scenarios and real-time applications
- **Features**: Built-in Flash Attention, extensive quantization support
- **Supports**: GPTQ, AWQ, bitsandbytes quantization

### Key Speed Optimization Techniques

Both frameworks implement several critical optimization techniques:

1. **Continuous Batching** - Process multiple requests in parallel efficiently
2. **KV Cache Optimization** - Efficient attention computation and memory usage
3. **Quantization** - Reduce model size (4-bit, 8-bit) for faster inference
4. **Tensor Parallelism** - Distribute model across multiple GPUs
5. **Speculative Decoding** - Use smaller models to predict tokens
6. **PagedAttention (vLLM)** - Eliminates memory fragmentation
7. **Flash Attention** - Optimized attention computation

<br>

## Part 1: vLLM Setup and Installation

### Installation

```bash
# Install vLLM
pip install vllm

# For development
pip install vllm[dev]
```

### Starting vLLM Server - Basic Configuration

```bash
# Basic server startup
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --port 8000 \
    --tensor-parallel-size 1
```

### Advanced vLLM Server Configurations

```bash
# With quantization (faster, less memory)
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --quantization awq \
    --port 8000

# With multiple GPUs (tensor parallelism)
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --tensor-parallel-size 2 \
    --port 8000

# With continuous batching optimization
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --max-num-seqs 256 \
    --gpu-memory-utilization 0.95 \
    --port 8000
```

### vLLM Client Implementation

vLLM provides an OpenAI-compatible API, making it easy to integrate:

```python
import asyncio
import time
from openai import OpenAI, AsyncOpenAI
from typing import List, Dict, Any

class VLLMClient:
    """
    Client for interacting with vLLM server.
    vLLM provides an OpenAI-compatible API.
    """
    
    def __init__(self, base_url: str = "http://localhost:8000/v1", api_key: str = "dummy"):
        self.client = OpenAI(base_url=base_url, api_key=api_key)
        self.base_url = base_url
    
    def generate(
        self,
        prompt: str,
        model: str = "meta-llama/Llama-2-7b-chat-hf",
        max_tokens: int = 100,
        temperature: float = 0.7,
        top_p: float = 1.0,
        stream: bool = False
    ) -> str:
        """
        Generate text using vLLM.
        
        Args:
            prompt: Input prompt
            model: Model name
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            top_p: Nucleus sampling parameter
            stream: Whether to stream the response
        """
        messages = [{"role": "user", "content": prompt}]
        
        response = self.client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            stream=stream
        )
        
        if stream:
            full_response = ""
            for chunk in response:
                if chunk.choices[0].delta.content:
                    full_response += chunk.choices[0].delta.content
            return full_response
        else:
            return response.choices[0].message.content
    
    def batch_generate(
        self,
        prompts: List[str],
        model: str = "meta-llama/Llama-2-7b-chat-hf",
        max_tokens: int = 100,
        temperature: float = 0.7
    ) -> List[str]:
        """
        Generate text for multiple prompts using vLLM's continuous batching.
        This is more efficient than sequential calls.
        """
        messages_list = [[{"role": "user", "content": prompt}] for prompt in prompts]
        
        results = []
        for messages in messages_list:
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature
            )
            results.append(response.choices[0].message.content)
        
        return results
    
    def async_batch_generate(
        self,
        prompts: List[str],
        model: str = "meta-llama/Llama-2-7b-chat-hf",
        max_tokens: int = 100,
        temperature: float = 0.7
    ) -> List[str]:
        """
        Generate text for multiple prompts asynchronously.
        This leverages vLLM's continuous batching for maximum throughput.
        """
        async def generate_single(prompt: str):
            client = AsyncOpenAI(base_url=self.base_url, api_key="dummy")
            messages = [{"role": "user", "content": prompt}]
            response = await client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature
            )
            return response.choices[0].message.content
        
        async def generate_all():
            tasks = [generate_single(prompt) for prompt in prompts]
            return await asyncio.gather(*tasks)
        
        return asyncio.run(generate_all())
```

<br>

## Part 2: TGI Setup and Installation

### Installation with Docker (Recommended)

```bash
# Basic TGI server with Docker
docker run --gpus all -p 8080:80 \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id meta-llama/Llama-2-7b-chat-hf
```

### Advanced TGI Server Configurations

```bash
# With quantization (4-bit for speed)
docker run --gpus all -p 8080:80 \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id meta-llama/Llama-2-7b-chat-hf \
    --quantize bitsandbytes \
    --num-shard 1

# With multiple GPUs (sharding)
docker run --gpus all -p 8080:80 \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id meta-llama/Llama-2-7b-chat-hf \
    --num-shard 2

# With custom batching configuration
docker run --gpus all -p 8080:80 \
    -v $PWD/data:/data \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id meta-llama/Llama-2-7b-chat-hf \
    --max-batch-total-tokens 4096 \
    --num-shard 1 \
    --port 80
```

### TGI Client Implementation

TGI provides a REST API (not OpenAI-compatible by default):

```python
import requests
import json
from typing import List, Dict, Any

class TGIClient:
    """
    Client for interacting with TGI server.
    TGI provides a REST API (not OpenAI-compatible by default).
    """
    
    def __init__(self, base_url: str = "http://localhost:8080"):
        self.base_url = base_url
    
    def generate(
        self,
        prompt: str,
        max_new_tokens: int = 100,
        temperature: float = 0.7,
        top_p: float = 0.95,
        top_k: int = 50,
        do_sample: bool = True,
        stream: bool = False
    ) -> str:
        """
        Generate text using TGI.
        
        Args:
            prompt: Input prompt
            max_new_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            top_p: Nucleus sampling parameter
            top_k: Top-k sampling parameter
            do_sample: Whether to use sampling
            stream: Whether to stream the response
        """
        url = f"{self.base_url}/generate"
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": max_new_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "top_k": top_k,
                "do_sample": do_sample
            }
        }
        
        if stream:
            payload["stream"] = True
            response = requests.post(url, json=payload, stream=True)
            full_response = ""
            for line in response.iter_lines():
                if line:
                    data = json.loads(line)
                    if "token" in data:
                        full_response += data["token"]["text"]
            return full_response
        else:
            response = requests.post(url, json=payload)
            response.raise_for_status()
            result = response.json()
            return result["generated_text"]
    
    def batch_generate(
        self,
        prompts: List[str],
        max_new_tokens: int = 100,
        temperature: float = 0.7
    ) -> List[str]:
        """
        Generate text for multiple prompts using TGI's batching.
        """
        results = []
        for prompt in prompts:
            result = self.generate(
                prompt=prompt,
                max_new_tokens=max_new_tokens,
                temperature=temperature
            )
            results.append(result)
        return results
```

<br>

## Part 3: vLLM Speed Optimization Strategies

### 1. Continuous Batching (Automatic)

vLLM automatically batches requests for optimal throughput:

```bash
# Tune batching with max-num-seqs parameter
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --max-num-seqs 256 \
    --port 8000
```

**Configuration Guide**:
- Higher `max-num-seqs` = More throughput, more memory usage
- Typical values: 64-256 depending on GPU memory
- Monitor GPU memory utilization to find optimal value

### 2. PagedAttention (Automatic)

PagedAttention is vLLM's key innovation for efficient memory management:
- **No configuration needed** - automatically enabled
- Eliminates memory fragmentation
- Enables higher batch sizes
- Up to 24x higher throughput vs traditional methods

### 3. Quantization for Speed

Quantization reduces model size and increases inference speed:

```bash
# AWQ quantization (recommended for speed)
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --quantization awq \
    --port 8000

# GPTQ quantization
python -m vllm.entrypoints.openai.api_server \
    --model TheBloke/Llama-2-7B-Chat-GPTQ \
    --quantization gptq \
    --port 8000
```

**Benefits**:
- 2-3x speedup
- 4x memory reduction
- Minimal quality loss (<2% typically)

### 4. Tensor Parallelism (Multi-GPU)

Distribute model across multiple GPUs:

```bash
# Use 2 GPUs
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --tensor-parallel-size 2 \
    --port 8000

# Use 4 GPUs for larger models
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-70b-chat-hf \
    --tensor-parallel-size 4 \
    --port 8000
```

**Performance**:
- Near-linear scaling up to 4 GPUs
- Best for large models (70B+)
- Requires compatible GPUs

### 5. KV Cache Optimization

Optimize GPU memory utilization for faster inference:

```bash
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --gpu-memory-utilization 0.95 \
    --port 8000
```

**Tuning Guide**:
- Default: 0.9 (90% of GPU memory)
- Higher values (0.95) = More caching, faster inference
- Leave some headroom to prevent OOM errors

### 6. Complete Optimization Example

Here's a fully optimized vLLM configuration:

```bash
# Maximum speed configuration
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --quantization awq \
    --tensor-parallel-size 2 \
    --max-num-seqs 256 \
    --gpu-memory-utilization 0.95 \
    --port 8000
```

**Expected Performance**:
- 3-5x faster than baseline
- 50-100 requests/second (depending on prompt length)
- <100ms latency for short prompts

<br>

## Part 4: TGI Speed Optimization Strategies

### 1. Continuous Batching

TGI automatically batches requests:

```bash
docker run --gpus all -p 8080:80 \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id meta-llama/Llama-2-7b-chat-hf \
    --max-batch-total-tokens 4096
```

**Configuration**:
- Higher `max-batch-total-tokens` = More throughput
- Typical values: 2048-8192

### 2. Flash Attention (Automatic)

Flash Attention is enabled by default for supported models:
- **No configuration needed**
- 2-4x faster attention computation
- Reduced memory usage
- Works with most modern architectures

### 3. Quantization

TGI supports multiple quantization methods:

```bash
# bitsandbytes (4-bit or 8-bit)
docker run --gpus all -p 8080:80 \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id meta-llama/Llama-2-7b-chat-hf \
    --quantize bitsandbytes

# GPTQ quantization
docker run --gpus all -p 8080:80 \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id TheBloke/Llama-2-7B-Chat-GPTQ \
    --quantize gptq

# AWQ quantization
docker run --gpus all -p 8080:80 \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id TheBloke/Llama-2-7B-Chat-AWQ \
    --quantize awq
```

### 4. Model Sharding (Multi-GPU)

Distribute model across GPUs:

```bash
# Use 2 GPUs
docker run --gpus all -p 8080:80 \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id meta-llama/Llama-2-7b-chat-hf \
    --num-shard 2
```

### 5. Token Streaming

Stream tokens for better perceived latency:

```python
# Client code for streaming
def generate_stream(self, prompt: str):
    url = f"{self.base_url}/generate_stream"
    payload = {
        "inputs": prompt,
        "parameters": {"max_new_tokens": 100}
    }
    
    response = requests.post(url, json=payload, stream=True)
    for line in response.iter_lines():
        if line:
            data = json.loads(line)
            if "token" in data:
                yield data["token"]["text"]
```

### 6. Complete Optimization Example

```bash
# Maximum speed configuration
docker run --gpus all -p 8080:80 \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id meta-llama/Llama-2-7b-chat-hf \
    --quantize bitsandbytes \
    --num-shard 2 \
    --max-batch-total-tokens 4096 \
    --port 80
```

<br>

## Part 5: Performance Benchmarking

### Benchmarking Framework

```python
import statistics

def benchmark_inference_speed(
    client,
    prompts: List[str],
    num_runs: int = 5
) -> Dict[str, Any]:
    """
    Benchmark inference speed with multiple runs.
    
    Args:
        client: vLLM or TGI client
        prompts: List of prompts to test
        num_runs: Number of benchmark runs
    """
    latencies = []
    throughputs = []
    
    for run in range(num_runs):
        start_time = time.time()
        
        if isinstance(client, VLLMClient):
            results = client.async_batch_generate(prompts)
        elif isinstance(client, TGIClient):
            results = client.batch_generate(prompts)
        else:
            raise ValueError("Unknown client type")
        
        end_time = time.time()
        total_time = end_time - start_time
        
        latencies.append(total_time / len(prompts))
        throughputs.append(len(prompts) / total_time)
    
    return {
        "avg_latency": statistics.mean(latencies),
        "std_latency": statistics.stdev(latencies) if len(latencies) > 1 else 0,
        "avg_throughput": statistics.mean(throughputs),
        "std_throughput": statistics.stdev(throughputs) if len(throughputs) > 1 else 0,
        "min_latency": min(latencies),
        "max_latency": max(latencies),
        "num_prompts": len(prompts),
        "num_runs": num_runs
    }
```

### Speed Comparison Utility

```python
class InferenceSpeedOptimizer:
    """
    Demonstrates various techniques to improve inference speed.
    """
    
    @staticmethod
    def measure_latency(func, *args, **kwargs) -> Dict[str, float]:
        """
        Measure the latency of a function call.
        """
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        return {
            "result": result,
            "latency": end_time - start_time,
            "tokens_per_second": len(result.split()) / (end_time - start_time) if result else 0
        }
    
    @staticmethod
    def measure_throughput(func, prompts: List[str], *args, **kwargs) -> Dict[str, float]:
        """
        Measure throughput (requests per second) for batch processing.
        """
        start_time = time.time()
        results = func(prompts, *args, **kwargs)
        end_time = time.time()
        
        total_time = end_time - start_time
        return {
            "results": results,
            "total_time": total_time,
            "throughput": len(prompts) / total_time,
            "avg_latency": total_time / len(prompts)
        }
```

<br>

## Part 6: Optimization Configuration Examples

### vLLM Configurations

```python
def vllm_optimization_examples():
    """
    Example vLLM server configurations for different optimization goals.
    """
    examples = {
        "max_speed": {
            "command": """python -m vllm.entrypoints.openai.api_server \\
    --model meta-llama/Llama-2-7b-chat-hf \\
    --quantization awq \\
    --tensor-parallel-size 2 \\
    --max-num-seqs 256 \\
    --gpu-memory-utilization 0.95 \\
    --port 8000""",
            "description": "Maximum speed: quantization + tensor parallelism + high batching"
        },
        
        "balanced": {
            "command": """python -m vllm.entrypoints.openai.api_server \\
    --model meta-llama/Llama-2-7b-chat-hf \\
    --max-num-seqs 128 \\
    --gpu-memory-utilization 0.9 \\
    --port 8000""",
            "description": "Balanced: good speed and quality"
        },
        
        "low_memory": {
            "command": """python -m vllm.entrypoints.openai.api_server \\
    --model meta-llama/Llama-2-7b-chat-hf \\
    --quantization awq \\
    --max-num-seqs 64 \\
    --gpu-memory-utilization 0.8 \\
    --port 8000""",
            "description": "Low memory: quantization + reduced batching"
        },
        
        "high_quality": {
            "command": """python -m vllm.entrypoints.openai.api_server \\
    --model meta-llama/Llama-2-7b-chat-hf \\
    --max-num-seqs 64 \\
    --gpu-memory-utilization 0.85 \\
    --dtype float16 \\
    --port 8000""",
            "description": "High quality: no quantization, lower batching"
        }
    }
    
    return examples
```

### TGI Configurations

```python
def tgi_optimization_examples():
    """
    Example TGI server configurations for different optimization goals.
    """
    examples = {
        "max_speed": {
            "command": """docker run --gpus all -p 8080:80 \\
    ghcr.io/huggingface/text-generation-inference:latest \\
    --model-id meta-llama/Llama-2-7b-chat-hf \\
    --quantize bitsandbytes \\
    --num-shard 2 \\
    --max-batch-total-tokens 4096 \\
    --port 80""",
            "description": "Maximum speed: quantization + sharding + high batching"
        },
        
        "balanced": {
            "command": """docker run --gpus all -p 8080:80 \\
    ghcr.io/huggingface/text-generation-inference:latest \\
    --model-id meta-llama/Llama-2-7b-chat-hf \\
    --num-shard 1 \\
    --max-batch-total-tokens 2048 \\
    --port 80""",
            "description": "Balanced: good speed and quality"
        },
        
        "low_memory": {
            "command": """docker run --gpus all -p 8080:80 \\
    ghcr.io/huggingface/text-generation-inference:latest \\
    --model-id meta-llama/Llama-2-7b-chat-hf \\
    --quantize bitsandbytes \\
    --num-shard 1 \\
    --max-batch-total-tokens 1024 \\
    --port 80""",
            "description": "Low memory: quantization + reduced batching"
        },
        
        "high_quality": {
            "command": """docker run --gpus all -p 8080:80 \\
    ghcr.io/huggingface/text-generation-inference:latest \\
    --model-id meta-llama/Llama-2-7b-chat-hf \\
    --num-shard 1 \\
    --max-batch-total-tokens 1024 \\
    --dtype float16 \\
    --port 80""",
            "description": "High quality: no quantization, lower batching"
        }
    }
    
    return examples
```

<br>

## Part 7: Best Practices for Speed Optimization

### 15 Essential Optimization Techniques

1. **Choose the Right Inference Server**
   - vLLM: Best for high-throughput batch processing
   - TGI: Best for low-latency real-time applications
   - Both support continuous batching and modern optimizations

2. **Use Quantization**
   - 4-bit quantization (AWQ, GPTQ, bitsandbytes)
   - 2-3x speedup with 4x memory reduction
   - Minimal quality loss (<2% typically)
   - AWQ recommended for vLLM, bitsandbytes for TGI

3. **Optimize Batching**
   - Increase `max-num-seqs` (vLLM) or `max-batch-total-tokens` (TGI)
   - Higher values = more throughput
   - Monitor GPU memory usage
   - Find sweet spot for your hardware

4. **Use Multiple GPUs**
   - Tensor parallelism (vLLM) or sharding (TGI)
   - 2-4x speedup with 2-4 GPUs
   - Best for models 13B+
   - Requires compatible GPUs

5. **Optimize KV Cache**
   - Increase `gpu-memory-utilization` (vLLM)
   - Increase `max-total-tokens` (TGI)
   - More cache = faster inference
   - Balance with available memory

6. **Use Async Requests**
   - Send multiple requests concurrently
   - Leverage continuous batching
   - Use asyncio or threading
   - Don't wait for sequential processing

7. **Stream Responses**
   - Better perceived latency
   - Users see results faster
   - Reduces time-to-first-token
   - Improves user experience

8. **Choose Appropriate Model Size**
   - Smaller models = faster inference
   - 7B models for most tasks
   - 13B for complex reasoning
   - 70B only when necessary

9. **Monitor Resource Usage**
   - Watch GPU memory utilization
   - Monitor GPU compute usage
   - Adjust parameters based on bottlenecks
   - Use `nvidia-smi` for monitoring

10. **Cache Frequent Prompts**
    - Cache identical prompts
    - Use memoization or Redis
    - Reduces redundant computation
    - Significant speedup for repeated queries

11. **Optimize Prompt Length**
    - Shorter prompts = faster inference
    - Remove unnecessary context
    - Use prompt compression techniques
    - KV cache size matters

12. **Tune Generation Parameters**
    - Lower `max_tokens` = faster inference
    - Adjust temperature, top_p, top_k
    - Use greedy decoding (temperature=0) for speed
    - Balance quality vs speed

13. **Use Appropriate Data Types**
    - float16 instead of float32
    - Reduces memory usage
    - Increases speed
    - Minimal quality impact

14. **Benchmark and Iterate**
    - Test different configurations
    - Measure latency and throughput
    - Optimize for your specific workload
    - Use profiling tools

15. **Hardware Considerations**
    - Use latest GPU architectures (A100, H100)
    - Ensure sufficient PCIe bandwidth
    - Use NVMe storage for model loading
    - Consider CPU-GPU memory transfer

<br>

## Part 8: Complete Working Example

```python
def example_usage():
    """
    Complete example demonstrating vLLM and TGI usage.
    """
    print("=" * 80)
    print("LLM Inference Hosting Tutorial - Example Usage")
    print("=" * 80)
    
    # Example prompts for testing
    prompts = [
        "Explain machine learning in one sentence.",
        "What is the capital of France?",
        "Write a haiku about programming.",
    ]
    
    # vLLM Example
    print("\n1. vLLM Client Example:")
    print("-" * 80)
    try:
        vllm_client = VLLMClient()
        result = vllm_client.generate(
            prompt="What is Python?",
            max_tokens=50,
            temperature=0.7
        )
        print(f"Result: {result}")
        
        # Batch generation
        print("\nBatch generation:")
        results = vllm_client.async_batch_generate(prompts)
        for i, result in enumerate(results):
            print(f"\nPrompt {i+1}: {result[:100]}...")
            
    except Exception as e:
        print(f"Error (vLLM server may not be running): {e}")
        print("\nStart vLLM server with:")
        print("python -m vllm.entrypoints.openai.api_server \\")
        print("  --model meta-llama/Llama-2-7b-chat-hf \\")
        print("  --port 8000")
    
    # TGI Example
    print("\n2. TGI Client Example:")
    print("-" * 80)
    try:
        tgi_client = TGIClient()
        result = tgi_client.generate(
            prompt="What is Python?",
            max_new_tokens=50,
            temperature=0.7
        )
        print(f"Result: {result}")
        
        # Batch generation
        print("\nBatch generation:")
        results = tgi_client.batch_generate(prompts)
        for i, result in enumerate(results):
            print(f"\nPrompt {i+1}: {result[:100]}...")
            
    except Exception as e:
        print(f"Error (TGI server may not be running): {e}")
        print("\nStart TGI server with Docker:")
        print("docker run --gpus all -p 8080:80 \\")
        print("  ghcr.io/huggingface/text-generation-inference:latest \\")
        print("  --model-id meta-llama/Llama-2-7b-chat-hf")
    
    # Optimization Examples
    print("\n3. Optimization Configuration Examples:")
    print("-" * 80)
    
    print("\nvLLM Optimizations:")
    vllm_examples = vllm_optimization_examples()
    for name, config in vllm_examples.items():
        print(f"\n{name.upper()}:")
        print(f"  {config['description']}")
        print(f"  {config['command']}")
    
    print("\n\nTGI Optimizations:")
    tgi_examples = tgi_optimization_examples()
    for name, config in tgi_examples.items():
        print(f"\n{name.upper()}:")
        print(f"  {config['description']}")
        print(f"  {config['command']}")
    
    print("\n" + "=" * 80)


if __name__ == "__main__":
    example_usage()
```

<br>

## Part 9: vLLM vs TGI Performance Comparison

### When to Choose vLLM

**Advantages**:
- ✅ Higher throughput for batch processing (2-3x)
- ✅ Better memory efficiency with PagedAttention
- ✅ Easier Python integration
- ✅ More flexible configuration options
- ✅ Better for high-concurrency scenarios
- ✅ Active development and community support

**Best For**:
- API serving with high request volume
- Batch processing workflows
- Research and experimentation
- Python-first environments

### When to Choose TGI

**Advantages**:
- ✅ Lower latency for single requests (10-20% faster)
- ✅ Rust-based for maximum performance
- ✅ Better Docker integration
- ✅ Built-in Flash Attention
- ✅ Extensive quantization support
- ✅ Production-ready with Hugging Face backing

**Best For**:
- Real-time chat applications
- Interactive user experiences
- Low-latency requirements
- Docker/Kubernetes deployments

### Performance Metrics (Same Hardware)

| Metric | vLLM | TGI |
|--------|------|-----|
| Throughput (batch) | **100-150 req/s** | 70-100 req/s |
| Single request latency | 100-120ms | **80-100ms** |
| Memory efficiency | **Excellent** (PagedAttention) | Good |
| GPU utilization | **95%+** | 90%+ |
| Concurrent users | **High** (256+) | Medium (128+) |

### Recommendation

- **Choose vLLM if**: You need maximum throughput and batch processing efficiency
- **Choose TGI if**: You need lowest possible latency for real-time applications
- **Both are excellent**: Production-ready, well-maintained, and highly optimized

<br>

## Key Takeaways

1. **Both frameworks are production-ready** with excellent performance
2. **Quantization is critical** - 2-3x speedup with minimal quality loss
3. **Continuous batching** enables efficient multi-request processing
4. **GPU utilization** should be monitored and optimized
5. **Choose based on use case**: vLLM for throughput, TGI for latency
6. **Start simple, then optimize** - measure before adding complexity
7. **Hardware matters** - invest in modern GPUs for best results
8. **Cache aggressively** - eliminate redundant computation
9. **Stream for UX** - improve perceived performance
10. **Monitor and iterate** - continuous optimization is key

With these techniques, you can achieve **3-5x speedup** over baseline inference and serve hundreds of requests per second efficiently!
