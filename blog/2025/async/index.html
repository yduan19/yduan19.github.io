<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> API Async Calls and Speed Optimization | Yifei Duan </title> <meta name="author" content=" "> <meta name="description" content="Complete guide to using Ollama's API with async/await and speed optimization techniques"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://19revey.github.io/blog/2025/async/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Yifei Duan </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">notes </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">API Async Calls and Speed Optimization</h1> <p class="post-meta"> November 22, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> python</a>   <a href="/blog/tag/async"> <i class="fa-solid fa-hashtag fa-sm"></i> async</a>   <a href="/blog/tag/ollama"> <i class="fa-solid fa-hashtag fa-sm"></i> ollama</a>   <a href="/blog/tag/api"> <i class="fa-solid fa-hashtag fa-sm"></i> api</a>   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning</a>     ·   <a href="/blog/category/computer-science"> <i class="fa-solid fa-tag fa-sm"></i> computer_science</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This tutorial demonstrates how to use Ollama’s API with async/await and various techniques to dramatically speed up your API calls. Ollama provides an OpenAI-compatible API, allowing us to use the familiar OpenAI client to interact with locally hosted models.</p> <h2 id="prerequisites">Prerequisites</h2> <p>Before starting, make sure you have:</p> <ol> <li> <strong>Ollama installed</strong>: Download from <a href="https://ollama.ai" rel="external nofollow noopener" target="_blank">https://ollama.ai</a> </li> <li> <strong>A model pulled</strong>: Run <code class="language-plaintext highlighter-rouge">ollama pull llama3.2</code> (or mistral, codellama, etc.)</li> <li> <strong>Ollama running</strong>: Start the server with <code class="language-plaintext highlighter-rouge">ollama serve</code> </li> <li> <strong>Required packages</strong>: Install with <code class="language-plaintext highlighter-rouge">pip install openai aiohttp</code> </li> </ol> <p><br></p> <h2 id="setup-and-configuration">Setup and Configuration</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">asyncio</span>
<span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">aiohttp</span>
<span class="kn">from</span> <span class="n">openai</span> <span class="kn">import</span> <span class="n">AsyncOpenAI</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>

<span class="c1"># Initialize the async client for Ollama
# Ollama runs on localhost:11434 by default and provides OpenAI-compatible API
</span><span class="n">client</span> <span class="o">=</span> <span class="nc">AsyncOpenAI</span><span class="p">(</span>
    <span class="n">base_url</span><span class="o">=</span><span class="sh">"</span><span class="s">http://localhost:11434/v1</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Ollama's API endpoint
</span>    <span class="n">api_key</span><span class="o">=</span><span class="sh">"</span><span class="s">ollama</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Ollama doesn't require a real API key, but the client needs one
</span><span class="p">)</span>

<span class="c1"># Default model to use (change this to any model you have pulled with Ollama)
# Common models: llama2, mistral, codellama, phi, gemma, llama3.2, etc.
</span><span class="n">DEFAULT_MODEL</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llama3.2</span><span class="sh">"</span>
</code></pre></div></div> <p><br></p> <h2 id="part-0-helper-functions">Part 0: Helper Functions</h2> <p>First, let’s create a helper function to list available models:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">list_available_models</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">
    List all available Ollama models on your system.
    This uses Ollama</span><span class="sh">'</span><span class="s">s native API endpoint.
    </span><span class="sh">"""</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">async</span> <span class="k">with</span> <span class="n">aiohttp</span><span class="p">.</span><span class="nc">ClientSession</span><span class="p">()</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
            <span class="k">async</span> <span class="k">with</span> <span class="n">session</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">http://localhost:11434/api/tags</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">response</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">response</span><span class="p">.</span><span class="n">status</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
                    <span class="n">data</span> <span class="o">=</span> <span class="k">await</span> <span class="n">response</span><span class="p">.</span><span class="nf">json</span><span class="p">()</span>
                    <span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="p">[</span><span class="sh">"</span><span class="s">name</span><span class="sh">"</span><span class="p">]</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">data</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">models</span><span class="sh">"</span><span class="p">,</span> <span class="p">[])]</span>
                    <span class="k">return</span> <span class="n">models</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Error fetching models: </span><span class="si">{</span><span class="n">response</span><span class="p">.</span><span class="n">status</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
                    <span class="k">return</span> <span class="p">[]</span>
    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Error connecting to Ollama: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Make sure Ollama is running: ollama serve</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[]</span>
</code></pre></div></div> <p><br></p> <h2 id="part-1-basic-async-api-call">Part 1: Basic Async API Call</h2> <p>The foundation of all async operations is a basic async function:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">basic_async_call</span><span class="p">(</span><span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">DEFAULT_MODEL</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Basic async call to Ollama API.
    This is the foundation for all async operations.
    
    Args:
        prompt: The prompt to send to the model
        model: The Ollama model to use (default: llama3.2)
    </span><span class="sh">"""</span>
    <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">client</span><span class="p">.</span><span class="n">chat</span><span class="p">.</span><span class="n">completions</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span>
        <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span>
</code></pre></div></div> <p><br></p> <h2 id="part-2-sequential-vs-concurrent---performance-comparison">Part 2: Sequential vs Concurrent - Performance Comparison</h2> <p>The key to speed optimization is understanding the difference between sequential and concurrent processing:</p> <h3 id="sequential-approach-slow">Sequential Approach (SLOW)</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">sequential_calls</span><span class="p">(</span><span class="n">prompts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">
    Sequential approach: Process one request at a time.
    This is SLOW because each request waits for the previous one to complete.
    </span><span class="sh">"""</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">basic_async_call</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="n">results</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div> <h3 id="concurrent-approach-fast">Concurrent Approach (FAST)</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">concurrent_calls</span><span class="p">(</span><span class="n">prompts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">
    Concurrent approach: Process all requests simultaneously.
    This is FAST because requests are sent in parallel.
    </span><span class="sh">"""</span>
    <span class="c1"># Create tasks for all prompts
</span>    <span class="n">tasks</span> <span class="o">=</span> <span class="p">[</span><span class="nf">basic_async_call</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">]</span>
    <span class="c1"># Wait for all tasks to complete
</span>    <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div> <h3 id="performance-comparison-demo">Performance Comparison Demo</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">compare_performance</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">
    Compare the performance of sequential vs concurrent calls.
    </span><span class="sh">"""</span>
    <span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">What is Python?</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">What is JavaScript?</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">What is Rust?</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">What is Go?</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">What is TypeScript?</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">]</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Performance Comparison: Sequential vs Concurrent</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    
    <span class="c1"># Sequential approach
</span>    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">sequential_results</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">sequential_calls</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>
    <span class="n">sequential_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Sequential approach: </span><span class="si">{</span><span class="n">sequential_time</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Concurrent approach
</span>    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">concurrent_results</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">concurrent_calls</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>
    <span class="n">concurrent_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Concurrent approach: </span><span class="si">{</span><span class="n">concurrent_time</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="n">speedup</span> <span class="o">=</span> <span class="n">sequential_time</span> <span class="o">/</span> <span class="n">concurrent_time</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Speedup: </span><span class="si">{</span><span class="n">speedup</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">x faster with concurrent calls!</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Expected Result</strong>: Concurrent calls are typically <strong>3-5x faster</strong> for 5 prompts!</p> <p><br></p> <h2 id="part-3-advanced-speed-optimization-techniques">Part 3: Advanced Speed Optimization Techniques</h2> <h3 id="technique-1-batch-processing-with-semaphore">Technique 1: Batch Processing with Semaphore</h3> <p>Use a semaphore to limit concurrent requests and prevent overwhelming the API:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">batch_with_semaphore</span><span class="p">(</span>
    <span class="n">prompts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> 
    <span class="n">max_concurrent</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">
    Use a semaphore to limit concurrent requests.
    This prevents overwhelming the API and helps with rate limits.
    
    Args:
        prompts: List of prompts to process
        max_concurrent: Maximum number of concurrent requests
    </span><span class="sh">"""</span>
    <span class="n">semaphore</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="nc">Semaphore</span><span class="p">(</span><span class="n">max_concurrent</span><span class="p">)</span>
    
    <span class="k">async</span> <span class="k">def</span> <span class="nf">bounded_call</span><span class="p">(</span><span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">async</span> <span class="k">with</span> <span class="n">semaphore</span><span class="p">:</span>  <span class="c1"># Limits concurrent execution
</span>            <span class="k">return</span> <span class="k">await</span> <span class="nf">basic_async_call</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
    
    <span class="n">tasks</span> <span class="o">=</span> <span class="p">[</span><span class="nf">bounded_call</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">]</span>
    <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div> <h3 id="technique-2-error-handling-with-retry-logic">Technique 2: Error Handling with Retry Logic</h3> <p>Robust error handling ensures speed without sacrificing reliability:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">batch_with_error_handling</span><span class="p">(</span>
    <span class="n">prompts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">max_retries</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
    <span class="sh">"""</span><span class="s">
    Process prompts with retry logic and error handling.
    This ensures robustness while maintaining speed.
    </span><span class="sh">"""</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">call_with_retry</span><span class="p">(</span><span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">for</span> <span class="n">attempt</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_retries</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">basic_async_call</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
                <span class="k">return</span> <span class="p">{</span>
                    <span class="sh">"</span><span class="s">index</span><span class="sh">"</span><span class="p">:</span> <span class="n">index</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">result</span><span class="sh">"</span><span class="p">:</span> <span class="n">result</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">success</span><span class="sh">"</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">attempts</span><span class="sh">"</span><span class="p">:</span> <span class="n">attempt</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="p">}</span>
            <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">attempt</span> <span class="o">==</span> <span class="n">max_retries</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">return</span> <span class="p">{</span>
                        <span class="sh">"</span><span class="s">index</span><span class="sh">"</span><span class="p">:</span> <span class="n">index</span><span class="p">,</span>
                        <span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
                        <span class="sh">"</span><span class="s">result</span><span class="sh">"</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
                        <span class="sh">"</span><span class="s">success</span><span class="sh">"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
                        <span class="sh">"</span><span class="s">error</span><span class="sh">"</span><span class="p">:</span> <span class="nf">str</span><span class="p">(</span><span class="n">e</span><span class="p">),</span>
                        <span class="sh">"</span><span class="s">attempts</span><span class="sh">"</span><span class="p">:</span> <span class="n">attempt</span> <span class="o">+</span> <span class="mi">1</span>
                    <span class="p">}</span>
                <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">attempt</span><span class="p">)</span>  <span class="c1"># Exponential backoff
</span>    
    <span class="n">tasks</span> <span class="o">=</span> <span class="p">[</span><span class="nf">call_with_retry</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">prompts</span><span class="p">)]</span>
    <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div> <h3 id="technique-3-streaming-for-better-user-experience">Technique 3: Streaming for Better User Experience</h3> <p>Stream responses for improved perceived performance:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">streaming_response</span><span class="p">(</span><span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">DEFAULT_MODEL</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Stream responses for better perceived performance.
    Users see results as they arrive, not all at once.
    
    Args:
        prompt: The prompt to send to the model
        model: The Ollama model to use (default: llama3.2)
    </span><span class="sh">"""</span>
    <span class="n">stream</span> <span class="o">=</span> <span class="k">await</span> <span class="n">client</span><span class="p">.</span><span class="n">chat</span><span class="p">.</span><span class="n">completions</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span>
        <span class="n">stream</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Streaming response for: </span><span class="sh">'</span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="sh">'"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="n">full_response</span> <span class="o">=</span> <span class="sh">""</span>
    <span class="k">async</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">chunk</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">delta</span><span class="p">.</span><span class="n">content</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">content</span> <span class="o">=</span> <span class="n">chunk</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">delta</span><span class="p">.</span><span class="n">content</span>
            <span class="nf">print</span><span class="p">(</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="sh">""</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">full_response</span> <span class="o">+=</span> <span class="n">content</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">-</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">full_response</span>
</code></pre></div></div> <p><br></p> <h2 id="part-4-practical-example---document-processing">Part 4: Practical Example - Document Processing</h2> <p>Here’s a real-world example of processing multiple documents concurrently:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">process_documents_async</span><span class="p">(</span><span class="n">documents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">
    Practical example: Process multiple documents concurrently.
    </span><span class="sh">"""</span>
    <span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sa">f</span><span class="sh">"</span><span class="s">Summarize the following text in one sentence:</span><span class="se">\n\n</span><span class="si">{</span><span class="n">doc</span><span class="si">}</span><span class="sh">"</span>
        <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span>
    <span class="p">]</span>
    
    <span class="c1"># Use semaphore to limit concurrent requests (respect rate limits)
</span>    <span class="n">summaries</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">batch_with_semaphore</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">max_concurrent</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">summaries</span>
</code></pre></div></div> <p><br></p> <h2 id="part-5-best-practices-and-optimization-tips">Part 5: Best Practices and Optimization Tips</h2> <h3 id="speed-optimization-checklist">Speed Optimization Checklist</h3> <ol> <li> <strong>Use <code class="language-plaintext highlighter-rouge">asyncio.gather()</code> for parallel requests</strong> <ul> <li>Don’t await each call individually</li> <li>Create all tasks first, then gather them</li> </ul> </li> <li> <strong>Use Semaphores to control concurrency</strong> <ul> <li>Prevents overwhelming the API</li> <li>Helps respect rate limits</li> <li>Typical values: 5-20 concurrent requests</li> </ul> </li> <li> <strong>Implement retry logic with exponential backoff</strong> <ul> <li>Handles transient errors</li> <li>Prevents losing progress on failures</li> </ul> </li> <li> <strong>Use streaming for long responses</strong> <ul> <li>Better user experience</li> <li>Perceived performance improvement</li> </ul> </li> <li> <strong>Batch similar requests together</strong> <ul> <li>Reduces overhead</li> <li>More efficient resource usage</li> </ul> </li> <li> <strong>Monitor resource usage</strong> <ul> <li>Ollama runs locally, limited by your hardware</li> <li>Use semaphores to prevent overwhelming your system</li> <li>Monitor GPU/CPU usage for concurrent requests</li> </ul> </li> <li> <strong>Choose the right model</strong> <ul> <li>Smaller models (phi, gemma) are faster</li> <li>Larger models (llama2, mistral) are more capable</li> <li>Use the fastest model that meets your needs</li> </ul> </li> <li> <strong>Cache responses when possible</strong> <ul> <li>Don’t re-request identical prompts</li> <li>Use memoization for repeated queries</li> </ul> </li> </ol> <p><br></p> <h2 id="part-6-complete-working-example">Part 6: Complete Working Example</h2> <p>Here’s a complete example demonstrating all techniques:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">
    Main function demonstrating various async techniques with Ollama.
    </span><span class="sh">"""</span>
    <span class="k">global</span> <span class="n">DEFAULT_MODEL</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Ollama API Async Tutorial - Examples</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    
    <span class="c1"># List available models
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">0. Available Ollama Models:</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">available_models</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">list_available_models</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">available_models</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Found </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">available_models</span><span class="p">)</span><span class="si">}</span><span class="s"> model(s):</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">available_models</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  - </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">No models found. Pull a model first: ollama pull llama3.2</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Using model: </span><span class="si">{</span><span class="n">DEFAULT_MODEL</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">DEFAULT_MODEL</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">available_models</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Warning: </span><span class="si">{</span><span class="n">DEFAULT_MODEL</span><span class="si">}</span><span class="s"> not found in available models!</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">available_models</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Using first available model: </span><span class="si">{</span><span class="n">available_models</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">DEFAULT_MODEL</span> <span class="o">=</span> <span class="n">available_models</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    
    <span class="c1"># Example 1: Basic async call
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">1. Basic Async Call:</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">basic_async_call</span><span class="p">(</span><span class="sh">"</span><span class="s">Explain async/await in Python in one sentence.</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Result: </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Example 2: Performance comparison
</span>    <span class="k">await</span> <span class="nf">compare_performance</span><span class="p">()</span>
    
    <span class="c1"># Example 3: Batch processing with semaphore
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">3. Batch Processing with Semaphore:</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">test_prompts</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">What is machine learning?</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">What is deep learning?</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">What is neural network?</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">batch_results</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">batch_with_semaphore</span><span class="p">(</span><span class="n">test_prompts</span><span class="p">,</span> <span class="n">max_concurrent</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">batch_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Processed </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">test_prompts</span><span class="p">)</span><span class="si">}</span><span class="s"> prompts in </span><span class="si">{</span><span class="n">batch_time</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">batch_results</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Prompt </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="si">:</span><span class="mi">100</span><span class="p">]</span><span class="si">}</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Example 4: Error handling
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">4. Batch Processing with Error Handling:</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">results_with_errors</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">batch_with_error_handling</span><span class="p">(</span><span class="n">test_prompts</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results_with_errors</span><span class="p">:</span>
        <span class="n">status</span> <span class="o">=</span> <span class="sh">"</span><span class="s">✓</span><span class="sh">"</span> <span class="k">if</span> <span class="n">result</span><span class="p">[</span><span class="sh">"</span><span class="s">success</span><span class="sh">"</span><span class="p">]</span> <span class="k">else</span> <span class="sh">"</span><span class="s">✗</span><span class="sh">"</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">status</span><span class="si">}</span><span class="s"> Prompt </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="sh">'</span><span class="s">index</span><span class="sh">'</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="sh">'</span><span class="s">success</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Example 5: Streaming
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">5. Streaming Response:</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">await</span> <span class="nf">streaming_response</span><span class="p">(</span><span class="sh">"</span><span class="s">Explain async/await in Python in 2-3 sentences.</span><span class="sh">"</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="c1"># Run the async main function
</span>    <span class="n">asyncio</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="nf">main</span><span class="p">())</span>
</code></pre></div></div> <p><br></p> <h2 id="running-the-tutorial">Running the Tutorial</h2> <p>To run this tutorial:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 1. Make sure Ollama is running</span>
ollama serve

<span class="c"># 2. Pull a model if you haven't already</span>
ollama pull llama3.2

<span class="c"># 3. Install required packages</span>
pip <span class="nb">install </span>openai aiohttp

<span class="c"># 4. Run the script</span>
python ollama_async_tutorial.py
</code></pre></div></div> <p><br></p> <h2 id="key-takeaways">Key Takeaways</h2> <ul> <li> <strong>Async is powerful</strong>: Concurrent API calls can be 3-5x faster than sequential</li> <li> <strong>Control concurrency</strong>: Use semaphores to prevent overwhelming resources</li> <li> <strong>Handle errors gracefully</strong>: Implement retry logic with exponential backoff</li> <li> <strong>Stream for UX</strong>: Streaming improves perceived performance</li> <li> <strong>Monitor resources</strong>: Local models are hardware-limited</li> <li> <strong>Choose wisely</strong>: Balance model size with speed requirements</li> </ul> <p>With these techniques, you can build fast, robust applications that leverage local LLMs efficiently!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/vllm/">High-Performance LLM Inference with vLLM and TGI</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/transformer/">Transformer using PyTorch</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/uv/">UV - Fast Python Package Manager</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/torch/">Frequently Used Tensor Operation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/pinn/">Physics Informed Neural Network</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 . Last updated: November 23, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>